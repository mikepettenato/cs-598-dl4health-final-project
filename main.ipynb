{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gensim\n",
    "import numpy as np\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import torch, stanza\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import threading\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:01.824854Z",
     "end_time": "2023-04-08T00:35:02.331708Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Processing\n",
    "\n",
    "This section can have stuff related to data prep.\n",
    "\n",
    "\n",
    "Should the MSPC Dataset be a part of this section? - Adam"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    # Note: Unable to use pd.read_csv... the function complained about an issue with the formatting of the tsv file\n",
    "    # train = pd.read_csv('data/msr_paraphrase_train.txt', sep='\\t', encoding='latin1')\n",
    "    # train\n",
    "\n",
    "    # opting to read file in and split columns manually to create a pandas dataframe\n",
    "    list = []\n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            fields = line.split('\\t')\n",
    "            list.append(fields)\n",
    "\n",
    "    df = pd.DataFrame(list[1:], columns=['Quality', 'ID1', 'ID2', 'String1', 'String2'])\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:01.868489Z",
     "end_time": "2023-04-08T00:35:02.351344Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Definition\n",
    "\n",
    "![Model Overview](./images/overview.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Input Layer\n",
    "\n",
    "In the input layer is made up of a Stanford Parser to provide a syntactic tree so that the model can extract significant words (mainly, subject, predicate, object) in the input corpus. Word2Vec is then used to map the words into vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 00:02:32 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-12 00:02:32 INFO: Using device: cpu\n",
      "2023-04-12 00:02:32 INFO: Loading: tokenize\n",
      "2023-04-12 00:02:32 INFO: Loading: pos\n",
      "2023-04-12 00:02:32 INFO: Loading: constituency\n",
      "2023-04-12 00:02:33 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# set 'download_method = None' to not download the resources over and over\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency', download_method=None, use_gpu=False)\n",
    "\n",
    "def trunk_construction(str, parent_label = None):\n",
    "    doc = nlp(str)\n",
    "    tree = doc.sentences[0].constituency\n",
    "\n",
    "    words = construct_sentence(tree, parent_label)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def construct_sentence(tree, parent_label = None, leave_pos=False):\n",
    "\n",
    "    sentences = []\n",
    "    if 'NN' in tree.label:\n",
    "        if parent_label == 'NP':\n",
    "            # sentences.append(tree)\n",
    "            sentences = sentences + tree.leaf_labels()\n",
    "    if 'VB' in tree.label:\n",
    "        if parent_label == 'VP':\n",
    "            #sentences.append(tree)\n",
    "            sentences = sentences + tree.leaf_labels()\n",
    "    for child in tree.children:\n",
    "        sentences = sentences + construct_sentence(child, tree.label)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def find_branches(tree, label, not_in_label=None, ancestors=[]):\n",
    "    branches = []\n",
    "    # print(\"-------------\")\n",
    "    # print(ancestors)\n",
    "    # print(f\"{tree.label} == {label}\")\n",
    "    if tree.label == label and not_in_label not in ancestors:\n",
    "        # print(f\"adding {tree}\")\n",
    "        branches.append(tree)\n",
    "    for child in tree.children:\n",
    "        branches = branches + find_branches(child, label, not_in_label, ancestors + [tree.label])\n",
    "\n",
    "    return branches\n",
    "\n",
    "#\n",
    "# # According to the paper the subject is the first NN child of NP\n",
    "def find_subject(noun_phrase_for_subject):\n",
    "    subject = []\n",
    "    for child in noun_phrase_for_subject.children:\n",
    "        if 'NN' in child.label:\n",
    "            subject = subject + child.leaf_labels()\n",
    "\n",
    "    #print(f\"subject = {subject}\")\n",
    "    #if len(subject) > 0:\n",
    "    #    return ' '.join(subject)\n",
    "    return subject\n",
    "\n",
    "    return None\n",
    "\n",
    "def find_predicate(verb_phrase_for_predicate):\n",
    "    predicate = []\n",
    "    for child in verb_phrase_for_predicate.children:\n",
    "        if child.label.startswith('VB'):\n",
    "            predicate = predicate + child.leaf_labels()\n",
    "\n",
    "    if len(predicate) > 0:\n",
    "        return ' '.join(predicate)\n",
    "\n",
    "    return None\n",
    "\n",
    "def find_object(verb_phase_for_object, parent='VP'):\n",
    "    objects = []\n",
    "    for child in verb_phase_for_object.children:\n",
    "        if child.label == 'VP':\n",
    "            continue\n",
    "        if 'NN' in child.label and parent in ['NP', 'PP', 'ADJP']:\n",
    "            #objects = objects + child.leaf_labels()\n",
    "            new_objects = child.leaf_labels()\n",
    "            for new_object in new_objects:\n",
    "                if new_object not in objects:\n",
    "                    objects.append(new_object)\n",
    "        else:\n",
    "            new_objects = find_object(child, child.label)\n",
    "            #if new_objects not in objects and new_objects is not None:\n",
    "            for new_object in new_objects:\n",
    "                if new_object not in objects:\n",
    "                    objects.append(new_object)\n",
    "                #objects = objects + new_objects\n",
    "\n",
    "    return objects\n",
    "    # if len(objects) > 0:\n",
    "    #     #return ' '.join(objects)\n",
    "    #     return objects\n",
    "    # else:\n",
    "    #     return None\n",
    "\n",
    "def find_spo(tree):\n",
    "    noun_phrases_for_subject = find_branches(tree, label='NP', not_in_label='VP', ancestors=[])\n",
    "    subject_list = []\n",
    "    for noun_phrase_for_subject in noun_phrases_for_subject:\n",
    "        subject = find_subject(noun_phrase_for_subject)\n",
    "        #if subject is not None:\n",
    "        #   subject_list.append(subject)\n",
    "        subject_list = subject_list + subject\n",
    "\n",
    "    verb_phrases = find_branches(tree, label='VP')\n",
    "    predicate_list = []\n",
    "    object_list = []\n",
    "    for verb_phrase in verb_phrases:\n",
    "        predicate = find_predicate(verb_phrase)\n",
    "        if predicate is not None:\n",
    "            predicate_list.append(predicate)\n",
    "        object = find_object(verb_phrase)\n",
    "        object_list = object_list + object\n",
    "        #if object is not None:\n",
    "        #    object_list.append(object)\n",
    "\n",
    "    # dedupe list\n",
    "    subject_list = list(dict.fromkeys(subject_list))\n",
    "    predicate_list = list(dict.fromkeys(predicate_list))\n",
    "    object_list = list(dict.fromkeys(object_list))\n",
    "\n",
    "    return subject_list, predicate_list, object_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:01.890215Z",
     "end_time": "2023-04-08T00:35:03.008883Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "outputs": [],
   "source": [
    " def test_parser(str, valid_subject, valid_predicate, valid_object):\n",
    "\n",
    "    # valid_subject = sorted(valid_subject, key = lambda s: s.casefold())\n",
    "    # valid_predicate = sorted(valid_predicate, key = lambda s: s.casefold())\n",
    "    # valid_object = sorted(valid_object, key = lambda s: s.casefold())\n",
    "\n",
    "    #new_sentence = trunk_construction(str)\n",
    "    #print(new_sentence)\n",
    "    #assert new_sentence == valid_sentence\n",
    "\n",
    "    doc = nlp(str)\n",
    "    tree = doc.sentences[0].constituency\n",
    "    # print(tree)\n",
    "    # print(\"-------\")\n",
    "    #\n",
    "    # noun_phrases_for_subject = find_branches(tree, label='NP', not_in_label='VP', ancestors=[])\n",
    "    # #print(noun_phrases_for_subject)\n",
    "    # subject_list = []\n",
    "    # for noun_phrase_for_subject in noun_phrases_for_subject:\n",
    "    #     subject = find_subject(noun_phrase_for_subject)\n",
    "    #     if subject is not None:\n",
    "    #         subject_list.append(subject)\n",
    "    #\n",
    "    # print(f\"Subject = {' '.join(subject_list)}\")\n",
    "    #\n",
    "    # verb_phrases = find_branches(tree, label='VP')\n",
    "    # predicate_list = []\n",
    "    # object_list = []\n",
    "    # for verb_phrase in verb_phrases:\n",
    "    #     predicate = find_predicate(verb_phrase)\n",
    "    #     if predicate is not None:\n",
    "    #         predicate_list.append(predicate)\n",
    "    #     object = find_object(verb_phrase)\n",
    "    #     if object is not None:\n",
    "    #         object_list.append(object)\n",
    "\n",
    "    subject_list, predicate_list, object_list = find_spo(tree)\n",
    "    print(f\"Subject = {' '.join(subject_list)}\")\n",
    "    print(f\"Predicate = {' '.join(predicate_list)}\")\n",
    "    print(f\"Object = {' '.join(object_list)}\")\n",
    "\n",
    "    print(f\"{subject_list} = {valid_subject}\")\n",
    "    print(f\"{predicate_list} = {valid_predicate}\")\n",
    "    print(f\"{object_list} = {valid_object}\")\n",
    "    assert subject_list == valid_subject\n",
    "    assert predicate_list == valid_predicate\n",
    "    assert object_list == valid_object\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:03.013569Z",
     "end_time": "2023-04-08T00:35:03.015960Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parser Test Cases\n",
    "Test the parser using some of the training data sentences as input and asserting the output sentence matches the algorithm defined in the paper."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject = \n",
      "Predicate = have do is designing\n",
      "Object = amount work set architectures\n",
      "[] = []\n",
      "['have', 'do', 'is', 'designing'] = ['have', 'do', 'is', 'designing']\n",
      "['amount', 'work', 'set', 'architectures'] = ['amount', 'work', 'set', 'architectures']\n",
      "Subject = instruction set\n",
      "Predicate = have got do said\n",
      "Object = amount work\n",
      "['instruction', 'set'] = ['instruction', 'set']\n",
      "['have', 'got', 'do', 'said'] = ['have', 'got', 'do', 'said']\n",
      "['amount', 'work'] = ['amount', 'work']\n",
      "Subject = forces\n",
      "Predicate = launch\n",
      "Object = attacks\n",
      "['forces'] = ['forces']\n",
      "['launch'] = ['launch']\n",
      "['attacks'] = ['attacks']\n",
      "Subject = tire\n",
      "Predicate = was replaced\n",
      "Object = driver\n",
      "['tire'] = ['tire']\n",
      "['was', 'replaced'] = ['was', 'replaced']\n",
      "['driver'] = ['driver']\n",
      "Subject = Amrozi\n",
      "Predicate = accused called distorting\n",
      "Object = brother witness evidence\n",
      "['Amrozi'] = ['Amrozi']\n",
      "['accused', 'called', 'distorting'] = ['accused', 'called', 'distorting']\n",
      "['brother', 'witness', 'evidence'] = ['brother', 'witness', 'evidence']\n",
      "Subject = Amrozi\n",
      "Predicate = Referring accused distorting\n",
      "Object = witness brother evidence\n",
      "['Amrozi'] = ['Amrozi']\n",
      "['Referring', 'accused', 'distorting'] = ['Referring', 'accused', 'distorting']\n",
      "['witness', 'brother', 'evidence'] = ['witness', 'brother', 'evidence']\n",
      "Subject = Shares Genentech company products market\n",
      "Predicate = rose\n",
      "Object = percent\n",
      "['Shares', 'Genentech', 'company', 'products', 'market'] = ['Shares', 'Genentech', 'company', 'products', 'market']\n",
      "['rose'] = ['rose']\n",
      "['percent'] = ['percent']\n",
      "Subject = Shares Xoma\n",
      "Predicate = fell were\n",
      "Object = percent trade shares Genentech company products market\n",
      "['Shares', 'Xoma'] = ['Shares', 'Xoma']\n",
      "['fell', 'were'] = ['fell', 'were']\n",
      "['percent', 'trade', 'shares', 'Genentech', 'company', 'products', 'market'] = ['percent', 'trade', 'shares', 'Genentech', 'company', 'products', 'market']\n",
      "Subject = Gyorgy Heizler head disaster unit\n",
      "Predicate = said was carrying\n",
      "Object = coach passengers\n",
      "['Gyorgy', 'Heizler', 'head', 'disaster', 'unit'] = ['Gyorgy', 'Heizler', 'head', 'disaster', 'unit']\n",
      "['said', 'was', 'carrying'] = ['said', 'was', 'carrying']\n",
      "['coach', 'passengers'] = ['coach', 'passengers']\n",
      "Subject = head disaster unit Gyorgy Heizler\n",
      "Predicate = said had failed heed\n",
      "Object = coach driver stop lights\n",
      "['head', 'disaster', 'unit', 'Gyorgy', 'Heizler'] = ['head', 'disaster', 'unit', 'Gyorgy', 'Heizler']\n",
      "['said', 'had', 'failed', 'heed'] = ['said', 'had', 'failed', 'heed']\n",
      "['coach', 'driver', 'stop', 'lights'] = ['coach', 'driver', 'stop', 'lights']\n",
      "Subject = Sheena Young Child network\n",
      "Predicate = hoped lead\n",
      "Object = guidelines service infertility sufferers\n",
      "['Sheena', 'Young', 'Child', 'network'] = ['Sheena', 'Young', 'Child', 'network']\n",
      "['hoped', 'lead'] = ['hoped', 'lead']\n",
      "['guidelines', 'service', 'infertility', 'sufferers'] = ['guidelines', 'service', 'infertility', 'sufferers']\n",
      "Subject = Sheena Young Child network\n",
      "Predicate = said lead\n",
      "Object = guidelines service infertility sufferers\n",
      "['Sheena', 'Young', 'Child', 'network'] = ['Sheena', 'Young', 'Child', 'network']\n",
      "['said', 'lead'] = ['said', 'lead']\n",
      "['guidelines', 'service', 'infertility', 'sufferers'] = ['guidelines', 'service', 'infertility', 'sufferers']\n"
     ]
    }
   ],
   "source": [
    "test_parser(\"\"\"\"We have an incredible amount of work to do, but it is not in [designing new] instruction set architectures.\"\"\", [], ['have', 'do', 'is', 'designing'], ['amount', 'work', 'set', 'architectures'])\n",
    "test_parser(\"\"\"We have got an incredible amount of work to do, but it ain't in the instruction set,\" he said.\"\"\",  ['instruction', 'set'], ['have', 'got', 'do', 'said'], ['amount', 'work'])\n",
    "test_parser('Syrian forces launch new attacks', ['forces'], ['launch'], ['attacks'])\n",
    "test_parser(\"\"\"the flat tire was replaced by the driver\"\"\", ['tire'], ['was', 'replaced'], ['driver'])\n",
    "test_parser(\"\"\"Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.\"\"\",\n",
    "            ['Amrozi'], ['accused', 'called', 'distorting'], ['brother',  'witness', 'evidence'])\n",
    "test_parser(\"\"\"Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.\"\"\",\n",
    "         ['Amrozi'], ['Referring', 'accused', 'distorting'], ['witness', 'brother', 'evidence'])\n",
    "test_parser(\"\"\"Shares of Genentech, a much larger company with several products on the market, rose more than 2 percent\"\"\",\n",
    "            ['Shares', 'Genentech', 'company', 'products', 'market'], ['rose'], ['percent'])\n",
    "\n",
    "test_parser(\"\"\"Shares of Xoma fell 16 percent in early trade, while shares of Genentech, a much larger company with several products on the market, were up 2 percent.\"\"\", ['Shares', 'Xoma'], ['fell', 'were'], ['percent', 'trade', 'shares', 'Genentech', 'company', 'products', 'market'])\n",
    "\n",
    "test_parser(\"\"\"Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\"\"\",\n",
    "              ['Gyorgy', 'Heizler', 'head', 'disaster', 'unit'], ['said', 'was', 'carrying'], ['coach', 'passengers'])\n",
    "test_parser(\"\"\"The head of the local disaster unit, Gyorgy Heizler, said the coach driver had failed to heed red stop lights.\"\"\",\n",
    "            ['head', 'disaster', 'unit', 'Gyorgy', 'Heizler'], ['said', 'had', 'failed', 'heed'], ['coach', 'driver', 'stop', 'lights'])\n",
    "# test_parser(\"\"\"His wife said he was \"100 percent behind George Bush\" and looked forward to using his years of training in the war.\"\"\",\n",
    "#             \"wife said was percent George Bush looked using years training war\")\n",
    "test_parser(\"\"\"Sheena Young of Child, the national infertility support network, hoped the guidelines would lead to a more \"fair and equitable\" service for infertility sufferers\"\"\", ['Sheena', 'Young', 'Child', 'network'], ['hoped', 'lead'], ['guidelines', 'service', 'infertility', 'sufferers'])\n",
    "test_parser(\"\"\"Sheena Young, for Child, the national infertility support network, said the proposed guidelines should lead to a more \"fair and equitable\" service for infertility sufferers.\"\"\", ['Sheena', 'Young', 'Child', 'network'], ['said', 'lead'], ['guidelines', 'service', 'infertility', 'sufferers'])\n",
    "#\n",
    "# test_parser(\"\"\"Among CNN viewers, 29 percent said they were Republicans and 36 percent called themselves conservatives.\"\"\",\n",
    "#             \"CNN viewers percent said were Republicans percent called conservatives\")\n",
    "# test_parser(\"\"\"Out of Fox viewers, 41 percent describe themselves as Republicans, 24 percent as Democrats and 30 percent as Independents\"\"\",\n",
    "#             \"Fox viewers percent describe Republicans percent Democrats percent Independents\")\n",
    "\n",
    "# Note: stanza parser has a problem with the below sentence.  It is unable to parse it correctly\n",
    "# test_parser(\"\"\"Sheena Young, for Child, the national infertility support network, said the proposed guidelines should lead to a more \"fair and equitable\" service for infertility sufferers.\"\"\", \"\")\n",
    "# test_parser(\"\"\"Among Fox viewers, 41 percent describe themselves as Republicans, 24 percent as Democrats and 30 percent as Independents\"\"\", \"Fox viewers percent describe Republicans percent Democrats percent Independents\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:03.023262Z",
     "end_time": "2023-04-08T00:35:08.918548Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Concurrency Parsing\n",
    "Added support for concurrent parsing.  This can help in the performance of the preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "outputs": [],
   "source": [
    "class SentenceProcessingThread(threading.Thread):\n",
    "    def __init__(self, sentences, output_list, begin, end):\n",
    "        super(SentenceProcessingThread, self).__init__()\n",
    "        self.sentences = sentences\n",
    "        self.nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency', download_method=None, use_gpu=True)\n",
    "        self.output_list = output_list\n",
    "        self.begin = begin\n",
    "        self.end = end\n",
    "\n",
    "    def trunk_construction(self, str, parent_label = None):\n",
    "        doc = self.nlp(str)\n",
    "        tree = doc.sentences[0].constituency\n",
    "\n",
    "        #words = construct_sentence(tree, parent_label)\n",
    "        #return ' '.join(words)\n",
    "\n",
    "        subjects, predicates, objects = find_spo(tree)\n",
    "\n",
    "        return f\"{' '.join(subjects)},{' '.join(predicates)},{' '.join(objects)}\"\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"going to process {self.begin} to {self.end}\")\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "            new_sentence = self.trunk_construction(sentence)\n",
    "            self.output_list[self.begin + i] = new_sentence\n",
    "\n",
    "def process_sentences_concurrently(sentences, output, p=2):\n",
    "    total = len(sentences)\n",
    "    interval = int(total / p)\n",
    "    threads = []\n",
    "    for i in range(p):\n",
    "        s = i*interval\n",
    "        if i == p-1:\n",
    "            e = total\n",
    "        else:\n",
    "            e = (i+1) * interval\n",
    "        sentences_slice = sentences[s:e]\n",
    "        sentence_thread = SentenceProcessingThread(sentences_slice, output, s, e)\n",
    "        sentence_thread.start()\n",
    "        threads.append(sentence_thread)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "def preprocess_corpus(input_file='data/msr_paraphrase_train.txt', output_file='data/msr_paraphrase_train_stanza.txt', N=None):\n",
    "    print(output_file)\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"{output_file} already exists\")\n",
    "        return\n",
    "\n",
    "    starttime = datetime.datetime.now()\n",
    "    df = read_file(input_file)\n",
    "\n",
    "    if N is None:\n",
    "        N = len(df.String1)\n",
    "\n",
    "    output1 = [None] * N\n",
    "    output2 = [None] * N\n",
    "\n",
    "    # we can process with more threads if we only have CPU\n",
    "    p = 8\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # if cuda is available we don't need that many threads\n",
    "        # and if the number of threads is set too large using cuda\n",
    "        # we can get out of memory exceptions\n",
    "        p = 2\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    process_sentences_concurrently(df.String1[:N], output1, p)\n",
    "\n",
    "    # try and be careful with gpu memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    process_sentences_concurrently(df.String2[:N], output2, p)\n",
    "\n",
    "    endtime = datetime.datetime.now()\n",
    "\n",
    "    print(f\"time to process {N*2} sentences is {endtime - starttime}\")\n",
    "\n",
    "    stanza_df = df[:N]\n",
    "\n",
    "    processed_string1 = pd.Series(output1)\n",
    "    # processed_string1.apply(gensim.utils.simple_preprocess)\n",
    "    processed_string2 = pd.Series(output2)\n",
    "    #processed_string2.apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "    stanza_df.String1 = processed_string1\n",
    "    stanza_df.String2 = processed_string2\n",
    "\n",
    "    # write the file out.  This can help in the future\n",
    "    print(f\"about to write out {output_file}\")\n",
    "    stanza_df.to_csv(output_file, sep=\"\\t\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.905455Z",
     "end_time": "2023-04-08T00:35:08.941546Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence Preprocessing\n",
    "pass the input sentences from the training dataset through the stanford/stanza parser, extracting the relevant parts of speech and then tokenize the processed sentences using the gensim.utils.simple_preprocess utility"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "outputs": [],
   "source": [
    "# start_time = datetime.datetime.now()\n",
    "#\n",
    "# processed_string1 = df[:500].String1.apply(trunk_construction)\n",
    "# processed_string2 = df[:500].String2.apply(trunk_construction)\n",
    "#\n",
    "# end_time = datetime.datetime.now()\n",
    "#\n",
    "# print (f\"Processing 200 sentences with stanza library took {end_time - start_time}\")\n",
    "#\n",
    "# start_time = datetime.datetime.now()\n",
    "#\n",
    "# processed_string1 = processed_string1.apply(gensim.utils.simple_preprocess)\n",
    "# processed_string2 = processed_string2.apply(gensim.utils.simple_preprocess)\n",
    "#\n",
    "# end_time = datetime.datetime.now()\n",
    "#\n",
    "# print (f\"Processing 200 sentences with gensim.utils.simple_preprocess took {end_time - start_time}\")\n",
    "# print(f\"Number of sentences processed in the String1 column: len(processed_string1\")\n",
    "# print(f\"Number of sentences processed in the String2 column: len(processed_string2\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.907470Z",
     "end_time": "2023-04-08T00:35:08.942412Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec Embeddings\n",
    "Take the preprocessed and tokenized sentences and use Word2Vec to get the word embeddings.  Take each word embedding in a sentence and find the mean which will represent the embedding for the sentence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "#\n",
    "# corpus = pd.concat([processed_string1, processed_string2], ignore_index=True)\n",
    "#\n",
    "# #model = Word2Vec(sentences=corpus, vector_size=(len(processed_string1) + len(processed_string2)), min_count=2)\n",
    "# # set vector size to reduce computational complexity\n",
    "# model = Word2Vec(sentences=corpus, min_count=1, window=2, vector_size=50)\n",
    "# #model.build_vocab(sentences=corpus)\n",
    "# #model.train(corpus, total_examples=model.corpus_count, epochs=5)\n",
    "#\n",
    "# print(model)\n",
    "# print(model.wv.key_to_index)\n",
    "#\n",
    "# model.wv.get_vector('president')\n",
    "#\n",
    "# # for index, word in enumerate(model.wv.index_to_key):\n",
    "# #     if index == 120:\n",
    "# #         break\n",
    "# #     print(f\"word #{index}/{len(model.wv.index_to_key)} is {word}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.908218Z",
     "end_time": "2023-04-08T00:35:08.942594Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Function is broken out for testing purposes\n",
    "def generate_word2vec_model(corpus):\n",
    "    # Creating the Word2Vec model\n",
    "    model = Word2Vec(sentences=corpus, min_count=1, window=2, vector_size=50)\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.908525Z",
     "end_time": "2023-04-08T00:35:08.942658Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "outputs": [],
   "source": [
    "# Function is broken out for testing purposes\n",
    "def sentence_embeddings(w2v_model, sentence, size):\n",
    "    np_embedding = np.zeros(size)\n",
    "    for i, word in enumerate(sentence):\n",
    "        #print(word)\n",
    "        np_embedding[i] = w2v_model.wv.get_vector(word)\n",
    "\n",
    "    return np_embedding\n",
    "    # list = []\n",
    "    # for word in sentence:\n",
    "    #     list.append(w2v_model.wv.get_vector(word))\n",
    "    #\n",
    "    # word_matrix = np.row_stack(list)\n",
    "    # #return np.mean(word_matrix, axis=0)\n",
    "    # return word_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.910846Z",
     "end_time": "2023-04-08T00:35:08.942887Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "outputs": [],
   "source": [
    "def test_word2vec():\n",
    "\n",
    "    df = read_file('data/msr_paraphrase_train.txt')\n",
    "\n",
    "    sentences1 = df.String1[:5].apply(gensim.utils.simple_preprocess)\n",
    "    sentences2 = df.String2[:5].apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "    corpus = pd.concat([sentences1, sentences2], ignore_index=True)\n",
    "\n",
    "    max_sentence_len = corpus.apply(len).max()\n",
    "\n",
    "    model = generate_word2vec_model(corpus)\n",
    "\n",
    "    embedding = sentence_embeddings(model, corpus[0], (max_sentence_len, 50))\n",
    "    assert embedding.shape == (max_sentence_len, 50)\n",
    "\n",
    "    # Not sure if this will always generate the same embedding\n",
    "    # test_embedding = np.array([-0.00113049, -0.00124808,  0.00252251,  0.00058141,  0.00187964,  0.00379025,\n",
    "    #           -0.00012356,  0.00347055, -0.00241507,  0.00545258, -0.00574078, -0.00489824,\n",
    "    #           -0.00224492,  0.00744946,  0.00350835, -0.00139295, -0.00081134,  0.00655962,\n",
    "    #           0.00244374, -0.00447209, -0.00124291, -0.00092616,  0.0021044,  -0.00092541,\n",
    "    #           0.00284307,  0.00367638,  0.00364716,  0.00519976, -0.00088121,  0.00109841,\n",
    "    #           -0.00219322, -0.00372483,  0.00078702, -0.00612309, -0.00312131,  0.00088071,\n",
    "    #           0.00503909, -0.0009484,  -0.00068209, -0.0004782,   0.00367015,  0.00314679,\n",
    "    #           -0.00302592,  0.00346377,  0.00151145, -0.00076442, -0.0012528,  -0.00087095,\n",
    "    #           -0.00075365,  0.00468711])\n",
    "    #\n",
    "    #\n",
    "    # # not sure if this will always be equal based on comment on test_embedding variable\n",
    "    # assert np.allclose(embedding, test_embedding)\n",
    "\n",
    "test_word2vec()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.911696Z",
     "end_time": "2023-04-08T00:35:09.477774Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "outputs": [],
   "source": [
    "def init_word2vec(train_input_file, test_input_file):\n",
    "\n",
    "    file_parts = os.path.splitext(train_input_file)\n",
    "    train_output_file = f\"{file_parts[0]}_stanza{file_parts[1]}\"\n",
    "    print(\"About to preprocess data\")\n",
    "    preprocess_corpus(input_file=train_input_file, output_file=train_output_file)\n",
    "    print(\"Done preprocessing data\")\n",
    "\n",
    "    file_parts = os.path.splitext(test_input_file)\n",
    "    test_output_file = f\"{file_parts[0]}_stanza{file_parts[1]}\"\n",
    "    print(\"About to preprocess data\")\n",
    "    preprocess_corpus(input_file=test_input_file, output_file=test_output_file)\n",
    "    print(\"Done preprocessing data\")\n",
    "\n",
    "\n",
    "    train_df = pd.read_csv(train_output_file, sep=\"\\t\")\n",
    "    test_df = pd.read_csv(test_output_file, sep=\"\\t\")\n",
    "\n",
    "    # train_df = read_file(train)\n",
    "    # test_df = read_file(test)\n",
    "\n",
    "    train_sentences1 = train_df.String1.apply(gensim.utils.simple_preprocess)\n",
    "    train_sentences2 = train_df.String2.apply(gensim.utils.simple_preprocess)\n",
    "    test_sentences1 = test_df.String1.apply(gensim.utils.simple_preprocess)\n",
    "    test_sentences2 = test_df.String2.apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "    corpus = pd.concat([train_sentences1, train_sentences2, test_sentences1, test_sentences2], ignore_index=True)\n",
    "    max_sentence_len = corpus.apply(len).max()\n",
    "\n",
    "    word2vec = generate_word2vec_model(corpus)\n",
    "\n",
    "\n",
    "    return word2vec, max_sentence_len\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "outputs": [],
   "source": [
    "def corpus_embeddings(model, corpus, max_sentence_len):\n",
    "    corpus_size = len(corpus)\n",
    "    embeddings_list = []\n",
    "    embedding_matrix = np.zeros((corpus_size, max_sentence_len, 50))\n",
    "    for i, sentence in enumerate(corpus):\n",
    "        embeddings = sentence_embeddings(model, sentence, size=(max_sentence_len, 50))\n",
    "        embedding_matrix[i] = embeddings\n",
    "        embeddings_list.append(embeddings)\n",
    "\n",
    "    return embedding_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.972122Z",
     "end_time": "2023-04-08T00:35:09.479438Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Dataset for the MSPC dataset\n",
    "class MSPCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        tsv_file (string): path to the tsv file with sentences to compare and associate quality score\n",
    "        num_records (int): number of records to load.  Defaults to None which is all\n",
    "    \"\"\"\n",
    "    def __init__(self, tsv_file, w2v_model, max_sentence_length, num_records=None):\n",
    "\n",
    "        self.max_sentence_len = max_sentence_length\n",
    "        self.w2v_model = w2v_model\n",
    "\n",
    "        file_parts = os.path.splitext(tsv_file)\n",
    "        output_file = f\"{file_parts[0]}_stanza{file_parts[1]}\"\n",
    "        print(\"About to preprocess data\")\n",
    "        preprocess_corpus(input_file=tsv_file, output_file=output_file)\n",
    "        print(\"Done preprocessing data\")\n",
    "\n",
    "\n",
    "        #df = read_file('data/msr_paraphrase_train.txt')\n",
    "        df = pd.read_csv(output_file, sep=\"\\t\")\n",
    "\n",
    "        if num_records is not None:\n",
    "            processed_string1 = df[:num_records].String1\n",
    "            processed_string2 = df[:num_records].String2\n",
    "            self.quality = df[:num_records].Quality\n",
    "        else:\n",
    "            processed_string1 = df.String1\n",
    "            processed_string2 = df.String2\n",
    "            self.quality = df.Quality\n",
    "\n",
    "        processed_string1 = processed_string1.apply(gensim.utils.simple_preprocess)\n",
    "        processed_string2 = processed_string2.apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "        #corpus = pd.concat([processed_string1, processed_string2], ignore_index=True)\n",
    "\n",
    "\n",
    "        #self.max_sentence_len = corpus.apply(len).max()\n",
    "\n",
    "        #w2v_model = generate_word2vec_model(corpus)\n",
    "\n",
    "        sentence_embeddings1 = corpus_embeddings(self.w2v_model, processed_string1, max_sentence_len=self.max_sentence_len)\n",
    "        sentence_embeddings2 = corpus_embeddings(self.w2v_model, processed_string2, max_sentence_len=self.max_sentence_len)\n",
    "\n",
    "        #self.w2v_model = w2v_model\n",
    "        self.sentences_embeddings1 = sentence_embeddings1\n",
    "        self.sentences_embeddings2 = sentence_embeddings2\n",
    "\n",
    "        # print (f\"Processing 200 sentences with gensim.utils.simple_preprocess took {end_time - start_time}\")\n",
    "        print(f\"Number of sentences processed in the String1 column: {len(processed_string1)}\")\n",
    "        print(f\"Number of sentences processed in the String2 column: {len(processed_string2)}\")\n",
    "        #print(self.sentences_embeddings1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_embeddings1)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.FloatTensor(np.stack((self.sentences_embeddings1[i], self.sentences_embeddings2[i]))), self.quality[i]\n",
    "\n",
    "    def get_max_sentence_length(self):\n",
    "        return self.max_sentence_len"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.981724Z",
     "end_time": "2023-04-08T00:35:09.479637Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 00:47:30 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-12 00:47:30 INFO: Using device: cuda\n",
      "2023-04-12 00:47:30 INFO: Loading: tokenize\n",
      "2023-04-12 00:47:30 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to preprocess data\n",
      "data/msr_paraphrase_train_stanza.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 00:47:31 INFO: Loading: constituency\n",
      "2023-04-12 00:47:32 INFO: Done loading processors!\n",
      "2023-04-12 00:47:32 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-12 00:47:32 INFO: Using device: cuda\n",
      "2023-04-12 00:47:32 INFO: Loading: tokenize\n",
      "2023-04-12 00:47:32 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to process 0 to 2038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 00:47:32 INFO: Loading: constituency\n",
      "2023-04-12 00:47:33 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to process 2038 to 4076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 00:56:18 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-12 00:56:18 INFO: Using device: cuda\n",
      "2023-04-12 00:56:18 INFO: Loading: tokenize\n",
      "2023-04-12 00:56:18 INFO: Loading: pos\n",
      "2023-04-12 00:56:18 INFO: Loading: constituency\n",
      "2023-04-12 00:56:19 INFO: Done loading processors!\n",
      "2023-04-12 00:56:19 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-12 00:56:19 INFO: Using device: cuda\n",
      "2023-04-12 00:56:19 INFO: Loading: tokenize\n",
      "2023-04-12 00:56:19 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to process 0 to 2038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 00:56:20 INFO: Loading: constituency\n",
      "2023-04-12 00:56:20 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to process 2038 to 4076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 01:04:28 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-12 01:04:28 INFO: Using device: cuda\n",
      "2023-04-12 01:04:28 INFO: Loading: tokenize\n",
      "2023-04-12 01:04:28 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to process 8152 sentences is 0:16:57.662985\n",
      "about to write out data/msr_paraphrase_train_stanza.txt\n",
      "Done preprocessing data\n",
      "About to preprocess data\n",
      "data/msr_paraphrase_test_stanza.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 01:04:28 INFO: Loading: constituency\n",
      "2023-04-12 01:04:29 INFO: Done loading processors!\n",
      "2023-04-12 01:04:29 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-12 01:04:29 INFO: Using device: cuda\n",
      "2023-04-12 01:04:29 INFO: Loading: tokenize\n",
      "2023-04-12 01:04:29 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to process 0 to 862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 01:04:29 INFO: Loading: constituency\n",
      "2023-04-12 01:04:29 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to process 862 to 1725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 01:08:47 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-12 01:08:47 INFO: Using device: cuda\n",
      "2023-04-12 01:08:47 INFO: Loading: tokenize\n",
      "2023-04-12 01:08:47 INFO: Loading: pos\n",
      "2023-04-12 01:08:48 INFO: Loading: constituency\n",
      "2023-04-12 01:08:48 INFO: Done loading processors!\n",
      "2023-04-12 01:08:48 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-12 01:08:48 INFO: Using device: cuda\n",
      "2023-04-12 01:08:48 INFO: Loading: tokenize\n",
      "2023-04-12 01:08:48 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to process 0 to 862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 01:08:49 INFO: Loading: constituency\n",
      "2023-04-12 01:08:49 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to process 862 to 1725\n",
      "time to process 3450 sentences is 0:07:45.922631\n",
      "about to write out data/msr_paraphrase_test_stanza.txt\n",
      "Done preprocessing data\n",
      "About to preprocess data\n",
      "data/msr_paraphrase_train_stanza.txt\n",
      "data/msr_paraphrase_train_stanza.txt already exists\n",
      "Done preprocessing data\n",
      "Number of sentences processed in the String1 column: 10\n",
      "Number of sentences processed in the String2 column: 10\n"
     ]
    }
   ],
   "source": [
    "def test_dataset():\n",
    "    word2vec, max_sentence_length = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt')\n",
    "    dataset = MSPCDataset('data/msr_paraphrase_train.txt', word2vec, max_sentence_length, 10)\n",
    "    assert len(dataset) == 10\n",
    "\n",
    "    x1, y1 = dataset[0]\n",
    "    assert x1.shape[0] == 2\n",
    "    assert x1.shape[1] == max_sentence_length\n",
    "    assert x1.shape[2] == 50\n",
    "\n",
    "    # for set in dataset:\n",
    "    #     print(len(set[0]), len(set[1]))\n",
    "    # print(len(dataset[0][0]))\n",
    "    # print(dataset[0])\n",
    "test_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T14:32:14.561411Z",
     "end_time": "2023-04-07T14:32:14.897963Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataloaders\n",
    "Create training and test dataloaders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to preprocess data\n",
      "data/msr_paraphrase_train_stanza.txt already exists\n",
      "Done preprocessing data\n",
      "About to preprocess data\n",
      "data/msr_paraphrase_test_stanza.txt already exists\n",
      "Done preprocessing data\n",
      "About to preprocess data\n",
      "data/msr_paraphrase_train_stanza.txt already exists\n",
      "Done preprocessing data\n",
      "Number of sentences processed in the String1 column: 4076\n",
      "Number of sentences processed in the String2 column: 4076\n",
      "About to preprocess data\n",
      "data/msr_paraphrase_test_stanza.txt already exists\n",
      "Done preprocessing data\n",
      "Number of sentences processed in the String1 column: 1725\n",
      "Number of sentences processed in the String2 column: 1725\n"
     ]
    }
   ],
   "source": [
    "word2vec, max_sentence_length = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt')\n",
    "train_dataset = MSPCDataset('data/msr_paraphrase_train.txt', word2vec, max_sentence_length)\n",
    "test_dataset = MSPCDataset('data/msr_paraphrase_test.txt', word2vec, max_sentence_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 3.63414316e-03,  5.48852012e-02, -3.68031161e-03,\n        -6.79941755e-03, -5.16677424e-02, -1.08867206e-01,\n         1.10616349e-01,  9.42443013e-02, -1.14618227e-01,\n        -3.30350250e-02, -5.65201323e-03, -6.79125935e-02,\n        -2.43657157e-02,  9.94733442e-03, -4.24002409e-02,\n         5.59655875e-02,  4.25746217e-02,  6.91236602e-03,\n        -1.31934986e-01, -1.13035485e-01, -6.25953963e-03,\n         1.09916583e-01,  1.58782274e-01, -1.37492120e-02,\n         3.47554944e-02,  3.92766781e-02,  1.29515110e-02,\n         3.03151296e-03, -1.02520771e-01, -3.01488023e-03,\n         2.64172275e-02, -3.69757228e-02, -2.55998261e-02,\n         7.33343186e-03, -5.19834980e-02,  4.42461781e-02,\n         2.47789808e-02, -6.04249910e-03,  4.53312835e-03,\n        -5.46494983e-02,  7.07153752e-02, -1.69684142e-02,\n        -3.98331601e-03,  3.28275412e-02,  1.90524653e-01,\n         1.45154493e-02, -3.36287804e-02, -5.81832603e-02,\n         3.87138128e-02,  2.32843179e-02],\n       [-5.14177531e-02,  1.55610621e-01, -2.89605409e-02,\n         2.69071274e-02, -1.19769827e-01, -3.72006208e-01,\n         4.10787880e-01,  4.07550901e-01, -4.77014959e-01,\n        -1.60419986e-01, -1.63350739e-02, -3.26336712e-01,\n        -2.19945684e-02, -1.43964393e-02, -1.57259941e-01,\n         1.52398363e-01,  2.35818863e-01, -1.91776529e-02,\n        -5.22156835e-01, -3.57128650e-01,  7.78630152e-02,\n         3.91087979e-01,  5.62322915e-01, -3.45834382e-02,\n         1.94956943e-01,  8.49514827e-02,  2.73454632e-03,\n         4.98175994e-02, -3.97591710e-01,  2.40817685e-02,\n         1.42364055e-01, -1.71857804e-01, -9.23518762e-02,\n         3.57558206e-02, -1.49299994e-01,  1.53328657e-01,\n         1.29108533e-01, -7.29054734e-02,  6.12729341e-02,\n        -2.76514947e-01,  2.53874928e-01, -1.06406122e-01,\n        -4.49001417e-02,  7.32716098e-02,  7.05070734e-01,\n         1.34963214e-01, -5.80711663e-02, -2.10303292e-01,\n         1.85080081e-01,  1.33466721e-01],\n       [-2.26296647e-03,  5.65390140e-02, -8.17040540e-03,\n         8.00699834e-03, -4.27835211e-02, -1.44575223e-01,\n         1.56893373e-01,  1.42016992e-01, -1.43423751e-01,\n        -6.16012290e-02,  1.40607182e-04, -1.10277221e-01,\n        -3.14061195e-02, -2.46267417e-03, -6.04715049e-02,\n         6.51864111e-02,  1.03519626e-01,  1.10457093e-02,\n        -1.73659727e-01, -1.24214716e-01,  1.08759198e-02,\n         1.29220977e-01,  1.98580742e-01, -4.34626490e-02,\n         7.08722323e-02,  4.31957766e-02, -1.13199688e-02,\n         2.21401621e-02, -1.50706261e-01,  6.49372768e-03,\n         2.61864141e-02, -7.80332983e-02, -4.42841724e-02,\n         1.35305235e-02, -6.75707459e-02,  3.33022065e-02,\n         3.29223685e-02, -3.77426110e-02,  2.15144586e-02,\n        -7.74827302e-02,  1.07780971e-01, -5.23984805e-02,\n        -3.18824723e-02,  1.14140110e-02,  2.65146971e-01,\n         3.97954173e-02, -1.90436430e-02, -6.74332231e-02,\n         5.85814603e-02,  4.34288047e-02],\n       [-9.89795253e-02,  2.76717603e-01, -1.03589073e-02,\n         2.80938037e-02, -2.02911437e-01, -6.47053778e-01,\n         7.22692668e-01,  7.23863244e-01, -7.78930247e-01,\n        -2.80264109e-01, -9.22008511e-03, -5.48746347e-01,\n        -7.00101703e-02,  1.29831219e-02, -2.76245773e-01,\n         2.71950841e-01,  4.03994143e-01, -2.35472191e-02,\n        -8.66414011e-01, -6.43028021e-01,  8.03567842e-02,\n         6.58612609e-01,  9.46374953e-01, -1.30451500e-01,\n         3.53419811e-01,  1.42824754e-01,  1.35350367e-02,\n         4.06339988e-02, -6.68655396e-01,  1.12412162e-02,\n         2.17540592e-01, -2.92689055e-01, -1.37350276e-01,\n         7.81186298e-02, -2.48016685e-01,  2.37483203e-01,\n         2.39747271e-01, -9.65996981e-02,  9.32956710e-02,\n        -4.61252630e-01,  4.48567778e-01, -1.82517365e-01,\n        -4.99982536e-02,  1.14669181e-01,  1.20942295e+00,\n         2.16832876e-01, -9.20702219e-02, -3.90735030e-01,\n         2.83442050e-01,  2.67374307e-01],\n       [-2.11460851e-02,  6.12728782e-02,  1.47613361e-02,\n         1.07749477e-02, -7.32194930e-02, -1.82277441e-01,\n         1.81900084e-01,  1.93298668e-01, -1.80102110e-01,\n        -5.68474457e-02, -1.75267290e-02, -1.44783258e-01,\n        -2.18874048e-02, -2.43757889e-02, -8.96806121e-02,\n         6.13794290e-02,  8.62253532e-02, -1.25296218e-02,\n        -2.04286054e-01, -1.63476765e-01,  1.33197829e-02,\n         1.55456662e-01,  2.41360694e-01, -2.30783429e-02,\n         6.57362714e-02,  4.44268771e-02, -4.02603857e-03,\n         1.17804576e-02, -1.85648352e-01,  2.91813053e-02,\n         7.65075684e-02, -6.59268722e-02, -2.83758193e-02,\n         1.35949953e-02, -5.75372800e-02,  4.93714623e-02,\n         6.44423887e-02, -2.43806560e-02,  4.20140363e-02,\n        -1.08518369e-01,  9.34250802e-02, -5.75873516e-02,\n        -1.02539835e-02,  3.74051929e-02,  3.00966531e-01,\n         4.44942154e-02, -2.80938335e-02, -7.95272738e-02,\n         8.33490491e-02,  7.49639869e-02],\n       [-4.30600718e-03, -6.14957069e-04,  1.51109900e-02,\n         1.40251149e-03, -1.41358664e-02, -1.21637117e-02,\n         2.69544963e-02,  2.21050866e-02, -2.37346441e-03,\n        -2.33087242e-02,  1.10647380e-02, -2.23391149e-02,\n        -1.33250877e-02, -1.81532726e-02, -2.35081334e-02,\n        -5.93363447e-03, -6.47243392e-03,  1.26177808e-02,\n        -8.64609797e-03, -5.61326160e-04,  8.84205755e-03,\n         2.38191187e-02,  1.36704044e-02, -1.89401116e-02,\n        -2.53508403e-03,  2.35610325e-02,  6.45167194e-03,\n        -1.35639438e-03,  5.38132666e-03,  1.03005953e-02,\n         1.36974351e-02,  5.87525917e-03,  1.12905968e-02,\n         2.74297129e-03,  6.20822096e-03,  9.63238347e-03,\n         1.86747331e-02, -2.24344488e-02,  1.32163735e-02,\n        -1.87822990e-02,  2.52959710e-02,  1.50766550e-02,\n        -8.34284257e-03, -4.11239685e-03,  2.92708687e-02,\n         1.76426396e-02, -3.05006839e-03, -9.23542865e-03,\n         2.33867019e-02, -1.35598863e-02],\n       [-5.54804355e-02,  1.44835830e-01,  7.76246004e-03,\n         1.00257080e-02, -1.23014495e-01, -3.69148910e-01,\n         4.06483769e-01,  4.27201152e-01, -4.54686612e-01,\n        -1.75354034e-01, -1.67785585e-02, -3.22307467e-01,\n        -4.74994220e-02, -2.39398424e-02, -1.68254301e-01,\n         1.62491366e-01,  2.30327547e-01, -2.82774232e-02,\n        -4.95105267e-01, -3.82008523e-01,  6.20796606e-02,\n         3.59757066e-01,  5.49364924e-01, -8.60125124e-02,\n         1.81551784e-01,  9.55552459e-02, -1.10902893e-03,\n         5.09124668e-03, -3.56384784e-01,  3.49430554e-02,\n         1.46168321e-01, -1.73499867e-01, -5.80835603e-02,\n         3.86551470e-02, -1.36204496e-01,  1.50127366e-01,\n         1.20404884e-01, -4.40153144e-02,  3.87371667e-02,\n        -2.62562692e-01,  2.47240037e-01, -9.70008597e-02,\n        -6.81357458e-03,  7.28548616e-02,  6.94328606e-01,\n         1.54115394e-01, -6.67066276e-02, -1.97753191e-01,\n         1.44323543e-01,  1.57852307e-01]], dtype=float32)"
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filters out all zero filled columns to get the proper number of words in a sentence\n",
    "#train_dataset[0][0][~np.all(train_dataset[0][0][0] == 0, axis=1)]\n",
    "x = train_dataset[0][0][0].numpy()\n",
    "x[~np.all(x == 0, axis=1)]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [],
   "source": [
    "def conv_output_volume(W, F, S, P):\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: Given the input volume size $W$, the kernel/filter size $F$,\n",
    "    the stride $S$, and the amount of zero padding $P$ used on the border,\n",
    "    calculate the output volume size.\n",
    "    Note the output should a integer.\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    #https://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "    return int((W-F+2*P)/S+1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T14:32:15.557166Z",
     "end_time": "2023-04-07T14:32:15.560063Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(conv_output_volume(50, 3, 1, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T14:32:18.231576Z",
     "end_time": "2023-04-07T14:32:18.377984Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pooling Layers\n",
    "\n",
    "The original paper used a dynamic k-max pooling method in their model. The _k_ value is determine by equation (1).\n",
    "\n",
    "\\begin{equation*} k=\\max \\left({k_{top},\\left \\lceil{ \\frac {L-l}{L} \\left |{ s }\\right | }\\right \\rceil }\\right)\\end{equation*}\n",
    "\n",
    "__Dynamic K-Max Pooling Implementation:__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [],
   "source": [
    "# #https://gist.github.com/anna-hope/7a2b2e66c3645aa8e4f94dbf06aed8dc\n",
    "# \"\"\"\n",
    "# TODO: Remove hardcoded values and max it dynamic.\n",
    "# \"\"\"\n",
    "# class DynamicKMaxPooling(nn.Module):\n",
    "#     def __init__(self, k_init, conv_layers, layer):\n",
    "#         super().__init__()\n",
    "#         # \"L is the total  number  of  convolutional  layers\n",
    "#         # in  the  network;\n",
    "#         # ktop is the fixed pooling parameter for the\n",
    "#         # topmost  convolutional  layer\"\n",
    "#         self.k_init = k_init\n",
    "#         self.conv_layers = conv_layers\n",
    "#         self.layer = layer\n",
    "#\n",
    "#     def forward(self, X):\n",
    "#         s = 50\n",
    "#         dyn_k = ((self.conv_layers - self.layer) / self.conv_layers) * 3\n",
    "#         k_max = int(round(max(self.k_init, np.ceil(dyn_k))))\n",
    "#         print(k_max)\n",
    "#         out = F.max_pool1d(X, kernel_size=k_max)\n",
    "#         return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T15:50:59.023740Z",
     "end_time": "2023-04-07T15:50:59.325715Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence Similarity Convolution Network (SSCN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pooling Layers\n",
    "\n",
    "The original paper used a dynamic k-max pooling method in their model. The _k_ value is determine by equation (1).\n",
    "\n",
    "\\begin{equation*} k=\\max \\left({k_{top},\\left \\lceil{ \\frac {L-l}{L} \\left |{ s }\\right | }\\right \\rceil }\\right)\\end{equation*}\n",
    "\n",
    "__Dynamic K-Max Pooling Implementation:__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [],
   "source": [
    "#https://gist.github.com/anna-hope/7a2b2e66c3645aa8e4f94dbf06aed8dc\n",
    "\"\"\"\n",
    "TODO: Remove hardcoded values and max it dynamic.\n",
    "\"\"\"\n",
    "class DynamicKMaxPooling(nn.Module):\n",
    "    def __init__(self, k_init, conv_layers):\n",
    "        super().__init__()\n",
    "        # \"L is the total  number  of  convolutional  layers\n",
    "        # in  the  network;\n",
    "        # ktop is the fixed pooling parameter for the\n",
    "        # topmost  convolutional  layer\"\n",
    "        self.k_init = k_init\n",
    "        self.conv_layers = conv_layers\n",
    "\n",
    "    def pool(self, X, l):\n",
    "        # s is sequence length\n",
    "        # l is current layer in network\n",
    "        s = X.shape[2]\n",
    "        dyn_k = ((self.conv_layers - l) / self.conv_layers) * s\n",
    "        k_max = int(round(max(self.k_init, np.ceil(dyn_k))))\n",
    "        return F.max_pool1d(X, kernel_size=k_max)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer_i in range(self.conv_layers,0,-1):\n",
    "            X = self.pool(X, layer_i)\n",
    "\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.384185Z",
     "end_time": "2023-04-08T00:35:09.528064Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing Dynamic K-Max Pooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [],
   "source": [
    "NUM_OF_SAMPLES = 20\n",
    "SAMPLE_SIZE = 2\n",
    "OUTPUT_SIZE = 15\n",
    "\n",
    "test_embedding = torch.rand((NUM_OF_SAMPLES, SAMPLE_SIZE, OUTPUT_SIZE))\n",
    "dyn_k_layer = DynamicKMaxPooling(3, SAMPLE_SIZE)\n",
    "\n",
    "# Call forward with convolution layer index [2,1]\n",
    "out = dyn_k_layer(test_embedding)\n",
    "\n",
    "assert out.shape[2] == 1\n",
    "assert out.shape[1] == SAMPLE_SIZE\n",
    "assert out.shape[0] == NUM_OF_SAMPLES"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.384376Z",
     "end_time": "2023-04-08T00:35:09.528155Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sentence Similarity\n",
    "\n",
    " \\begin{align*} Man(\\vec V_{x}, \\vec V_{y})=&\\left |{ x_{1}-y_{1} }\\right |\\! +\\! \\left |{ x_{2}-y_{2} }\\right | \\!+ \\!\\ldots \\!+ \\!\\left |{ x_{n}-y_{n} }\\right |\n",
    " \\\\ score=&e^{-Man(\\vec V_{x}, \\vec V_{y})},\\quad score\\in [{0,1}] \\end{align*}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* X: Pooled output of SSCN model of shape (sample_size, -1)\n",
    "* For the purpose of this experiment sample_size = 2\n",
    "\"\"\"\n",
    "def manhattan_similarity_score(X):\n",
    "    sample_count, _, M = X.shape\n",
    "    Vx = X[:,0].reshape((sample_count,M))\n",
    "    Vy = X[:,1].reshape((sample_count,M))\n",
    "    mdist = torch.sum(torch.abs(Vx-Vy),dim=1).view(sample_count,-1)\n",
    "    score = torch.exp(-1*mdist)\n",
    "    return score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.385728Z",
     "end_time": "2023-04-08T00:35:09.528218Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "outputs": [],
   "source": [
    "class SentenceSimilarityCNN(nn.Module):\n",
    "    def __init__(self, max_sentence_size, stride=1, kernel_size=3, padding=1):\n",
    "        super(SentenceSimilarityCNN, self).__init__()\n",
    "        self.max_sentence_size = max_sentence_size\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=2, kernel_size=self.kernel_size, padding=self.padding)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        # TODO: change this to k-max pooling\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "        output_volume = conv_output_volume(50, self.kernel_size, self.padding, self.stride)\n",
    "        print(f\"output_volume = {output_volume}\")\n",
    "        self.fc = nn.Linear(in_features=14336, out_features=100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"shape before convolution {x.shape}\")\n",
    "        x = self.conv1(x)\n",
    "        print(f\"shape after convolution {x.shape}\")\n",
    "        x = self.activation(x)\n",
    "        x = self.pooling(x)\n",
    "        print(f\"shape after pooling {x.shape}\")\n",
    "        #x = self.fc(x)\n",
    "        #x = x.reshape(2, self.max_sentence_size)\n",
    "        permute_x = torch.permute(x, (1, 0, 2, 3))\n",
    "        x1 = permute_x[0]\n",
    "        x2 = permute_x[1]\n",
    "\n",
    "        x_prime = self.fc(x)\n",
    "        print(x_prime.shape)\n",
    "        # print(x1.shape)\n",
    "        # print(x2.shape)\n",
    "        # the p parameter below is the p-norm; p=1 is manhattan distance\n",
    "        #man_dist = torch.cdist(x1, x2, p=1)\n",
    "        sentence1_mean = torch.mean(x1, axis=1)\n",
    "        sentence2_mean = torch.mean(x2, axis=1)\n",
    "        man_dist = torch.sum(torch.abs(sentence1_mean - sentence2_mean), axis=1)\n",
    "        # print(man_dist.shape)\n",
    "        return torch.exp(-man_dist)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_volume = 50\n",
      "shape before convolution torch.Size([64, 2, 22, 50])\n",
      "shape after convolution torch.Size([64, 2, 22, 50])\n",
      "shape after pooling torch.Size([64, 2, 7, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (896x16 and 14336x100)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[324], line 27\u001B[0m\n\u001B[0;32m     23\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: curr_epoch_loss=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mmean(curr_epoch_loss)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n\u001B[1;32m---> 27\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[324], line 10\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(train_loader, n_epochs)\u001B[0m\n\u001B[0;32m      8\u001B[0m curr_epoch_loss \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence_pairs, y \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m---> 10\u001B[0m     y_hat \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentence_pairs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m# print(\"training\")\u001B[39;00m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m# print(y_hat)\u001B[39;00m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;66;03m# print(y)\u001B[39;00m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;66;03m# print(\"======\")\u001B[39;00m\n\u001B[0;32m     15\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(y_hat, y\u001B[38;5;241m.\u001B[39mfloat())\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\final-project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[323], line 32\u001B[0m, in \u001B[0;36mSentenceSimilarityCNN.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     29\u001B[0m x1 \u001B[38;5;241m=\u001B[39m permute_x[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     30\u001B[0m x2 \u001B[38;5;241m=\u001B[39m permute_x[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m---> 32\u001B[0m x_prime \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(x_prime\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# print(x1.shape)\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m# print(x2.shape)\u001B[39;00m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# the p parameter below is the p-norm; p=1 is manhattan distance\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m#man_dist = torch.cdist(x1, x2, p=1)\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\final-project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\final-project\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (896x16 and 14336x100)"
     ]
    }
   ],
   "source": [
    "model = SentenceSimilarityCNN(train_dataset.get_max_sentence_length())\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "n_epochs=50\n",
    "def train(train_loader, n_epochs=n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        curr_epoch_loss = []\n",
    "        for sentence_pairs, y in train_loader:\n",
    "            y_hat = model(sentence_pairs)\n",
    "            # print(\"training\")\n",
    "            # print(y_hat)\n",
    "            # print(y)\n",
    "            # print(\"======\")\n",
    "            loss = criterion(y_hat, y.float())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "\n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train(train_dataloader)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8588, 1.0000, 0.7812, 0.5668, 0.6405, 0.6734, 0.6828, 0.8812, 0.7834,\n",
      "        0.5719, 0.3877, 0.6191, 0.5482, 0.6603, 0.7332, 0.5343, 0.5461, 0.8182,\n",
      "        0.4273, 0.7210, 0.6237, 0.9475, 0.8669, 0.5546, 0.5876, 0.6561, 0.9435,\n",
      "        0.6570, 0.6492, 0.6918, 0.7740, 0.5517, 0.7816, 0.6981, 0.6710, 0.6562,\n",
      "        0.5267, 0.8267, 0.8072, 0.9235, 0.7557, 0.7782, 0.5443, 0.8632, 0.3880,\n",
      "        0.7220, 0.7140, 0.7749, 0.7321, 0.8115, 0.6356, 0.6241, 0.5230, 0.4469,\n",
      "        0.6363, 0.5721, 0.6978, 0.4932, 0.8411, 0.6681, 0.6899, 0.8395, 0.7494,\n",
      "        0.6779], grad_fn=<ExpBackward0>)\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0], dtype=torch.int32)\n",
      "tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "def eval_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    for sentence_pairs, y in test_dataloader:\n",
    "        y_hat = model(sentence_pairs)\n",
    "        print(y_hat)\n",
    "        y_pred = torch.zeros(y_hat.shape)\n",
    "        y_pred = (y_hat > 0.7).int()\n",
    "        print(y_pred)\n",
    "        print(y)\n",
    "        break\n",
    "\n",
    "eval_model(model, train_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " bn_Testing Similarity Scoring Function_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "NUM_OF_SAMPLES = 20\n",
    "SAMPLE_SIZE = 2\n",
    "OUTPUT_SIZE = 6\n",
    "\n",
    "test_embedding = torch.rand((NUM_OF_SAMPLES, SAMPLE_SIZE, OUTPUT_SIZE))\n",
    "scores = manhattan_similarity_score(test_embedding)\n",
    "\n",
    "assert scores.shape == (NUM_OF_SAMPLES, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.386951Z",
     "end_time": "2023-04-08T00:35:09.528280Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "class SSCN(nn.Module):\n",
    "    def __init__(self, sample_size, stride=1, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.conv_layers =sample_size\n",
    "\n",
    "        #NN layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.conv_layers, out_channels=self.conv_layers, \\\n",
    "                               kernel_size=self.kernel_size, padding=self.padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.conv_layers, out_channels=self.conv_layers, \\\n",
    "                               kernel_size=self.kernel_size, padding=self.padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool1 = DynamicKMaxPooling(self.kernel_size, self.conv_layers)\n",
    "\n",
    "        self.sscn = nn.Sequential(self.conv1, self.relu1, self.conv2, self.relu2, self.pool1)\n",
    "\n",
    "    \"\"\"\n",
    "    * X: Pooled output of SSCN model of shape (sample_size, -1)\n",
    "    * For the purpose of this experiment sample_size = 2\n",
    "    \"\"\"\n",
    "    def manhattan_similarity_score(self, X):\n",
    "        score = manhattan_similarity_score(X)\n",
    "        return score\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X = self.conv1(X)\n",
    "        # print(X.shape)\n",
    "        # X = self.relu1(X)\n",
    "        # print(X.shape)\n",
    "        # X = self.conv1(X)\n",
    "        # print(X.shape)\n",
    "        # X = self.relu2(X)\n",
    "        # print(X.shape)\n",
    "        # X = self.pool1(X)\n",
    "        # print(X.shape)\n",
    "        # X = self.manhattan_similarity_score(X)\n",
    "        # print(X.shape)\n",
    "        X = self.manhattan_similarity_score(self.sscn(X))\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.387486Z",
     "end_time": "2023-04-08T00:35:09.528347Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Testing:__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSCN(\n",
      "  (conv1): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (relu2): ReLU()\n",
      "  (pool1): DynamicKMaxPooling()\n",
      "  (sscn): Sequential(\n",
      "    (0): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (3): ReLU()\n",
      "    (4): DynamicKMaxPooling()\n",
      "  )\n",
      ")\n",
      "torch.Size([20, 1])\n"
     ]
    }
   ],
   "source": [
    "NUM_OF_SAMPLES = 20\n",
    "SAMPLE_SIZE = 2\n",
    "UNIQUE_FEATURES = 18\n",
    "\n",
    "test_embedding = torch.rand((NUM_OF_SAMPLES, SAMPLE_SIZE, UNIQUE_FEATURES))\n",
    "\n",
    "model = SSCN(SAMPLE_SIZE)\n",
    "# shape (batch,sample,sentence,word)?\n",
    "print(model)\n",
    "\n",
    "out = model(test_embedding)\n",
    "\n",
    "assert out.shape[0] == NUM_OF_SAMPLES\n",
    "assert out.shape[1] == 1\n",
    "print(out.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.388774Z",
     "end_time": "2023-04-08T00:35:09.528518Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
