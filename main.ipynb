{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility Project\n",
    "CS598 Deep Learning for Healthcare - Spring 2023\n",
    "\n",
    "Orignal Paper: [Disease Prediction and Early Intervention System Based on Symptom Similarity Analysis](https://ieeexplore.ieee.org/document/8924757)<br>\n",
    "Reproduction Paper (DRAFT): [Reproducibility Project for CS598 DL4H in Spring 2023](https://drive.google.com/file/d/1OlaeqqWVPh9-TqqRmcs1-KRbOKmktVPi/view?usp=sharing)\n",
    "\n",
    "*Note: Reproduction Paper must be accessed using UIUC email address (i.e, @illinois.edu).*\n",
    "\n",
    "_Contributors:_\n",
    " * Michael Pettenato - mp34@illinois.edu\n",
    " * Adam Michalsky - adamwm3@illinois.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "When physicians work with their patients, they often start by listening to their patient's symptom statements. The physcian will then map the patient's sympton statements to the symptoms that have been cataloged by the healthcare industry. This process or task of assess similarity between two sentences (e.g., A patient's symptom statement _and_ The cataloged symptom) is a task that can be easily mapped into computing.\n",
    "\n",
    "Sentence similarity is a task that has had significant research done on it already. Deep learning models were developed to perform this task in the healthcare industry already but each model has its pros and cons. One of the most common traits of models that assess similarity is the training time required, which is what the original paper (cited above) aimed to solve using an approach comprised of leveraging a Stanford Parser, Word2Vec embedding, and a convolutional neural network (CNN) based model.  \n",
    "\n",
    "The original paper did not have a documented repository, but did offer pseudo-code for certain aspects of the experiment. The following notebook is a reproduction of the original paper with some ablations that will be called out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T12:33:35.384649Z",
     "start_time": "2023-04-23T12:33:24.775447Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gensim\n",
    "import numpy as np\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import torch, stanza,spacy\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from enum import Enum\n",
    "from gensim.models import Word2Vec\n",
    "import threading\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Data\n",
    "\n",
    "The data that will be used to train and test is sourced from [Microsoft Research Paraphrase Corpus](https://www.microsoft.com/en-us/download/details.aspx?id=52398). \n",
    "\n",
    "\n",
    "### Helper Functions\n",
    "\n",
    "Below we define helper functions for ingesting the data used in the experiments.\n",
    "* `read_file` - This will be used to read in local files containing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    # Note: Unable to use pd.read_csv... the function complained about an issue with the formatting of the tsv file\n",
    "    # train = pd.read_csv('data/msr_paraphrase_train.txt', sep='\\t', encoding='latin1')\n",
    "    # train\n",
    "\n",
    "    # opting to read file in and split columns manually to create a pandas dataframe\n",
    "    list = []\n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            fields = line.split('\\t')\n",
    "            list.append(fields)\n",
    "\n",
    "    df = pd.DataFrame(list[1:], columns=['Quality', 'ID1', 'ID2', 'String1', 'String2'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "The approach described in the original paper can be described in three seperate _stages_. \n",
    "\n",
    "1) **Data Pre-Processing** - Here we will read in the raw data, parse it using the described algorithm, and prepare the parsed data by performing embedding using `Word2Vec`.\n",
    "\n",
    "2) **Neural Network** - Here we build the CNN-based model described in the original paper. This CNN will take the vectorized sentences pairs as inputs and return a similarity assesment [0,1].  No specifics of network architecture were offered in the original paper so the architecture is based on the our own knowledge.\n",
    "\n",
    "3) **Training & Validation** - Here we train and test the model using the data and network established in the previous stages.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "<div>\n",
    "    <figure>\n",
    "        <img src=\"./images/overview.png\" width=\"500\"/>\n",
    "        <figcaption align='center'>Figure 1: Overview of the Approach</figcaption>\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "In data pre-processing, perform two critical functions:\n",
    "\n",
    "   1. Read in and parse the files stored `.\\data` directory.\n",
    "   2. Embed the parsed sentences using `Word2Vec` embedding.\n",
    "\n",
    "Data parsed is the stored in a custom dataset using PyTorch class `Dataset`. Our implementation is `MSPCDataset`. This will allow us to take advantage PyTorch's `DataLoader` functionality, which has data batching facilities.  The `MSPCDataset` produced an output format shown in the equation below.   \n",
    "\n",
    "\\begin{equation} \n",
    "Item=(Sent1, Sent2, SimFlag)\n",
    "\\label{eq:Item}\n",
    "\\end{equation}\n",
    "\n",
    "$Sent1$ and $Sent2$ have a format like $Sen$, discussed in the *Embedding with `Word2Vec`* section below, and $SimFlag$ is a binary target value, `0` being not similar and `1` being similar.\n",
    "\n",
    "### Parsing \n",
    "<br>\n",
    "\n",
    "The original paper used the Stanford Parser to find the parts of speech for each word in a sentence. This parser has been deprecated in favor of the `stanza` parser, so we used this as a replacement for the Stanford parser.\n",
    "\n",
    "\n",
    "_Ablation 1:_ After reviewing the output of the `stanza` parser, we found that we did not agree with the subject, predicate, and object (SPO) result 100% of the time. Therefore the decision was made to support alternative parsing approaches. In addition to `stanza`, we have introduced support for two more parsing methods:\n",
    "* `spaCy` - Documentation and install instructions can be found at their [site](https://spacy.io/usage).\n",
    "* `2 Char Stop Word Removal` - A method where stop words less than 3 characters in length are not considered while using `spaCy` parsing. \n",
    "\n",
    "To allow flexibility in coding we have defined enum values for each parse method:\n",
    " * `STANZA` - Stanford Parser(`stanza`)\n",
    " * `SPACY` - spaCy Parser\n",
    " * `RAW` - Raw sentences are embedded using `Word2Vec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parsing(Enum):\n",
    "    STANZA=1\n",
    "    SPACY=2\n",
    "    RAW=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stanford Parser (`stanza`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the Stanford Parser was what was used by the original paper to parse its raw sentences so we were able to reproduce the custom logic used to parse the tree produced by the parser. For ease of use, we implemented a function designed to extract _subject(s)_, _predicate(s)_, and _object(s)_ from an input parse tree (Figure 2). Going forward this will be reffered to as a _SPO Kernel Function_.\n",
    "\n",
    "\n",
    "<div>\n",
    "    <figure>\n",
    "        <img src=\"./images/parsetree.png\" width=\"500\"/>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In in **Section III** of the original paper, the _SPO Kernel Function_ is discussed along with some pseudo-code.  We obsertved that the paper had discrepancies, where the pseudo-code did not match the textual description, found in the trunk construction algorithm.  We implemented our parsing algorithm according to the textual description, which was more detailed than the pseudo-code provided.\n",
    "\n",
    "###### Helper Functions\n",
    "\n",
    "Below we haved defined several functions that aid in finding the SPO provided a parsed sentence tree obtained from a `stanza` parser. \n",
    "\n",
    "* `find_branches()` - Extract phrases from a provided tree given a list of labels to include and exclude.\n",
    "* `find_subject()` - Locate a _subject_ given a noun phrase.\n",
    "    * _subject_ - Any `NN` child found in a noun phrase extracted from the parse tree.\n",
    "* `find_predicate()` - Locate a _predicate_ given a verb phrase.\n",
    "    * _predicate_ - Any `VB`child found in a verb phrase extracted from the parse tree.\n",
    "* `find_object()` - Locate an _object_ given a verb phrase.\n",
    "   * _object_ - Any `NN` child of a `NP`, `PP`, or `ADJP`object OR any child object that is not a `VP`found in the verb phrase extracted from the parse tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:03.008883Z",
     "start_time": "2023-04-08T00:35:01.890215Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_branches(tree, label, not_in_label=None, ancestors=[]):\n",
    "    branches = []\n",
    "    # print(\"-------------\")\n",
    "    # print(ancestors)\n",
    "    # print(f\"{tree.label} == {label}\")\n",
    "    if tree.label == label and not_in_label not in ancestors:\n",
    "        # print(f\"adding {tree}\")\n",
    "        branches.append(tree)\n",
    "    for child in tree.children:\n",
    "        branches = branches + find_branches(child, label, not_in_label, ancestors + [tree.label])\n",
    "\n",
    "    return branches\n",
    "\n",
    "#\n",
    "# # According to the paper the subject is the first NN child of NP\n",
    "def find_subject(noun_phrase_for_subject):\n",
    "    subject = []\n",
    "    for child in noun_phrase_for_subject.children:\n",
    "        if 'NN' in child.label:\n",
    "            subject = subject + child.leaf_labels()\n",
    "\n",
    "    #print(f\"subject = {subject}\")\n",
    "    #if len(subject) > 0:\n",
    "    #    return ' '.join(subject)\n",
    "    return subject\n",
    "\n",
    "    return None\n",
    "\n",
    "def find_predicate(verb_phrase_for_predicate):\n",
    "    predicate = []\n",
    "    for child in verb_phrase_for_predicate.children:\n",
    "        if child.label.startswith('VB'):\n",
    "            predicate = predicate + child.leaf_labels()\n",
    "\n",
    "    if len(predicate) > 0:\n",
    "        return ' '.join(predicate)\n",
    "\n",
    "    return None\n",
    "\n",
    "def find_object(verb_phase_for_object, parent='VP'):\n",
    "    objects = []\n",
    "    for child in verb_phase_for_object.children:\n",
    "        if child.label == 'VP':\n",
    "            continue\n",
    "        if 'NN' in child.label and parent in ['NP', 'PP', 'ADJP']:\n",
    "            #objects = objects + child.leaf_labels()\n",
    "            new_objects = child.leaf_labels()\n",
    "            for new_object in new_objects:\n",
    "                if new_object not in objects:\n",
    "                    objects.append(new_object)\n",
    "        else:\n",
    "            new_objects = find_object(child, child.label)\n",
    "            #if new_objects not in objects and new_objects is not None:\n",
    "            for new_object in new_objects:\n",
    "                if new_object not in objects:\n",
    "                    objects.append(new_object)\n",
    "                #objects = objects + new_objects\n",
    "\n",
    "    return objects\n",
    "    # if len(objects) > 0:\n",
    "    #     #return ' '.join(objects)\n",
    "    #     return objects\n",
    "    # else:\n",
    "    #     return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SPO Kernel Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:03.008883Z",
     "start_time": "2023-04-08T00:35:01.890215Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_spo(tree):\n",
    "    noun_phrases_for_subject = find_branches(tree, label='NP', not_in_label='VP', ancestors=[])\n",
    "    subject_list = []\n",
    "    for noun_phrase_for_subject in noun_phrases_for_subject:\n",
    "        subject = find_subject(noun_phrase_for_subject)\n",
    "        #if subject is not None:\n",
    "        #   subject_list.append(subject)\n",
    "        subject_list = subject_list + subject\n",
    "\n",
    "    verb_phrases = find_branches(tree, label='VP')\n",
    "    predicate_list = []\n",
    "    object_list = []\n",
    "    for verb_phrase in verb_phrases:\n",
    "        predicate = find_predicate(verb_phrase)\n",
    "        if predicate is not None:\n",
    "            predicate_list.append(predicate)\n",
    "        object = find_object(verb_phrase)\n",
    "        object_list = object_list + object\n",
    "        #if object is not None:\n",
    "        #    object_list.append(object)\n",
    "\n",
    "    # dedupe list\n",
    "    subject_list = list(dict.fromkeys(subject_list))\n",
    "    predicate_list = list(dict.fromkeys(predicate_list))\n",
    "    object_list = list(dict.fromkeys(object_list))\n",
    "\n",
    "    return subject_list, predicate_list, object_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaCy Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the Stanford Parser, a _SPO Function_ is implemented to traverse a sentence _tree_ or `doc` to locate the SPO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SPO Kernel Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_spacy_spo(doc):\n",
    "    # Extract the subject, predicate, and object\n",
    "    subject = []\n",
    "    predicate = []\n",
    "    obj = []\n",
    "\n",
    "    for token in doc:\n",
    "        #print(f\"{token.dep_} : {token.text}\")\n",
    "        if \"subj\" in token.dep_:\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            subject .append(doc[start:end])\n",
    "        elif \"obj\" in token.dep_ or \"pcomp\" in token.dep_:\n",
    "            obj.append(token.text)\n",
    "        elif \"ROOT\" in token.dep_ or \"pred\" in token.dep_:\n",
    "            predicate.append(token.text)\n",
    "\n",
    "    # Print the results\n",
    "    # print(\"Subject: \", subject)\n",
    "    # print(\"Predicate: \", predicate)\n",
    "    # print(\"Object: \", obj)\n",
    "    return subject, predicate, object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No specialized _SPO Kernel Function_ is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concurreny Support\n",
    "\n",
    "After implentation of the _SPO Functions_, the original paper began embedding a vector using `Word2Vec`. Before we started the embedding process, we decided to further improve the speed of parsing by implementing concurrency. \n",
    "\n",
    "_Ablation 2:_ We found the parts-of-speech parsing to be slow.  In order to speed it up we implemented a multi-threaded parsing class called `SentenceProcessingThread`.    \n",
    "\n",
    "This class instantiates a parser based on the enumerated `Parsing` types and performs the specified parsing method against the input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:08.941546Z",
     "start_time": "2023-04-08T00:35:08.905455Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentenceProcessingThread(threading.Thread):\n",
    "    def __init__(self, sentences, output_list, begin, end, parsing_enum=Parsing.STANZA):\n",
    "        super(SentenceProcessingThread, self).__init__()\n",
    "        self.sentences = sentences\n",
    "        self.parsing_enum = parsing_enum\n",
    "\n",
    "        if parsing_enum == Parsing.STANZA:\n",
    "            self.nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency', download_method=None, use_gpu=True)\n",
    "        else:\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.output_list = output_list\n",
    "        self.begin = begin\n",
    "        self.end = end\n",
    "\n",
    "    def trunk_construction(self, str, parent_label = None):\n",
    "        doc = self.nlp(str)\n",
    "        tree = doc.sentences[0].constituency\n",
    "\n",
    "        #words = construct_sentence(tree, parent_label)\n",
    "        #return ' '.join(words)\n",
    "\n",
    "        if self.parsing_enum == Parsing.SPACY:\n",
    "            subjects, predicates, objects = find_spacy_spo(tree)\n",
    "        else:\n",
    "            subjects, predicates, objects = find_spo(tree)\n",
    "\n",
    "        return f\"{' '.join(subjects)},{' '.join(predicates)},{' '.join(objects)}\"\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"going to process {self.begin} to {self.end}\")\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "            new_sentence = self.trunk_construction(sentence)\n",
    "            self.output_list[self.begin + i] = new_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we implement functions to facilitate parsing using `SentenceProcessingThread`. To further improve performance we added support to detect and using GPUs if GPUs are available on the host machine. If GPUs are unavailable, the parsing process will simply utilitze CPUs with the appropriate number of threads.\n",
    "\n",
    "* `process_sentences_concurrently` - Processes the provided `sentences` across `p` threads and stores the processed sentence in the `output`.\n",
    "\n",
    "* `preprocess_corpus` - Reads sentences in an `input_file` containing sentences to parse using the provided parsing method (`parsing_enum`) and saves the result in the `output_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:08.941546Z",
     "start_time": "2023-04-08T00:35:08.905455Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_sentences_concurrently(sentences, output, p=2):\n",
    "    total = len(sentences)\n",
    "    interval = int(total / p)\n",
    "    threads = []\n",
    "    for i in range(p):\n",
    "        s = i*interval\n",
    "        if i == p-1:\n",
    "            e = total\n",
    "        else:\n",
    "            e = (i+1) * interval\n",
    "        sentences_slice = sentences[s:e]\n",
    "        sentence_thread = SentenceProcessingThread(sentences_slice, output, s, e)\n",
    "        sentence_thread.start()\n",
    "        threads.append(sentence_thread)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "def preprocess_corpus(input_file='data/msr_paraphrase_train.txt', output_file='data/msr_paraphrase_train_stanza.txt', N=None, parsing_enum=Parsing.STANZA):\n",
    "    print(output_file)\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"{output_file} already exists\")\n",
    "        return\n",
    "\n",
    "    starttime = datetime.datetime.now()\n",
    "    df = read_file(input_file)\n",
    "\n",
    "    if N is None:\n",
    "        N = len(df.String1)\n",
    "\n",
    "    output1 = [None] * N\n",
    "    output2 = [None] * N\n",
    "\n",
    "    # we can process with more threads if we only have CPU\n",
    "    p = 8\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # if cuda is available we don't need that many threads\n",
    "        # and if the number of threads is set too large using cuda\n",
    "        # we can get out of memory exceptions\n",
    "        p = 2\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    process_sentences_concurrently(df.String1[:N], output1, p)\n",
    "\n",
    "    # try and be careful with gpu memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    process_sentences_concurrently(df.String2[:N], output2, p)\n",
    "\n",
    "    endtime = datetime.datetime.now()\n",
    "\n",
    "    print(f\"time to process {N*2} sentences is {endtime - starttime}\")\n",
    "\n",
    "    stanza_df = df[:N]\n",
    "\n",
    "    processed_string1 = pd.Series(output1)\n",
    "    # processed_string1.apply(gensim.utils.simple_preprocess)\n",
    "    processed_string2 = pd.Series(output2)\n",
    "    #processed_string2.apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "    stanza_df.String1 = processed_string1\n",
    "    stanza_df.String2 = processed_string2\n",
    "\n",
    "    # write the file out.  This can help in the future\n",
    "    print(f\"about to write out {output_file}\")\n",
    "    stanza_df.to_csv(output_file, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding with `Word2Vec`\n",
    "\n",
    "Each word in the sentence needs to be replaced by an embedding vector. Consistent with the information described in the original paper, we used Word2Vec, with an embedding size of 50 create sentence vectors.  The following equation describes a word $W$ in vector form:\n",
    "\n",
    "<br>\n",
    "\\begin{equation} \n",
    "W=(\\mathbf {w}^1, \\mathbf {w}^2,...,\\mathbf {w}^n)\n",
    "\\label{eq:Word}\n",
    "\\end{equation}\n",
    "<br>\n",
    "where $w$ represents an embedding vector and the superscript represents the sequence of words in a sentence.\n",
    "<br><br>\n",
    "The next equation shows how the sentence is further organized\n",
    "<br>\n",
    "\\begin{equation} \n",
    "Sen=(\\mathbf {S}^{\\mathrm {T}}, \\mathbf {P}^{\\mathrm {T}}, \\mathbf {O}^{\\mathrm {T}})\n",
    "\\label{eq:Sent}\n",
    "\\end{equation}\n",
    "<br>\n",
    "where $S^T$, $P^T$, $O^T$ represent the subject, predicate, and object words, all with a similar structure to $W$.\n",
    "\n",
    "\n",
    "The original paper did not provide details on how their `Word2Vec` model was trained. This is the motivation for our next ablation.\n",
    "\n",
    "_Ablation 3_: Due to the uncertainty of how the original paper's `Word2Vec` model was trained, we opted to try two different models:\n",
    "1) A Word2Vec model that is trained on MSRP corpus. \n",
    "2) A pre-trained Word2Vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-trained Model Details\n",
    "\n",
    "For our comparison, we chose to leverage one trained on Google News vectors from `gensim`. For further documentation please see their [website](https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "\n",
    "**WARNING:** The following code block is to download the pre-trained model mentioned above and it takes several minutes to complete!\n",
    "\n",
    "Please be advised that some default jupyter settings will need to be modified to ensure a successful download. We modfied the `c.NotebookApp.iopub_data_rate_limit` setting to 100000000.\n",
    "\n",
    "Instructions on how to create a configuration file and to modify the variable can be found [here](https://stackoverflow.com/questions/43490495/how-to-set-notebookapp-iopub-data-rate-limit-and-others-notebookapp-settings-in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "pretrained_word2vec_path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "print(pretrained_word2vec_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fake Word Embedding\n",
    "\n",
    "When using a pretrained word2vec model there may be some words that don't appear in the vocabulary for the pretrained model.  In our experiment this occured with people's names, which appeared in sentences from news articles.  There are a few ways to handle this, for instance you could ignore the word that does not appear in the vocabulary, or you could train word2vec with that additional information in the corpus. Due to the fact that the words that were missing were mostly firstnames or surnames, we decided to generate a fake word embedding.  The requirement of the word embedding was to be real numbers between -0.5 and 0.5 with a euclidean norm of the array equal to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_embeddings = {}\n",
    "def get_fake_embedding(word, size):\n",
    "\n",
    "    if word in fake_embeddings:\n",
    "        return fake_embeddings[word]\n",
    "\n",
    "    embedding = np.random.rand(300) - 0.5\n",
    "\n",
    "    # Calculate the Euclidean norm of the array\n",
    "    norm = np.linalg.norm(embedding)\n",
    "\n",
    "    embedding /= norm\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we implementation the following functions:\n",
    "\n",
    "* `generate_word2vec_model(corpus)` - Create a `Word2Vec` model given a `corpus`.\n",
    "* `sentence_embeddings(w2v_model, sentence, size)` - Embed a `sentence` of length `size` using a provided `Word2Vec` model, `w2vmodel`.\n",
    "* `corpus_embeddings(model, corpus, max_sentence_len)` - Embed a `corpus` with a maximum sentence length of `max_sentence_len` using a provided model, `model`. \n",
    "* `init_word2vec(train_input_file, test_input_file, parsing_enum)` - Provided `train_input_file`, `test_input_file`, and a parsing method identified by `parsing_enum`, this function will return a tuple containing a `Word2Vec` model trained on the MSRP corpus dataset along with the maximum sentence length, `max_sentence_length` for all the sentences in a corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:08.942658Z",
     "start_time": "2023-04-08T00:35:08.908525Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function is broken out for testing purposes\n",
    "def generate_word2vec_model(corpus):\n",
    "    # Creating the Word2Vec model\n",
    "    model = Word2Vec(sentences=corpus, min_count=1, window=2, vector_size=50)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:08.942887Z",
     "start_time": "2023-04-08T00:35:08.910846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function is broken out for testing purposes\n",
    "def sentence_embeddings(w2v_model, sentence, size):\n",
    "    np_embedding = np.zeros(size)\n",
    "    for i, word in enumerate(sentence):\n",
    "        #print(word)\n",
    "        if hasattr(w2v_model, \"wv\"):\n",
    "            np_embedding[i] = w2v_model.wv.get_vector(word)\n",
    "        else:\n",
    "            try:\n",
    "                np_embedding[i] = w2v_model.get_vector(word)\n",
    "            except KeyError:\n",
    "                print(f\"Word {word} was not found in the vocabulary\")\n",
    "                np_embedding[i] = get_fake_embedding(word, size)\n",
    "                print(\"Generated a fake embedding \", np_embedding[i])\n",
    "\n",
    "    return np_embedding\n",
    "    # list = []\n",
    "    # for word in sentence:\n",
    "    #     list.append(w2v_model.wv.get_vector(word))\n",
    "    #\n",
    "    # word_matrix = np.row_stack(list)\n",
    "    # #return np.mean(word_matrix, axis=0)\n",
    "    # return word_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:09.479438Z",
     "start_time": "2023-04-08T00:35:08.972122Z"
    }
   },
   "outputs": [],
   "source": [
    "def corpus_embeddings(model, corpus, max_sentence_len, embedding_size=50):\n",
    "    corpus_size = len(corpus)\n",
    "    embeddings_list = []\n",
    "    embedding_matrix = np.zeros((corpus_size, max_sentence_len, embedding_size))\n",
    "    for i, sentence in enumerate(corpus):\n",
    "        embeddings = sentence_embeddings(model, sentence, size=(max_sentence_len, embedding_size))\n",
    "        embedding_matrix[i] = embeddings\n",
    "        embeddings_list.append(embeddings)\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_word2vec(train_input_file, test_input_file, parsing_enum=Parsing.STANZA):\n",
    "\n",
    "    if parsing_enum == Parsing.RAW:\n",
    "        train_output_file = train_input_file\n",
    "        test_output_file = test_input_file\n",
    "        train_df = read_file(train_output_file)\n",
    "        test_df = read_file(test_output_file)\n",
    "\n",
    "    elif parsing_enum == Parsing.STANZA:\n",
    "        file_parts = os.path.splitext(train_input_file)\n",
    "        train_output_file = f\"{file_parts[0]}_stanza{file_parts[1]}\"\n",
    "        print(\"About to preprocess spacy data\")\n",
    "        preprocess_corpus(input_file=train_input_file, output_file=train_output_file, parsing_enum=parsing_enum)\n",
    "        print(\"Done preprocessing spacy data\")\n",
    "\n",
    "        file_parts = os.path.splitext(test_input_file)\n",
    "        test_output_file = f\"{file_parts[0]}_stanza{file_parts[1]}\"\n",
    "        print(\"About to preprocess data\")\n",
    "        preprocess_corpus(input_file=test_input_file, output_file=test_output_file, parsing_enum=parsing_enum)\n",
    "        print(\"Done preprocessing data\")\n",
    "        train_df = pd.read_csv(train_output_file, sep=\"\\t\")\n",
    "        test_df = pd.read_csv(test_output_file, sep=\"\\t\")\n",
    "    else:\n",
    "        file_parts = os.path.splitext(train_input_file)\n",
    "        train_output_file = f\"{file_parts[0]}_spacy{file_parts[1]}\"\n",
    "        print(\"About to preprocess spacy data\")\n",
    "        preprocess_corpus(input_file=train_input_file, output_file=train_output_file, parsing_enum=parsing_enum)\n",
    "        print(\"Done preprocessing spacy data\")\n",
    "\n",
    "        file_parts = os.path.splitext(test_input_file)\n",
    "        test_output_file = f\"{file_parts[0]}_spacy{file_parts[1]}\"\n",
    "        print(\"About to preprocess data\")\n",
    "        preprocess_corpus(input_file=test_input_file, output_file=test_output_file, parsing_enum=parsing_enum)\n",
    "        print(\"Done preprocessing data\")\n",
    "        train_df = pd.read_csv(train_output_file, sep=\"\\t\")\n",
    "        test_df = pd.read_csv(test_output_file, sep=\"\\t\")\n",
    "\n",
    "    # train_df = read_file(train)\n",
    "    # test_df = read_file(test)\n",
    "\n",
    "    train_sentences1 = train_df.String1.apply(gensim.utils.simple_preprocess)\n",
    "    train_sentences2 = train_df.String2.apply(gensim.utils.simple_preprocess)\n",
    "    test_sentences1 = test_df.String1.apply(gensim.utils.simple_preprocess)\n",
    "    test_sentences2 = test_df.String2.apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "    corpus = pd.concat([train_sentences1, train_sentences2, test_sentences1, test_sentences2], ignore_index=True)\n",
    "    max_sentence_len = corpus.apply(len).max()\n",
    "\n",
    "    word2vec = generate_word2vec_model(corpus)\n",
    "\n",
    "\n",
    "    return word2vec, max_sentence_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataset: `MSPCDataset`\n",
    "\n",
    "Our custom dataset is implmented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:09.479637Z",
     "start_time": "2023-04-08T00:35:08.981724Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset for the MSPC dataset\n",
    "class MSPCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        tsv_file (string): path to the tsv file with sentences to compare and associate quality score\n",
    "        num_records (int): number of records to load.  Defaults to None which is all\n",
    "    \"\"\"\n",
    "    def __init__(self, tsv_file, w2v_model, max_sentence_length, num_records=None, parsing_enum=Parsing.STANZA, embedding_size=50):\n",
    "\n",
    "        self.max_sentence_len = max_sentence_length\n",
    "        self.w2v_model = w2v_model\n",
    "\n",
    "        if parsing_enum == Parsing.STANZA:\n",
    "            file_parts = os.path.splitext(tsv_file)\n",
    "            output_file = f\"{file_parts[0]}_stanza{file_parts[1]}\"\n",
    "            print(\"About to preprocess stanza data\")\n",
    "            preprocess_corpus(input_file=tsv_file, output_file=output_file, parsing_enum=parsing_enum)\n",
    "            print(\"Done preprocessing stanza data\")\n",
    "            #df = read_file('data/msr_paraphrase_train.txt')\n",
    "            df = pd.read_csv(output_file, sep=\"\\t\")\n",
    "        elif parsing_enum == Parsing.SPACY:\n",
    "            file_parts = os.path.splitext(tsv_file)\n",
    "            output_file = f\"{file_parts[0]}_spacy{file_parts[1]}\"\n",
    "            print(\"About to preprocess spacy data\")\n",
    "            preprocess_corpus(input_file=tsv_file, output_file=output_file, parsing_enum=parsing_enum)\n",
    "            print(\"Done preprocessing spacy data\")\n",
    "            #df = read_file('data/msr_paraphrase_train.txt')\n",
    "            df = pd.read_csv(output_file, sep=\"\\t\")\n",
    "        else:\n",
    "            df = read_file(tsv_file)\n",
    "\n",
    "        if num_records is not None:\n",
    "            processed_string1 = df[:num_records].String1\n",
    "            processed_string2 = df[:num_records].String2\n",
    "            self.quality = df[:num_records].Quality\n",
    "        else:\n",
    "            processed_string1 = df.String1\n",
    "            processed_string2 = df.String2\n",
    "            self.quality = df.Quality\n",
    "\n",
    "        if parsing_enum == Parsing.RAW:\n",
    "            processed_string1 = processed_string1.apply(gensim.parsing.preprocessing.remove_stopwords)\n",
    "            processed_string2 = processed_string2.apply(gensim.parsing.preprocessing.remove_stopwords)\n",
    "            processed_string1 = processed_string1.apply(lambda x: gensim.parsing.preprocessing.strip_short(x, minsize=3))\n",
    "            processed_string2 = processed_string2.apply(lambda x: gensim.parsing.preprocessing.strip_short(x, minsize=3))\n",
    "\n",
    "        processed_string1 = processed_string1.apply(gensim.utils.simple_preprocess)\n",
    "        processed_string2 = processed_string2.apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "\n",
    "        print(processed_string1)\n",
    "\n",
    "\n",
    "        #corpus = pd.concat([processed_string1, processed_string2], ignore_index=True)\n",
    "        #self.max_sentence_len = corpus.apply(len).max()\n",
    "        #w2v_model = generate_word2vec_model(corpus)\n",
    "\n",
    "        sentence_embeddings1 = corpus_embeddings(self.w2v_model, processed_string1, max_sentence_len=self.max_sentence_len, embedding_size=embedding_size)\n",
    "        sentence_embeddings2 = corpus_embeddings(self.w2v_model, processed_string2, max_sentence_len=self.max_sentence_len, embedding_size= embedding_size)\n",
    "\n",
    "        #self.w2v_model = w2v_model\n",
    "        self.sentences_embeddings1 = sentence_embeddings1\n",
    "        self.sentences_embeddings2 = sentence_embeddings2\n",
    "\n",
    "        # print (f\"Processing 200 sentences with gensim.utils.simple_preprocess took {end_time - start_time}\")\n",
    "        print(f\"Number of sentences processed in the String1 column: {len(processed_string1)}\")\n",
    "        print(f\"Number of sentences processed in the String2 column: {len(processed_string2)}\")\n",
    "        #print(self.sentences_embeddings1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_embeddings1)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        #return torch.FloatTensor(np.stack((self.sentences_embeddings1[i], self.sentences_embeddings2[i]))), self.quality[i]\n",
    "        return torch.FloatTensor(self.sentences_embeddings1[i]), torch.FloatTensor(self.sentences_embeddings2[i]), self.quality[i]\n",
    "\n",
    "    def get_max_sentence_length(self):\n",
    "        return self.max_sentence_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing Validations\n",
    "\n",
    "<br>\n",
    "Below we are simply validating that the output is what expected when using either parsing method.\n",
    "<br>\n",
    "\n",
    "##### Stanford Parser (`stanza`) Validations\n",
    "\n",
    "\n",
    "_MSRP Corpus Trained Word2Vec:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T14:32:14.897963Z",
     "start_time": "2023-04-07T14:32:14.561411Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_dataset():\n",
    "    word2vec, max_sentence_length = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt', parsing_enum=Parsing.STANZA)\n",
    "    dataset = MSPCDataset('data/msr_paraphrase_train.txt', word2vec, max_sentence_length, 10)\n",
    "    assert len(dataset) == 10\n",
    "\n",
    "    x1, y1, quality = dataset[0]\n",
    "    assert x1.shape[0] == max_sentence_length\n",
    "    assert x1.shape[1] == 50\n",
    "\n",
    "test_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Pre-trained Word2Vec (Google News Vectors):_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_with_pretrained_word2vec():\n",
    "    word2vec, max_sentence_length = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt', parsing_enum=Parsing.STANZA)\n",
    "    print(\"Initiating a pretrained model for word2vec... this may take a while\")\n",
    "    #word2vec_pretrained = gensim.models.keyedvectors.load_word2vec_format('data/w2v/Google-news-vectors.bin.gz', binary=True)\n",
    "    word2vec_pretrained = gensim.models.keyedvectors.load_word2vec_format(pretrained_word2vec_path, binary=True)\n",
    "    print(\"Done initiating the pretrained model for word2vec\")\n",
    "    dataset = MSPCDataset('data/msr_paraphrase_train.txt', word2vec_pretrained, max_sentence_length, 10, embedding_size=300)\n",
    "    assert len(dataset) == 10\n",
    "    #\n",
    "    x1, y1, quality = dataset[0]\n",
    "    assert x1.shape[0] == max_sentence_length\n",
    "    assert x1.shape[1] == 300\n",
    "\n",
    "test_dataset_with_pretrained_word2vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SpaCy Parser Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_dataset_spacy_sentences():\n",
    "    word2vec_spacy, max_sentence_length_spacy = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt', parsing_enum=Parsing.SPACY)\n",
    "    dataset = MSPCDataset('data/msr_paraphrase_train.txt', word2vec_spacy, max_sentence_length_spacy, 10, parsing_enum=Parsing.SPACY)\n",
    "    assert len(dataset) == 10\n",
    "\n",
    "    x1, y1, quality = dataset[0]\n",
    "    assert x1.shape[0] == max_sentence_length_spacy\n",
    "    assert x1.shape[1] == 50\n",
    "\n",
    "test_dataset_spacy_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Raw Parser Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_dataset_raw_sentences():\n",
    "    word2vec_raw, max_sentence_length_raw = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt', Parsing.RAW)\n",
    "    dataset = MSPCDataset('data/msr_paraphrase_train.txt', word2vec_raw, max_sentence_length_raw, 10, parsing_enum=Parsing.RAW)\n",
    "    assert len(dataset) == 10\n",
    "\n",
    "    x1, y1, quality = dataset[0]\n",
    "    assert x1.shape[0] == max_sentence_length_raw\n",
    "    assert x1.shape[1] == 50\n",
    "\n",
    "test_dataset_raw_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders\n",
    "\n",
    "Create training and test dataloaders for sentences parsed with...\n",
    "<br>\n",
    "1. The stanza parts-of-speech parser and a Word2Vec model trained on the MSRP corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec, max_sentence_length = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt')\n",
    "\n",
    "train_dataset = MSPCDataset('data/msr_paraphrase_train.txt', word2vec, max_sentence_length)\n",
    "test_dataset = MSPCDataset('data/msr_paraphrase_test.txt', word2vec, max_sentence_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The stanza parts-of-speech parser and a Word2Vec model loaded with the \"Google-news-vectors\" pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initiating a pretrained model for word2vec... this may take a while\")\n",
    "word2vec_pretrained = gensim.models.keyedvectors.load_word2vec_format(pretrained_word2vec_path, binary=True)\n",
    "print(\"Done initiating the pretrained model for word2vec\")\n",
    "\n",
    "pretrained_train_dataset = MSPCDataset('data/msr_paraphrase_train.txt', word2vec_pretrained, max_sentence_length, embedding_size=300)\n",
    "pretrained_test_dataset = MSPCDataset('data/msr_paraphrase_test.txt', word2vec_pretrained, max_sentence_length, embedding_size=300)\n",
    "\n",
    "\n",
    "pretrained_train_dataloader = DataLoader(pretrained_train_dataset, batch_size=64, shuffle=False)\n",
    "pretrained_test_dataloader = DataLoader(pretrained_test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The SpaCy parts-of-speech parser and a Word2Vec model trained on the MSRP corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_spacy_sentences, max_sentence_length_spacy_sentences = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt')\n",
    "train_dataset_spacy_sentences = MSPCDataset('data/msr_paraphrase_train.txt', word2vec_spacy_sentences, max_sentence_length_spacy_sentences)\n",
    "test_dataset_spacy_sentences = MSPCDataset('data/msr_paraphrase_test.txt', word2vec_spacy_sentences, max_sentence_length_spacy_sentences)\n",
    "\n",
    "train_dataloader_spacy_sentences = DataLoader(train_dataset_spacy_sentences, batch_size=64, shuffle=False)\n",
    "test_dataloader_spacy_sentences = DataLoader(test_dataset_spacy_sentences, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Stop words and words with 2 or less characters removed  and a Word2Vec model trained on the MSRP corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_raw_sentences, max_sentence_length_raw_sentences = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt')\n",
    "train_dataset_raw_sentences = MSPCDataset('data/msr_paraphrase_train.txt', word2vec_raw_sentences, max_sentence_length_raw_sentences)\n",
    "test_dataset_raw_sentences = MSPCDataset('data/msr_paraphrase_test.txt', word2vec_raw_sentences, max_sentence_length_raw_sentences)\n",
    "\n",
    "train_dataloader_raw_sentences = DataLoader(train_dataset_raw_sentences, batch_size=64, shuffle=False)\n",
    "test_dataloader_raw_sentences = DataLoader(test_dataset_raw_sentences, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "The original paper noted that a CNN was used but only provided details on the pooling method, _Dynamic K-Max Pooling_, and the sentence similarity calculations. The following table is the final architecture used for the neural network, `SentenceSimilarityCNN2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic K-Max Pooling\n",
    "\n",
    "The original paper used a dynamic k-max pooling method in their model. The _k_ value is determine by equation.\n",
    "\n",
    "\\begin{equation*} k=\\max \\left({k_{top},\\left \\lceil{ \\frac {L-l}{L} \\left |{ s }\\right | }\\right \\rceil }\\right)\\end{equation*}\n",
    "\n",
    "While the pooling method was discussed, the implementation details were not provided. After some research, we were inspired by how it was implemented by [Kalchbrenner et. al 2014](https://arxiv.org/pdf/1404.2188.pdf).\n",
    "\n",
    "We implmented two custom layers for our network to support _dynamic k-max pooling_:\n",
    "\n",
    "* `DynamicKMaxPoolId` - K-Max pooling function where $k$ is defined as a function of the current depth in the network. This will be used twice in the neural network.\n",
    "* `KMaxPool1d` - Will be used to perform the final pooling function before passing throught the fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicKMaxPoolId(nn.Module):\n",
    "    def __init__(self, k, l, L):\n",
    "        super(DynamicKMaxPoolId, self).__init__()\n",
    "        self.k = k\n",
    "        self.l = l\n",
    "        self.L = L\n",
    "\n",
    "    def forward(self, x, sentence_length):\n",
    "        ktop = max(self.k, int((self.L - self.l)/self.L * sentence_length))\n",
    "        #print(f\"ktop: {ktop}\")\n",
    "        k_max_values, k_max_indices = torch.topk(x, ktop, dim=2)\n",
    "        return k_max_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMaxPool1d(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super(KMaxPool1d, self).__init__()\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input shape (batch_size, num_channels, sequence_length)\n",
    "        # output shape (batch_size num_channels, k)\n",
    "        k_max_values, k_max_indices  = torch.topk(x, self.k, dim=2)\n",
    "        return k_max_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture: `SentenceSimilarityCNN2`\n",
    "\n",
    "\n",
    "Layers | Configuration | Activation Function \n",
    "--- | --- | --- | \n",
    "Conv1d | input channel `embedding_dim`, output channel `num_filters` | ReLU* \n",
    "DynamicKMaxPool | (k=3, l=1, L=3) | -\n",
    "Conv1d | input channel `num_filters`, output channel `num_filters * 2` | ReLU*\n",
    "KMaxPool1d | (k=3) | - \n",
    "Linear | input = `k * num_filters *2` output = `hidden_dim` | Sentence Similarity\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "_* ReLU was chosen as the activation function for its ability to introduce sparsity into the network_\n",
    "\n",
    "#### Sentence Similarity\n",
    "\n",
    "\n",
    "The output of the network is a `Tensor` containing a similarity score defined by the equations below. As you can see, Manhattan distance is used to calulated similarity between the sentence pairs.\n",
    "\n",
    "\n",
    "\n",
    " \\begin{align*} Man(\\vec V_{x}, \\vec V_{y})=&\\left |{ x_{1}-y_{1} }\\right |\\! +\\! \\left |{ x_{2}-y_{2} }\\right | \\!+ \\!\\ldots \\!+ \\!\\left |{ x_{n}-y_{n} }\\right |\n",
    " \\\\ score=&e^{-Man(\\vec V_{x}, \\vec V_{y})},\\quad score\\in [{0,1}] \\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicKMaxPoolId(nn.Module):\n",
    "    def __init__(self, k, l, L):\n",
    "        super(DynamicKMaxPoolId, self).__init__()\n",
    "        self.k = k\n",
    "        self.l = l\n",
    "        self.L = L\n",
    "\n",
    "    def forward(self, x, sentence_length):\n",
    "        ktop = max(self.k, int((self.L - self.l)/self.L * sentence_length))\n",
    "        #print(f\"ktop: {ktop}\")\n",
    "        k_max_values, k_max_indices = torch.topk(x, ktop, dim=2)\n",
    "        return k_max_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentenceSimilarityCNN2(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_filters, filter_size, hidden_dim):\n",
    "        super(SentenceSimilarityCNN2, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=filter_size, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels= num_filters, out_channels=num_filters * 2, kernel_size=filter_size, padding=1)\n",
    "        #self.conv3 = nn.Conv1d(in_channels= num_filters * 2, out_channels=num_filters * 3, kernel_size=filter_size, padding=1)\n",
    "\n",
    "        self.pool1 = DynamicKMaxPoolId(k=3, l=1, L=3)\n",
    "\n",
    "        #self.pool2 = DynamicKMaxPoolId(k=3, l=2, L=3)\n",
    "\n",
    "        self.k = 3\n",
    "        self.kmaxPool1d = KMaxPool1d(k=self.k)\n",
    "\n",
    "        #self.fc1 = nn.Linear(self.k * num_filters , hidden_dim)\n",
    "        self.fc1 = nn.Linear(self.k * num_filters * 2, hidden_dim)\n",
    "\n",
    "    def forward(self, input1_embedded, input2_embedded):\n",
    "\n",
    "        # input: input1_embedded is sentence1 and input2_embedding is sentence2\n",
    "        # input shape: (batch_size, max_sentence_length, embedding_size)\n",
    "        # output shap: (50) dimension vector that represents the sentence\n",
    "        # output: similarity score\n",
    "\n",
    "        # Find the sentence lengths\n",
    "        sent_length1 = (torch.nonzero(input1_embedded).max(dim=0).values[1] + 1).item()\n",
    "        sent_length2 = (torch.nonzero(input2_embedded).max(dim=0).values[1] + 1).item()\n",
    "        #print(f\"sent length1: {sent_length1} AND sent length2: {sent_length2}\")\n",
    "\n",
    "        # Convolution\n",
    "        #print(input1_embedded.shape)\n",
    "        # input shape (batch_size, max_sentence_length, embedding_size)\n",
    "        # permuted shape (batch_size, embedding_size, max_sentence_length)\n",
    "        # output shape (batch_size, out_channels, (max_sentence_length - kernel_size + 2*padding)/stride + 1\n",
    "        # i.e. if max_sentence_length=19 and kernel_size=3 and padding=1 and stride=1 and out_channels=64\n",
    "        #      output_shape (19-3+2*1)/1 + 1 = 19 => (64, 64, 19)\n",
    "        input1_embedded = self.conv1(input1_embedded.permute(0, 2, 1))\n",
    "        input2_embedded = self.conv1(input2_embedded.permute(0, 2, 1))\n",
    "        #print(f\"output of conv1: {input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = torch.relu(input1_embedded)\n",
    "        input2_embedded = torch.relu(input2_embedded)\n",
    "\n",
    "        # pool1 is dynamic k-max pooling and is a function of the following formula\n",
    "        # max(k-top, (L-l)/l * |s|) where L is total number of convolutional layers, l is the current convolution layer and |s| is sentence length k-top is the\n",
    "        # k-top important features and serves as a lower bound... we will always try and find at least k-top features\n",
    "        input1_embedded = self.pool1(input1_embedded, sent_length1)\n",
    "        input2_embedded = self.pool1(input2_embedded, sent_length2)\n",
    "        #print(f\"output of pool1: {input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = self.conv2(input1_embedded)\n",
    "        input2_embedded = self.conv2(input2_embedded)\n",
    "        #print(f\"output of conv2: {input1_embedded.shape}\")\n",
    "\n",
    "        #input1_embedded = torch.relu(input1_embedded)\n",
    "        #input2_embedded = torch.relu(input2_embedded)\n",
    "\n",
    "        #input1_embedded = self.pool2(input1_embedded, sent_length1)\n",
    "        #input2_embedded = self.pool2(input2_embedded, sent_length2)\n",
    "        #print(f\"output of pool2: {input1_embedded.shape}\")\n",
    "\n",
    "        #input1_embedded = self.conv3(input1_embedded)\n",
    "        #input2_embedded = self.conv3(input2_embedded)\n",
    "        #print(f\"output of conv3: {input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = torch.relu(input1_embedded)\n",
    "        input2_embedded = torch.relu(input2_embedded)\n",
    "\n",
    "        input1_embedded = self.kmaxPool1d(input1_embedded)\n",
    "        input2_embedded = self.kmaxPool1d(input2_embedded)\n",
    "        #print(f\"output of k-max pool: {input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = input1_embedded.view(input1_embedded.shape[0], input1_embedded.shape[1] * input1_embedded.shape[2])\n",
    "        input2_embedded = input2_embedded.view(input2_embedded.shape[0], input2_embedded.shape[1] * input2_embedded.shape[2])\n",
    "\n",
    "        #kmax_input1_embedded = self.kmaxPool1d(input1_embedded)\n",
    "        #input1_embedded = F.max_pool1d(input1_embedded, input1_embedded.shape[2]).squeeze(2)\n",
    "        #nput2_embedded = F.max_pool1d(input2_embedded, input2_embedded.shape[2]).squeeze(2)\n",
    "        #print(f\"output of max pool: {input1_embedded.shape}\")\n",
    "        #print(f\"output of kmax pool: {kmax_input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = self.fc1(input1_embedded)\n",
    "        input2_embedded = self.fc1(input2_embedded)\n",
    "        #print(input1_embedded.shape)\n",
    "\n",
    "        man_dist = torch.sum(torch.abs(input1_embedded - input2_embedded), axis=1)\n",
    "        # sentence1_mean = torch.mean(x1, axis=1)\n",
    "        # sentence2_mean = torch.mean(x2, axis=1)\n",
    "        # man_dist = torch.sum(torch.abs(sentence1_mean - sentence2_mean), axis=1)\n",
    "        # print(man_dist.shape)\n",
    "\n",
    "        return torch.exp(-man_dist)\n",
    "\n",
    "\n",
    "        #input2_embedded = self.conv1(input2_embedded.permute(0, 2, 1))\n",
    "\n",
    "\n",
    "        # Max pooling\n",
    "        # input1_pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in input1_conv]\n",
    "        # input2_pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in input2_conv]\n",
    "        #\n",
    "        # # Concatenate and flatten\n",
    "        # input1_concat = torch.cat(input1_pooled, dim=1)\n",
    "        # input2_concat = torch.cat(input2_pooled, dim=1)\n",
    "        #\n",
    "        # # Concatenate the two sentence representations\n",
    "        # sentence_similarity = torch.cat([input1_concat, input2_concat], dim=1)\n",
    "        #\n",
    "        # # Dense layers\n",
    "        # #sentence_similarity = self.dropout(F.relu(self.fc1(sentence_similarity)))\n",
    "        # #sentence_similarity = self.fc2(sentence_similarity)\n",
    "        #\n",
    "        # return torch.sigmoid(sentence_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Results\n",
    "\n",
    "Below we define a custom `train` function that facilitates the training process as well as a custom evaluation function `eval_model` to allow flexibility when evaluating the output of our model.\n",
    "\n",
    "\n",
    "### Training Details\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "The hyperparameters are as follows:\n",
    "\n",
    "* `embedding_dim` = `50`\n",
    "* `num_filters` = `100`\n",
    "* `filter_size` = `3`\n",
    "* `hidden_dim` = `300`\n",
    "* `dropout` = `0.5`\n",
    "* `n_epochs` = `80`\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "The Mean Square Error loss is used to calculate the loss between the expected value of the similarity flag and the predicated value.  The loss will be used to update the weights during the back-propagation phase.\n",
    "\n",
    "#### Optimizer\n",
    "\n",
    "Stochastic Gradient descent is an iterative optimization algorithm used to find optimal results by taking small steps in the direction of the gradient.  The size of the step is defined by the learning rate.  The learning rate used is defined above in _Hyperparameters_, which was based on the configuration information described in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2vec.wv)\n",
    "embedding_dim = 50\n",
    "num_filters = 100\n",
    "filter_size = 3\n",
    "hidden_dim = 300\n",
    "dropout = 0.5\n",
    "n_epochs=80\n",
    "\n",
    "def train(train_loader, n_epochs=n_epochs, embedding_dim=embedding_dim, num_filters=num_filters, hidden_dim=hidden_dim, lr=1e-1):\n",
    "    model = SentenceSimilarityCNN2(embedding_dim, num_filters, filter_size, hidden_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        curr_epoch_loss = []\n",
    "        for x1, x2, y in train_loader:\n",
    "            #print(x1.shape)\n",
    "            y_hat = model(x1, x2)\n",
    "            loss = criterion(y_hat, y.float())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "\n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def eval_model(model, test_dataloader, diff_score=0.435):\n",
    "    model.eval()\n",
    "    Y_pred = []\n",
    "    Y = []\n",
    "    for x1, x2, y in test_dataloader:\n",
    "        y_hat = model(x1, x2)\n",
    "        #print(y_hat)\n",
    "        y_pred = torch.zeros(y_hat.shape)\n",
    "        y_pred = (y_hat > diff_score).int()\n",
    "\n",
    "        Y_pred = np.concatenate((Y_pred, y_pred), axis=0)\n",
    "        Y = np.concatenate((Y, y), axis=0)\n",
    "\n",
    "        #print(y_pred)\n",
    "        #print(y)\n",
    "    return Y_pred, Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Stanza Parser Training & Results - _MSRP Trained Model_\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train stanza sentences\n",
    "start_time = datetime.datetime.now()\n",
    "model = train(train_dataloader)\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Number of epochs: \", n_epochs)\n",
    "print (\"Training dataset size\", len(train_dataset))\n",
    "print(\"Training time: \", (end_time - start_time))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y = eval_model(model, test_dataloader)\n",
    "\n",
    "print(\"size of pos test corpus = \", len(test_dataset))\n",
    "print(\"accuracy for pos test corpus = \", accuracy_score(y, y_pred))\n",
    "print(\"f1 score test corpus = \", f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Stanza Parser Training & Results - _Pre-trained Model_\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 35 epochs yields - accuracy .7026 with 0.45 - 300 - 400 - 800\n",
    "n_epochs_pretrained=50\n",
    "start_time = datetime.datetime.now()\n",
    "pretrained_model = train(pretrained_train_dataloader, n_epochs=n_epochs_pretrained, embedding_dim=300, num_filters=400, hidden_dim=800)\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Number of epochs: \", n_epochs_pretrained)\n",
    "print (\"Training dataset size\", len(pretrained_train_dataset))\n",
    "print(\"Training time: \", (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_pretrained, y = eval_model(pretrained_model, pretrained_test_dataloader)\n",
    "print(\"size of pos test corpus = \", len(pretrained_test_dataloader))\n",
    "print(\"accuracy for pos test corpus = \", accuracy_score(y, y_pred_pretrained))\n",
    "print(\"f1 score test corpus = \", f1_score(y, y_pred_pretrained))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SpaCy Parser Training & Results\n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train spacy sentences\n",
    "start_time = datetime.datetime.now()\n",
    "model_spacy_sentences = train(train_dataloader_spacy_sentences)\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Number of epochs: \", n_epochs)\n",
    "print (\"Training dataset size\", len(train_dataset))\n",
    "print(\"Training time: \", (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_spacy, y_spacy = eval_model(model_spacy_sentences, test_dataloader_spacy_sentences)\n",
    "\n",
    "print(\"size of spacy test corpus = \", len(test_dataset_spacy_sentences))\n",
    "print(\"accuracy for spacy test corpus = \", accuracy_score(y_spacy, y_pred_spacy))\n",
    "print(\"f1 score test corpus = \", f1_score(y, y_pred_spacy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Raw Parser Training & Results\n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the raw sentences\n",
    "start_time = datetime.datetime.now()\n",
    "model_raw_sentences = train(train_dataloader_raw_sentences)\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Number of epochs: \", n_epochs)\n",
    "print (\"Training dataset size\", len(train_dataset))\n",
    "print(\"Training time: \", (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_raw, y_raw = eval_model(model_raw_sentences, test_dataloader_raw_sentences, diff_score=0.4)\n",
    "\n",
    "print(\"size of raw test corpus = \", len(test_dataset_raw_sentences))\n",
    "print(\"accuracy for raw test corpus = \", accuracy_score(y_raw, y_pred_raw))\n",
    "print(\"f1 score test corpus = \", f1_score(y, y_pred_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't work yet\n",
    "\n",
    "acc_df = pd.DataFrame([['STANZA-MSRP', accuracy_score(y, y_pred)], \\\n",
    "                   ['STANZA-PRETRAIN', accuracy_score(y, y_pred_pretrained)], \\\n",
    "                   ['SPACY', accuracy_score(y_spacy, y_pred_spacy)], \\\n",
    "                   ['RAW', accuracy_score(y_raw, y_pred_raw)]], columns=['parser', 'val'])\n",
    "\n",
    "acc_df.plot(kind='bar')\n",
    "\n",
    "f1_df = pd.DataFrame([['STANZA-MSRP', f1_score(y, y_pred)], \\\n",
    "                     ['STANZA-PRETRAIN',  f1_score(y, y_pred_pretrained)], \\\n",
    "                     ['SPACY',  f1_score(y, y_pred_spacy)], \\\n",
    "                     ['RAW',  f1_score(y, y_pred_raw)]], columns=['parser', 'val'])\n",
    "\n",
    "f1_df.plot(kind='bar')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
