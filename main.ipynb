{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gensim\n",
    "import numpy as np\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import torch, stanza\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import threading\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:01.824854Z",
     "end_time": "2023-04-08T00:35:02.331708Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Processing\n",
    "\n",
    "This section can have stuff related to data prep.\n",
    "\n",
    "\n",
    "Should the MSPC Dataset be a part of this section? - Adam"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    # Note: Unable to use pd.read_csv... the function complained about an issue with the formatting of the tsv file\n",
    "    # train = pd.read_csv('data/msr_paraphrase_train.txt', sep='\\t', encoding='latin1')\n",
    "    # train\n",
    "\n",
    "    # opting to read file in and split columns manually to create a pandas dataframe\n",
    "    list = []\n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            fields = line.split('\\t')\n",
    "            list.append(fields)\n",
    "\n",
    "    df = pd.DataFrame(list[1:], columns=['Quality', 'ID1', 'ID2', 'String1', 'String2'])\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:01.868489Z",
     "end_time": "2023-04-08T00:35:02.351344Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Definition\n",
    "\n",
    "![Model Overview](./images/overview.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Input Layer\n",
    "\n",
    "In the input layer is made up of a Stanford Parser to provide a syntactic tree so that the model can extract significant words (mainly, subject, predicate, object) in the input corpus. Word2Vec is then used to map the words into vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 19:54:42 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-18 19:54:42 INFO: Using device: cpu\n",
      "2023-04-18 19:54:42 INFO: Loading: tokenize\n",
      "2023-04-18 19:54:42 INFO: Loading: pos\n",
      "2023-04-18 19:54:43 INFO: Loading: constituency\n",
      "2023-04-18 19:54:43 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# set 'download_method = None' to not download the resources over and over\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency', download_method=None, use_gpu=False)\n",
    "\n",
    "def trunk_construction(str, parent_label = None):\n",
    "    doc = nlp(str)\n",
    "    tree = doc.sentences[0].constituency\n",
    "\n",
    "    words = construct_sentence(tree, parent_label)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def construct_sentence(tree, parent_label = None, leave_pos=False):\n",
    "\n",
    "    sentences = []\n",
    "    if 'NN' in tree.label:\n",
    "        if parent_label == 'NP':\n",
    "            # sentences.append(tree)\n",
    "            sentences = sentences + tree.leaf_labels()\n",
    "    if 'VB' in tree.label:\n",
    "        if parent_label == 'VP':\n",
    "            #sentences.append(tree)\n",
    "            sentences = sentences + tree.leaf_labels()\n",
    "    for child in tree.children:\n",
    "        sentences = sentences + construct_sentence(child, tree.label)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def find_branches(tree, label, not_in_label=None, ancestors=[]):\n",
    "    branches = []\n",
    "    # print(\"-------------\")\n",
    "    # print(ancestors)\n",
    "    # print(f\"{tree.label} == {label}\")\n",
    "    if tree.label == label and not_in_label not in ancestors:\n",
    "        # print(f\"adding {tree}\")\n",
    "        branches.append(tree)\n",
    "    for child in tree.children:\n",
    "        branches = branches + find_branches(child, label, not_in_label, ancestors + [tree.label])\n",
    "\n",
    "    return branches\n",
    "\n",
    "#\n",
    "# # According to the paper the subject is the first NN child of NP\n",
    "def find_subject(noun_phrase_for_subject):\n",
    "    subject = []\n",
    "    for child in noun_phrase_for_subject.children:\n",
    "        if 'NN' in child.label:\n",
    "            subject = subject + child.leaf_labels()\n",
    "\n",
    "    #print(f\"subject = {subject}\")\n",
    "    #if len(subject) > 0:\n",
    "    #    return ' '.join(subject)\n",
    "    return subject\n",
    "\n",
    "    return None\n",
    "\n",
    "def find_predicate(verb_phrase_for_predicate):\n",
    "    predicate = []\n",
    "    for child in verb_phrase_for_predicate.children:\n",
    "        if child.label.startswith('VB'):\n",
    "            predicate = predicate + child.leaf_labels()\n",
    "\n",
    "    if len(predicate) > 0:\n",
    "        return ' '.join(predicate)\n",
    "\n",
    "    return None\n",
    "\n",
    "def find_object(verb_phase_for_object, parent='VP'):\n",
    "    objects = []\n",
    "    for child in verb_phase_for_object.children:\n",
    "        if child.label == 'VP':\n",
    "            continue\n",
    "        if 'NN' in child.label and parent in ['NP', 'PP', 'ADJP']:\n",
    "            #objects = objects + child.leaf_labels()\n",
    "            new_objects = child.leaf_labels()\n",
    "            for new_object in new_objects:\n",
    "                if new_object not in objects:\n",
    "                    objects.append(new_object)\n",
    "        else:\n",
    "            new_objects = find_object(child, child.label)\n",
    "            #if new_objects not in objects and new_objects is not None:\n",
    "            for new_object in new_objects:\n",
    "                if new_object not in objects:\n",
    "                    objects.append(new_object)\n",
    "                #objects = objects + new_objects\n",
    "\n",
    "    return objects\n",
    "    # if len(objects) > 0:\n",
    "    #     #return ' '.join(objects)\n",
    "    #     return objects\n",
    "    # else:\n",
    "    #     return None\n",
    "\n",
    "def find_spo(tree):\n",
    "    noun_phrases_for_subject = find_branches(tree, label='NP', not_in_label='VP', ancestors=[])\n",
    "    subject_list = []\n",
    "    for noun_phrase_for_subject in noun_phrases_for_subject:\n",
    "        subject = find_subject(noun_phrase_for_subject)\n",
    "        #if subject is not None:\n",
    "        #   subject_list.append(subject)\n",
    "        subject_list = subject_list + subject\n",
    "\n",
    "    verb_phrases = find_branches(tree, label='VP')\n",
    "    predicate_list = []\n",
    "    object_list = []\n",
    "    for verb_phrase in verb_phrases:\n",
    "        predicate = find_predicate(verb_phrase)\n",
    "        if predicate is not None:\n",
    "            predicate_list.append(predicate)\n",
    "        object = find_object(verb_phrase)\n",
    "        object_list = object_list + object\n",
    "        #if object is not None:\n",
    "        #    object_list.append(object)\n",
    "\n",
    "    # dedupe list\n",
    "    subject_list = list(dict.fromkeys(subject_list))\n",
    "    predicate_list = list(dict.fromkeys(predicate_list))\n",
    "    object_list = list(dict.fromkeys(object_list))\n",
    "\n",
    "    return subject_list, predicate_list, object_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:01.890215Z",
     "end_time": "2023-04-08T00:35:03.008883Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "outputs": [],
   "source": [
    " def test_parser(str, valid_subject, valid_predicate, valid_object):\n",
    "\n",
    "    #new_sentence = trunk_construction(str)\n",
    "    #print(new_sentence)\n",
    "    #assert new_sentence == valid_sentence\n",
    "\n",
    "    doc = nlp(str)\n",
    "    tree = doc.sentences[0].constituency\n",
    "\n",
    "    subject_list, predicate_list, object_list = find_spo(tree)\n",
    "    print(f\"Subject = {' '.join(subject_list)}\")\n",
    "    print(f\"Predicate = {' '.join(predicate_list)}\")\n",
    "    print(f\"Object = {' '.join(object_list)}\")\n",
    "\n",
    "    print(f\"{subject_list} = {valid_subject}\")\n",
    "    print(f\"{predicate_list} = {valid_predicate}\")\n",
    "    print(f\"{object_list} = {valid_object}\")\n",
    "    assert subject_list == valid_subject\n",
    "    assert predicate_list == valid_predicate\n",
    "    assert object_list == valid_object\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:03.013569Z",
     "end_time": "2023-04-08T00:35:03.015960Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parser Test Cases\n",
    "Test the parser using some of the training data sentences as input and asserting the output sentence matches the algorithm defined in the paper."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject = \n",
      "Predicate = have do is designing\n",
      "Object = amount work set architectures\n",
      "[] = []\n",
      "['have', 'do', 'is', 'designing'] = ['have', 'do', 'is', 'designing']\n",
      "['amount', 'work', 'set', 'architectures'] = ['amount', 'work', 'set', 'architectures']\n",
      "Subject = instruction set\n",
      "Predicate = have got do said\n",
      "Object = amount work\n",
      "['instruction', 'set'] = ['instruction', 'set']\n",
      "['have', 'got', 'do', 'said'] = ['have', 'got', 'do', 'said']\n",
      "['amount', 'work'] = ['amount', 'work']\n",
      "Subject = forces\n",
      "Predicate = launch\n",
      "Object = attacks\n",
      "['forces'] = ['forces']\n",
      "['launch'] = ['launch']\n",
      "['attacks'] = ['attacks']\n",
      "Subject = tire\n",
      "Predicate = was replaced\n",
      "Object = driver\n",
      "['tire'] = ['tire']\n",
      "['was', 'replaced'] = ['was', 'replaced']\n",
      "['driver'] = ['driver']\n",
      "Subject = Amrozi\n",
      "Predicate = accused called distorting\n",
      "Object = brother witness evidence\n",
      "['Amrozi'] = ['Amrozi']\n",
      "['accused', 'called', 'distorting'] = ['accused', 'called', 'distorting']\n",
      "['brother', 'witness', 'evidence'] = ['brother', 'witness', 'evidence']\n",
      "Subject = Amrozi\n",
      "Predicate = Referring accused distorting\n",
      "Object = witness brother evidence\n",
      "['Amrozi'] = ['Amrozi']\n",
      "['Referring', 'accused', 'distorting'] = ['Referring', 'accused', 'distorting']\n",
      "['witness', 'brother', 'evidence'] = ['witness', 'brother', 'evidence']\n",
      "Subject = Shares Genentech company products market\n",
      "Predicate = rose\n",
      "Object = percent\n",
      "['Shares', 'Genentech', 'company', 'products', 'market'] = ['Shares', 'Genentech', 'company', 'products', 'market']\n",
      "['rose'] = ['rose']\n",
      "['percent'] = ['percent']\n",
      "Subject = Shares Xoma\n",
      "Predicate = fell were\n",
      "Object = percent trade shares Genentech company products market\n",
      "['Shares', 'Xoma'] = ['Shares', 'Xoma']\n",
      "['fell', 'were'] = ['fell', 'were']\n",
      "['percent', 'trade', 'shares', 'Genentech', 'company', 'products', 'market'] = ['percent', 'trade', 'shares', 'Genentech', 'company', 'products', 'market']\n",
      "Subject = Gyorgy Heizler head disaster unit\n",
      "Predicate = said was carrying\n",
      "Object = coach passengers\n",
      "['Gyorgy', 'Heizler', 'head', 'disaster', 'unit'] = ['Gyorgy', 'Heizler', 'head', 'disaster', 'unit']\n",
      "['said', 'was', 'carrying'] = ['said', 'was', 'carrying']\n",
      "['coach', 'passengers'] = ['coach', 'passengers']\n",
      "Subject = head disaster unit Gyorgy Heizler\n",
      "Predicate = said had failed heed\n",
      "Object = coach driver stop lights\n",
      "['head', 'disaster', 'unit', 'Gyorgy', 'Heizler'] = ['head', 'disaster', 'unit', 'Gyorgy', 'Heizler']\n",
      "['said', 'had', 'failed', 'heed'] = ['said', 'had', 'failed', 'heed']\n",
      "['coach', 'driver', 'stop', 'lights'] = ['coach', 'driver', 'stop', 'lights']\n",
      "Subject = Sheena Young Child network\n",
      "Predicate = hoped lead\n",
      "Object = guidelines service infertility sufferers\n",
      "['Sheena', 'Young', 'Child', 'network'] = ['Sheena', 'Young', 'Child', 'network']\n",
      "['hoped', 'lead'] = ['hoped', 'lead']\n",
      "['guidelines', 'service', 'infertility', 'sufferers'] = ['guidelines', 'service', 'infertility', 'sufferers']\n",
      "Subject = Sheena Young Child network\n",
      "Predicate = said lead\n",
      "Object = guidelines service infertility sufferers\n",
      "['Sheena', 'Young', 'Child', 'network'] = ['Sheena', 'Young', 'Child', 'network']\n",
      "['said', 'lead'] = ['said', 'lead']\n",
      "['guidelines', 'service', 'infertility', 'sufferers'] = ['guidelines', 'service', 'infertility', 'sufferers']\n"
     ]
    }
   ],
   "source": [
    "test_parser(\"\"\"\"We have an incredible amount of work to do, but it is not in [designing new] instruction set architectures.\"\"\", [], ['have', 'do', 'is', 'designing'], ['amount', 'work', 'set', 'architectures'])\n",
    "test_parser(\"\"\"We have got an incredible amount of work to do, but it ain't in the instruction set,\" he said.\"\"\",  ['instruction', 'set'], ['have', 'got', 'do', 'said'], ['amount', 'work'])\n",
    "test_parser('Syrian forces launch new attacks', ['forces'], ['launch'], ['attacks'])\n",
    "test_parser(\"\"\"the flat tire was replaced by the driver\"\"\", ['tire'], ['was', 'replaced'], ['driver'])\n",
    "test_parser(\"\"\"Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.\"\"\",\n",
    "            ['Amrozi'], ['accused', 'called', 'distorting'], ['brother',  'witness', 'evidence'])\n",
    "test_parser(\"\"\"Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.\"\"\",\n",
    "         ['Amrozi'], ['Referring', 'accused', 'distorting'], ['witness', 'brother', 'evidence'])\n",
    "test_parser(\"\"\"Shares of Genentech, a much larger company with several products on the market, rose more than 2 percent\"\"\",\n",
    "            ['Shares', 'Genentech', 'company', 'products', 'market'], ['rose'], ['percent'])\n",
    "\n",
    "test_parser(\"\"\"Shares of Xoma fell 16 percent in early trade, while shares of Genentech, a much larger company with several products on the market, were up 2 percent.\"\"\", ['Shares', 'Xoma'], ['fell', 'were'], ['percent', 'trade', 'shares', 'Genentech', 'company', 'products', 'market'])\n",
    "\n",
    "test_parser(\"\"\"Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\"\"\",\n",
    "              ['Gyorgy', 'Heizler', 'head', 'disaster', 'unit'], ['said', 'was', 'carrying'], ['coach', 'passengers'])\n",
    "test_parser(\"\"\"The head of the local disaster unit, Gyorgy Heizler, said the coach driver had failed to heed red stop lights.\"\"\",\n",
    "            ['head', 'disaster', 'unit', 'Gyorgy', 'Heizler'], ['said', 'had', 'failed', 'heed'], ['coach', 'driver', 'stop', 'lights'])\n",
    "# test_parser(\"\"\"His wife said he was \"100 percent behind George Bush\" and looked forward to using his years of training in the war.\"\"\",\n",
    "#             \"wife said was percent George Bush looked using years training war\")\n",
    "test_parser(\"\"\"Sheena Young of Child, the national infertility support network, hoped the guidelines would lead to a more \"fair and equitable\" service for infertility sufferers\"\"\", ['Sheena', 'Young', 'Child', 'network'], ['hoped', 'lead'], ['guidelines', 'service', 'infertility', 'sufferers'])\n",
    "test_parser(\"\"\"Sheena Young, for Child, the national infertility support network, said the proposed guidelines should lead to a more \"fair and equitable\" service for infertility sufferers.\"\"\", ['Sheena', 'Young', 'Child', 'network'], ['said', 'lead'], ['guidelines', 'service', 'infertility', 'sufferers'])\n",
    "#\n",
    "# test_parser(\"\"\"Among CNN viewers, 29 percent said they were Republicans and 36 percent called themselves conservatives.\"\"\",\n",
    "#             \"CNN viewers percent said were Republicans percent called conservatives\")\n",
    "# test_parser(\"\"\"Out of Fox viewers, 41 percent describe themselves as Republicans, 24 percent as Democrats and 30 percent as Independents\"\"\",\n",
    "#             \"Fox viewers percent describe Republicans percent Democrats percent Independents\")\n",
    "\n",
    "# Note: stanza parser has a problem with the below sentence.  It is unable to parse it correctly\n",
    "# test_parser(\"\"\"Sheena Young, for Child, the national infertility support network, said the proposed guidelines should lead to a more \"fair and equitable\" service for infertility sufferers.\"\"\", \"\")\n",
    "# test_parser(\"\"\"Among Fox viewers, 41 percent describe themselves as Republicans, 24 percent as Democrats and 30 percent as Independents\"\"\", \"Fox viewers percent describe Republicans percent Democrats percent Independents\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:03.023262Z",
     "end_time": "2023-04-08T00:35:08.918548Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def find_spacy_spo(doc):\n",
    "    # Extract the subject, predicate, and object\n",
    "    subject = []\n",
    "    predicate = []\n",
    "    obj = []\n",
    "\n",
    "    for token in doc:\n",
    "        #print(f\"{token.dep_} : {token.text}\")\n",
    "        if \"subj\" in token.dep_:\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            subject .append(doc[start:end])\n",
    "        elif \"obj\" in token.dep_ or \"pcomp\" in token.dep_:\n",
    "            obj.append(token.text)\n",
    "        elif \"ROOT\" in token.dep_ or \"pred\" in token.dep_:\n",
    "            predicate.append(token.text)\n",
    "\n",
    "    # Print the results\n",
    "    # print(\"Subject: \", subject)\n",
    "    # print(\"Predicate: \", predicate)\n",
    "    # print(\"Object: \", obj)\n",
    "    return subject, predicate, object"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Parsing(Enum):\n",
    "    STANZA=1\n",
    "    SPACY=2\n",
    "    RAW=3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Concurrency Parsing\n",
    "Added support for concurrent parsing.  This can help in the performance of the preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "outputs": [],
   "source": [
    "class SentenceProcessingThread(threading.Thread):\n",
    "    def __init__(self, sentences, output_list, begin, end, parsing_enum=Parsing.STANZA):\n",
    "        super(SentenceProcessingThread, self).__init__()\n",
    "        self.sentences = sentences\n",
    "        self.parsing_enum = parsing_enum\n",
    "\n",
    "        if parsing_enum == Parsing.STANZA:\n",
    "            self.nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency', download_method=None, use_gpu=True)\n",
    "        else:\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.output_list = output_list\n",
    "        self.begin = begin\n",
    "        self.end = end\n",
    "\n",
    "    def trunk_construction(self, str, parent_label = None):\n",
    "        doc = self.nlp(str)\n",
    "        tree = doc.sentences[0].constituency\n",
    "\n",
    "        #words = construct_sentence(tree, parent_label)\n",
    "        #return ' '.join(words)\n",
    "\n",
    "        if self.parsing_enum == Parsing.SPACY:\n",
    "            subjects, predicates, objects = find_spacy_spo(tree)\n",
    "        else:\n",
    "            subjects, predicates, objects = find_spo(tree)\n",
    "\n",
    "        return f\"{' '.join(subjects)},{' '.join(predicates)},{' '.join(objects)}\"\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"going to process {self.begin} to {self.end}\")\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "            new_sentence = self.trunk_construction(sentence)\n",
    "            self.output_list[self.begin + i] = new_sentence\n",
    "\n",
    "def process_sentences_concurrently(sentences, output, p=2):\n",
    "    total = len(sentences)\n",
    "    interval = int(total / p)\n",
    "    threads = []\n",
    "    for i in range(p):\n",
    "        s = i*interval\n",
    "        if i == p-1:\n",
    "            e = total\n",
    "        else:\n",
    "            e = (i+1) * interval\n",
    "        sentences_slice = sentences[s:e]\n",
    "        sentence_thread = SentenceProcessingThread(sentences_slice, output, s, e)\n",
    "        sentence_thread.start()\n",
    "        threads.append(sentence_thread)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "def preprocess_corpus(input_file='data/msr_paraphrase_train.txt', output_file='data/msr_paraphrase_train_stanza.txt', N=None, parsing_enum=Parsing.STANZA):\n",
    "    print(output_file)\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"{output_file} already exists\")\n",
    "        return\n",
    "\n",
    "    starttime = datetime.datetime.now()\n",
    "    df = read_file(input_file)\n",
    "\n",
    "    if N is None:\n",
    "        N = len(df.String1)\n",
    "\n",
    "    output1 = [None] * N\n",
    "    output2 = [None] * N\n",
    "\n",
    "    # we can process with more threads if we only have CPU\n",
    "    p = 8\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # if cuda is available we don't need that many threads\n",
    "        # and if the number of threads is set too large using cuda\n",
    "        # we can get out of memory exceptions\n",
    "        p = 2\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    process_sentences_concurrently(df.String1[:N], output1, p)\n",
    "\n",
    "    # try and be careful with gpu memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    process_sentences_concurrently(df.String2[:N], output2, p)\n",
    "\n",
    "    endtime = datetime.datetime.now()\n",
    "\n",
    "    print(f\"time to process {N*2} sentences is {endtime - starttime}\")\n",
    "\n",
    "    stanza_df = df[:N]\n",
    "\n",
    "    processed_string1 = pd.Series(output1)\n",
    "    # processed_string1.apply(gensim.utils.simple_preprocess)\n",
    "    processed_string2 = pd.Series(output2)\n",
    "    #processed_string2.apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "    stanza_df.String1 = processed_string1\n",
    "    stanza_df.String2 = processed_string2\n",
    "\n",
    "    # write the file out.  This can help in the future\n",
    "    print(f\"about to write out {output_file}\")\n",
    "    stanza_df.to_csv(output_file, sep=\"\\t\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.905455Z",
     "end_time": "2023-04-08T00:35:08.941546Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence Preprocessing\n",
    "pass the input sentences from the training dataset through the stanford/stanza parser, extracting the relevant parts of speech and then tokenize the processed sentences using the gensim.utils.simple_preprocess utility"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec Embeddings\n",
    "Take the preprocessed and tokenized sentences and use Word2Vec to get the word embeddings.  Take each word embedding in a sentence and find the mean which will represent the embedding for the sentence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Function is broken out for testing purposes\n",
    "def generate_word2vec_model(corpus):\n",
    "    # Creating the Word2Vec model\n",
    "    model = Word2Vec(sentences=corpus, min_count=1, window=2, vector_size=50)\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.908525Z",
     "end_time": "2023-04-08T00:35:08.942658Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "outputs": [],
   "source": [
    "# Function is broken out for testing purposes\n",
    "def sentence_embeddings(w2v_model, sentence, size):\n",
    "    np_embedding = np.zeros(size)\n",
    "    for i, word in enumerate(sentence):\n",
    "        #print(word)\n",
    "        np_embedding[i] = w2v_model.wv.get_vector(word)\n",
    "\n",
    "    return np_embedding\n",
    "    # list = []\n",
    "    # for word in sentence:\n",
    "    #     list.append(w2v_model.wv.get_vector(word))\n",
    "    #\n",
    "    # word_matrix = np.row_stack(list)\n",
    "    # #return np.mean(word_matrix, axis=0)\n",
    "    # return word_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.910846Z",
     "end_time": "2023-04-08T00:35:08.942887Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "outputs": [],
   "source": [
    "def test_word2vec():\n",
    "\n",
    "    df = read_file('data/msr_paraphrase_train.txt')\n",
    "\n",
    "    sentences1 = df.String1[:5].apply(gensim.utils.simple_preprocess)\n",
    "    sentences2 = df.String2[:5].apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "    corpus = pd.concat([sentences1, sentences2], ignore_index=True)\n",
    "\n",
    "    max_sentence_len = corpus.apply(len).max()\n",
    "\n",
    "    model = generate_word2vec_model(corpus)\n",
    "\n",
    "    embedding = sentence_embeddings(model, corpus[0], (max_sentence_len, 50))\n",
    "    assert embedding.shape == (max_sentence_len, 50)\n",
    "\n",
    "test_word2vec()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.911696Z",
     "end_time": "2023-04-08T00:35:09.477774Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "outputs": [],
   "source": [
    "def init_word2vec(train_input_file, test_input_file, parsing_enum=Parsing.STANZA):\n",
    "\n",
    "    if parsing_enum == Parsing.RAW:\n",
    "        train_output_file = train_input_file\n",
    "        test_output_file = test_input_file\n",
    "        train_df = read_file(train_output_file)\n",
    "        test_df = read_file(test_output_file)\n",
    "\n",
    "    elif parsing_enum == Parsing.STANZA:\n",
    "        file_parts = os.path.splitext(train_input_file)\n",
    "        train_output_file = f\"{file_parts[0]}_stanza{file_parts[1]}\"\n",
    "        print(\"About to preprocess spacy data\")\n",
    "        preprocess_corpus(input_file=train_input_file, output_file=train_output_file, parsing_enum=parsing_enum)\n",
    "        print(\"Done preprocessing spacy data\")\n",
    "\n",
    "        file_parts = os.path.splitext(test_input_file)\n",
    "        test_output_file = f\"{file_parts[0]}_stanza{file_parts[1]}\"\n",
    "        print(\"About to preprocess data\")\n",
    "        preprocess_corpus(input_file=test_input_file, output_file=test_output_file, parsing_enum=parsing_enum)\n",
    "        print(\"Done preprocessing data\")\n",
    "        train_df = pd.read_csv(train_output_file, sep=\"\\t\")\n",
    "        test_df = pd.read_csv(test_output_file, sep=\"\\t\")\n",
    "    else:\n",
    "        file_parts = os.path.splitext(train_input_file)\n",
    "        train_output_file = f\"{file_parts[0]}_spacy{file_parts[1]}\"\n",
    "        print(\"About to preprocess spacy data\")\n",
    "        preprocess_corpus(input_file=train_input_file, output_file=train_output_file, parsing_enum=parsing_enum)\n",
    "        print(\"Done preprocessing spacy data\")\n",
    "\n",
    "        file_parts = os.path.splitext(test_input_file)\n",
    "        test_output_file = f\"{file_parts[0]}_spacy{file_parts[1]}\"\n",
    "        print(\"About to preprocess data\")\n",
    "        preprocess_corpus(input_file=test_input_file, output_file=test_output_file, parsing_enum=parsing_enum)\n",
    "        print(\"Done preprocessing data\")\n",
    "        train_df = pd.read_csv(train_output_file, sep=\"\\t\")\n",
    "        test_df = pd.read_csv(test_output_file, sep=\"\\t\")\n",
    "\n",
    "    # train_df = read_file(train)\n",
    "    # test_df = read_file(test)\n",
    "\n",
    "    train_sentences1 = train_df.String1.apply(gensim.utils.simple_preprocess)\n",
    "    train_sentences2 = train_df.String2.apply(gensim.utils.simple_preprocess)\n",
    "    test_sentences1 = test_df.String1.apply(gensim.utils.simple_preprocess)\n",
    "    test_sentences2 = test_df.String2.apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "    corpus = pd.concat([train_sentences1, train_sentences2, test_sentences1, test_sentences2], ignore_index=True)\n",
    "    max_sentence_len = corpus.apply(len).max()\n",
    "\n",
    "    word2vec = generate_word2vec_model(corpus)\n",
    "\n",
    "\n",
    "    return word2vec, max_sentence_len\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "outputs": [],
   "source": [
    "def corpus_embeddings(model, corpus, max_sentence_len):\n",
    "    corpus_size = len(corpus)\n",
    "    embeddings_list = []\n",
    "    embedding_matrix = np.zeros((corpus_size, max_sentence_len, 50))\n",
    "    for i, sentence in enumerate(corpus):\n",
    "        embeddings = sentence_embeddings(model, sentence, size=(max_sentence_len, 50))\n",
    "        embedding_matrix[i] = embeddings\n",
    "        embeddings_list.append(embeddings)\n",
    "\n",
    "    return embedding_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.972122Z",
     "end_time": "2023-04-08T00:35:09.479438Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Dataset for the MSPC dataset\n",
    "class MSPCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        tsv_file (string): path to the tsv file with sentences to compare and associate quality score\n",
    "        num_records (int): number of records to load.  Defaults to None which is all\n",
    "    \"\"\"\n",
    "    def __init__(self, tsv_file, w2v_model, max_sentence_length, num_records=None, parsing_enum=Parsing.STANZA):\n",
    "\n",
    "        self.max_sentence_len = max_sentence_length\n",
    "        self.w2v_model = w2v_model\n",
    "\n",
    "        if parsing_enum == Parsing.STANZA:\n",
    "            file_parts = os.path.splitext(tsv_file)\n",
    "            output_file = f\"{file_parts[0]}_stanza{file_parts[1]}\"\n",
    "            print(\"About to preprocess stanza data\")\n",
    "            preprocess_corpus(input_file=tsv_file, output_file=output_file, parsing_enum=parsing_enum)\n",
    "            print(\"Done preprocessing stanza data\")\n",
    "            #df = read_file('data/msr_paraphrase_train.txt')\n",
    "            df = pd.read_csv(output_file, sep=\"\\t\")\n",
    "        elif parsing_enum == Parsing.SPACY:\n",
    "            file_parts = os.path.splitext(tsv_file)\n",
    "            output_file = f\"{file_parts[0]}_spacy{file_parts[1]}\"\n",
    "            print(\"About to preprocess spacy data\")\n",
    "            preprocess_corpus(input_file=tsv_file, output_file=output_file, parsing_enum=parsing_enum)\n",
    "            print(\"Done preprocessing spacy data\")\n",
    "            #df = read_file('data/msr_paraphrase_train.txt')\n",
    "            df = pd.read_csv(output_file, sep=\"\\t\")\n",
    "        else:\n",
    "            df = read_file(tsv_file)\n",
    "\n",
    "        if num_records is not None:\n",
    "            processed_string1 = df[:num_records].String1\n",
    "            processed_string2 = df[:num_records].String2\n",
    "            self.quality = df[:num_records].Quality\n",
    "        else:\n",
    "            processed_string1 = df.String1\n",
    "            processed_string2 = df.String2\n",
    "            self.quality = df.Quality\n",
    "\n",
    "        if parsing_enum == Parsing.RAW:\n",
    "            processed_string1 = processed_string1.apply(gensim.parsing.preprocessing.remove_stopwords)\n",
    "            processed_string2 = processed_string2.apply(gensim.parsing.preprocessing.remove_stopwords)\n",
    "            processed_string1 = processed_string1.apply(lambda x: gensim.parsing.preprocessing.strip_short(x, minsize=3))\n",
    "            processed_string2 = processed_string2.apply(lambda x: gensim.parsing.preprocessing.strip_short(x, minsize=3))\n",
    "\n",
    "        processed_string1 = processed_string1.apply(gensim.utils.simple_preprocess)\n",
    "        processed_string2 = processed_string2.apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "\n",
    "        print(processed_string1)\n",
    "\n",
    "\n",
    "        #corpus = pd.concat([processed_string1, processed_string2], ignore_index=True)\n",
    "        #self.max_sentence_len = corpus.apply(len).max()\n",
    "        #w2v_model = generate_word2vec_model(corpus)\n",
    "\n",
    "        sentence_embeddings1 = corpus_embeddings(self.w2v_model, processed_string1, max_sentence_len=self.max_sentence_len)\n",
    "        sentence_embeddings2 = corpus_embeddings(self.w2v_model, processed_string2, max_sentence_len=self.max_sentence_len)\n",
    "\n",
    "        #self.w2v_model = w2v_model\n",
    "        self.sentences_embeddings1 = sentence_embeddings1\n",
    "        self.sentences_embeddings2 = sentence_embeddings2\n",
    "\n",
    "        # print (f\"Processing 200 sentences with gensim.utils.simple_preprocess took {end_time - start_time}\")\n",
    "        print(f\"Number of sentences processed in the String1 column: {len(processed_string1)}\")\n",
    "        print(f\"Number of sentences processed in the String2 column: {len(processed_string2)}\")\n",
    "        #print(self.sentences_embeddings1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_embeddings1)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        #return torch.FloatTensor(np.stack((self.sentences_embeddings1[i], self.sentences_embeddings2[i]))), self.quality[i]\n",
    "        return torch.FloatTensor(self.sentences_embeddings1[i]), torch.FloatTensor(self.sentences_embeddings2[i]), self.quality[i]\n",
    "\n",
    "    def get_max_sentence_length(self):\n",
    "        return self.max_sentence_len"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.981724Z",
     "end_time": "2023-04-08T00:35:09.479637Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to preprocess spacy data\n",
      "data/msr_paraphrase_train_stanza.txt\n",
      "data/msr_paraphrase_train_stanza.txt already exists\n",
      "Done preprocessing spacy data\n",
      "About to preprocess data\n",
      "data/msr_paraphrase_test_stanza.txt\n",
      "data/msr_paraphrase_test_stanza.txt already exists\n",
      "Done preprocessing data\n",
      "About to preprocess stanza data\n",
      "data/msr_paraphrase_train_stanza.txt\n",
      "data/msr_paraphrase_train_stanza.txt already exists\n",
      "Done preprocessing stanza data\n",
      "0    [amrozi, accused, called, distorting, brother,...\n",
      "1    [yucaipa, owned, selling, dominick, chain, saf...\n",
      "2    [had, published, offering, added, advertisemen...\n",
      "3    [gmt, tab, shares, were, having, set, cents, h...\n",
      "4    [stock, rose, close, percent, friday, stock, e...\n",
      "5    [revenue, quarter, year, dropped, percent, per...\n",
      "6        [nasdaq, had, closing, gain, percent, friday]\n",
      "7          [dvd, cca, appealed, state, supreme, court]\n",
      "8               [compared, cents, share, year, period]\n",
      "9    [said, does, fit, business, company, growth, s...\n",
      "Name: String1, dtype: object\n",
      "Number of sentences processed in the String1 column: 10\n",
      "Number of sentences processed in the String2 column: 10\n"
     ]
    }
   ],
   "source": [
    "def test_dataset():\n",
    "    word2vec, max_sentence_length = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt', parsing_enum=Parsing.STANZA)\n",
    "    dataset = MSPCDataset('data/msr_paraphrase_train.txt', word2vec, max_sentence_length, 10)\n",
    "    assert len(dataset) == 10\n",
    "\n",
    "    x1, y1, quality = dataset[0]\n",
    "    assert x1.shape[0] == max_sentence_length\n",
    "    assert x1.shape[1] == 50\n",
    "\n",
    "test_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T14:32:14.561411Z",
     "end_time": "2023-04-07T14:32:14.897963Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [amrozi, accused, brother, called, the, witnes...\n",
      "1    [yucaipa, owned, dominick, selling, chain, saf...\n",
      "2    [they, published, advertisement, internet, jun...\n",
      "3    [around, gmt, tab, shares, cents, having, earl...\n",
      "4    [the, stock, rose, percent, close, friday, new...\n",
      "5    [revenue, quarter, year, dropped, percent, per...\n",
      "6    [the, nasdaq, weekly, gain, percent, closing, ...\n",
      "7     [the, dvd, cca, appealed, state, supreme, court]\n",
      "8    [that, compared, million, cents, share, year, ...\n",
      "9    [said, foodservice, pie, business, doesn, fit,...\n",
      "Name: String1, dtype: object\n",
      "Number of sentences processed in the String1 column: 10\n",
      "Number of sentences processed in the String2 column: 10\n"
     ]
    }
   ],
   "source": [
    "def test_dataset_raw_sentences():\n",
    "    word2vec_raw, max_sentence_length_raw = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt', Parsing.RAW)\n",
    "    dataset = MSPCDataset('data/msr_paraphrase_train.txt', word2vec_raw, max_sentence_length_raw, 10, parsing_enum=Parsing.RAW)\n",
    "    assert len(dataset) == 10\n",
    "\n",
    "    x1, y1, quality = dataset[0]\n",
    "    assert x1.shape[0] == max_sentence_length_raw\n",
    "    assert x1.shape[1] == 50\n",
    "\n",
    "test_dataset_raw_sentences()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 19:59:31 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-18 19:59:31 INFO: Using device: cuda\n",
      "2023-04-18 19:59:31 INFO: Loading: tokenize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to preprocess spacy data\n",
      "data/msr_paraphrase_train_spacy.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 19:59:35 INFO: Loading: pos\n",
      "2023-04-18 19:59:35 INFO: Loading: constituency\n",
      "2023-04-18 19:59:36 INFO: Done loading processors!\n",
      "2023-04-18 19:59:36 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-18 19:59:36 INFO: Using device: cuda\n",
      "2023-04-18 19:59:36 INFO: Loading: tokenize\n",
      "2023-04-18 19:59:36 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to process 0 to 2038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 19:59:36 INFO: Loading: constituency\n",
      "2023-04-18 19:59:37 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to process 2038 to 4076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 20:07:56 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-18 20:07:56 INFO: Using device: cuda\n",
      "2023-04-18 20:07:56 INFO: Loading: tokenize\n",
      "2023-04-18 20:07:56 INFO: Loading: pos\n",
      "2023-04-18 20:07:57 INFO: Loading: constituency\n",
      "2023-04-18 20:07:57 INFO: Done loading processors!\n",
      "2023-04-18 20:07:57 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-18 20:07:57 INFO: Using device: cuda\n",
      "2023-04-18 20:07:57 INFO: Loading: tokenize\n",
      "2023-04-18 20:07:57 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to process 0 to 2038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 20:07:58 INFO: Loading: constituency\n",
      "2023-04-18 20:07:58 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to process 2038 to 4076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 20:16:23 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-18 20:16:23 INFO: Using device: cuda\n",
      "2023-04-18 20:16:23 INFO: Loading: tokenize\n",
      "2023-04-18 20:16:23 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to process 8152 sentences is 0:16:51.955237\n",
      "about to write out data/msr_paraphrase_train_spacy.txt\n",
      "Done preprocessing spacy data\n",
      "About to preprocess data\n",
      "data/msr_paraphrase_test_spacy.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 20:16:23 INFO: Loading: constituency\n",
      "2023-04-18 20:16:23 INFO: Done loading processors!\n",
      "2023-04-18 20:16:23 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-18 20:16:23 INFO: Using device: cuda\n",
      "2023-04-18 20:16:23 INFO: Loading: tokenize\n",
      "2023-04-18 20:16:24 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to process 0 to 862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 20:16:24 INFO: Loading: constituency\n",
      "2023-04-18 20:16:25 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to process 862 to 1725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 20:20:16 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-18 20:20:16 INFO: Using device: cuda\n",
      "2023-04-18 20:20:16 INFO: Loading: tokenize\n",
      "2023-04-18 20:20:16 INFO: Loading: pos\n",
      "2023-04-18 20:20:17 INFO: Loading: constituency\n",
      "2023-04-18 20:20:17 INFO: Done loading processors!\n",
      "2023-04-18 20:20:17 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-18 20:20:17 INFO: Using device: cuda\n",
      "2023-04-18 20:20:17 INFO: Loading: tokenize\n",
      "2023-04-18 20:20:17 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to process 0 to 862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 20:20:18 INFO: Loading: constituency\n",
      "2023-04-18 20:20:18 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to process 862 to 1725\n",
      "time to process 3450 sentences is 0:07:27.730531\n",
      "about to write out data/msr_paraphrase_test_spacy.txt\n",
      "Done preprocessing data\n",
      "About to preprocess spacy data\n",
      "data/msr_paraphrase_train_spacy.txt\n",
      "data/msr_paraphrase_train_spacy.txt already exists\n",
      "Done preprocessing spacy data\n",
      "0    [amrozi, accused, called, distorting, brother,...\n",
      "1    [yucaipa, owned, selling, dominick, chain, saf...\n",
      "2    [had, published, offering, added, advertisemen...\n",
      "3    [gmt, tab, shares, were, having, set, cents, h...\n",
      "4    [stock, rose, close, percent, friday, stock, e...\n",
      "5    [revenue, quarter, year, dropped, percent, per...\n",
      "6        [nasdaq, had, closing, gain, percent, friday]\n",
      "7          [dvd, cca, appealed, state, supreme, court]\n",
      "8               [compared, cents, share, year, period]\n",
      "9    [said, does, fit, business, company, growth, s...\n",
      "Name: String1, dtype: object\n",
      "Number of sentences processed in the String1 column: 10\n",
      "Number of sentences processed in the String2 column: 10\n"
     ]
    }
   ],
   "source": [
    "def test_dataset_spacy_sentences():\n",
    "    word2vec_spacy, max_sentence_length_spacy = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt', parsing_enum=Parsing.SPACY)\n",
    "    dataset = MSPCDataset('data/msr_paraphrase_train.txt', word2vec_spacy, max_sentence_length_spacy, 10, parsing_enum=Parsing.SPACY)\n",
    "    assert len(dataset) == 10\n",
    "\n",
    "    x1, y1, quality = dataset[0]\n",
    "    assert x1.shape[0] == max_sentence_length_spacy\n",
    "    assert x1.shape[1] == 50\n",
    "\n",
    "test_dataset_spacy_sentences()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataloaders\n",
    "Create training and test dataloaders for sentences parsed with parts-of-speech parser"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to preprocess spacy data\n",
      "data/msr_paraphrase_train_stanza.txt\n",
      "data/msr_paraphrase_train_stanza.txt already exists\n",
      "Done preprocessing spacy data\n",
      "About to preprocess data\n",
      "data/msr_paraphrase_test_stanza.txt\n",
      "data/msr_paraphrase_test_stanza.txt already exists\n",
      "Done preprocessing data\n",
      "About to preprocess stanza data\n",
      "data/msr_paraphrase_train_stanza.txt\n",
      "data/msr_paraphrase_train_stanza.txt already exists\n",
      "Done preprocessing stanza data\n",
      "0       [amrozi, accused, called, distorting, brother,...\n",
      "1       [yucaipa, owned, selling, dominick, chain, saf...\n",
      "2       [had, published, offering, added, advertisemen...\n",
      "3       [gmt, tab, shares, were, having, set, cents, h...\n",
      "4       [stock, rose, close, percent, friday, stock, e...\n",
      "                              ...                        \n",
      "4071    [point, mr, brando, announced, put, continued,...\n",
      "4072    [martin, be, freed, serving, today, thirds, se...\n",
      "4073    [duisenberg, have, concluded, has, improved, l...\n",
      "4074         [notification, was, reported, friday, msnbc]\n",
      "4075    [bond, us, yt, rr, rose, yield, percent, wedne...\n",
      "Name: String1, Length: 4076, dtype: object\n",
      "Number of sentences processed in the String1 column: 4076\n",
      "Number of sentences processed in the String2 column: 4076\n",
      "About to preprocess stanza data\n",
      "data/msr_paraphrase_test_stanza.txt\n",
      "data/msr_paraphrase_test_stanza.txt already exists\n",
      "Done preprocessing stanza data\n",
      "0       [operating, officer, pccw, mike, butcher, alex...\n",
      "1       [automakers, world, said, declined, predicted,...\n",
      "2       [federal, centers, disease, control, preventio...\n",
      "3       [storm, developed, was, expected, hit, gulf, m...\n",
      "4       [company, did, detail, costs, replacement, rep...\n",
      "                              ...                        \n",
      "1720    [hughes, refused, rehire, complained, hernande...\n",
      "1721              [are, democrats, assembly, republicans]\n",
      "1722    [bethany, hamilton, remained, condition, satur...\n",
      "1723    [week, power, station, aes, corp, walked, refu...\n",
      "1724                                              [sobig]\n",
      "Name: String1, Length: 1725, dtype: object\n",
      "Number of sentences processed in the String1 column: 1725\n",
      "Number of sentences processed in the String2 column: 1725\n"
     ]
    }
   ],
   "source": [
    "word2vec, max_sentence_length = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt')\n",
    "train_dataset = MSPCDataset('data/msr_paraphrase_train.txt', word2vec, max_sentence_length)\n",
    "test_dataset = MSPCDataset('data/msr_paraphrase_test.txt', word2vec, max_sentence_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to preprocess spacy data\n",
      "data/msr_paraphrase_train_stanza.txt\n",
      "data/msr_paraphrase_train_stanza.txt already exists\n",
      "Done preprocessing spacy data\n",
      "About to preprocess data\n",
      "data/msr_paraphrase_test_stanza.txt\n",
      "data/msr_paraphrase_test_stanza.txt already exists\n",
      "Done preprocessing data\n",
      "About to preprocess stanza data\n",
      "data/msr_paraphrase_train_stanza.txt\n",
      "data/msr_paraphrase_train_stanza.txt already exists\n",
      "Done preprocessing stanza data\n",
      "0       [amrozi, accused, called, distorting, brother,...\n",
      "1       [yucaipa, owned, selling, dominick, chain, saf...\n",
      "2       [had, published, offering, added, advertisemen...\n",
      "3       [gmt, tab, shares, were, having, set, cents, h...\n",
      "4       [stock, rose, close, percent, friday, stock, e...\n",
      "                              ...                        \n",
      "4071    [point, mr, brando, announced, put, continued,...\n",
      "4072    [martin, be, freed, serving, today, thirds, se...\n",
      "4073    [duisenberg, have, concluded, has, improved, l...\n",
      "4074         [notification, was, reported, friday, msnbc]\n",
      "4075    [bond, us, yt, rr, rose, yield, percent, wedne...\n",
      "Name: String1, Length: 4076, dtype: object\n",
      "Number of sentences processed in the String1 column: 4076\n",
      "Number of sentences processed in the String2 column: 4076\n",
      "About to preprocess stanza data\n",
      "data/msr_paraphrase_test_stanza.txt\n",
      "data/msr_paraphrase_test_stanza.txt already exists\n",
      "Done preprocessing stanza data\n",
      "0       [operating, officer, pccw, mike, butcher, alex...\n",
      "1       [automakers, world, said, declined, predicted,...\n",
      "2       [federal, centers, disease, control, preventio...\n",
      "3       [storm, developed, was, expected, hit, gulf, m...\n",
      "4       [company, did, detail, costs, replacement, rep...\n",
      "                              ...                        \n",
      "1720    [hughes, refused, rehire, complained, hernande...\n",
      "1721              [are, democrats, assembly, republicans]\n",
      "1722    [bethany, hamilton, remained, condition, satur...\n",
      "1723    [week, power, station, aes, corp, walked, refu...\n",
      "1724                                              [sobig]\n",
      "Name: String1, Length: 1725, dtype: object\n",
      "Number of sentences processed in the String1 column: 1725\n",
      "Number of sentences processed in the String2 column: 1725\n"
     ]
    }
   ],
   "source": [
    "word2vec_raw_sentences, max_sentence_length_raw_sentences = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt')\n",
    "train_dataset_raw_sentences = MSPCDataset('data/msr_paraphrase_train.txt', word2vec_raw_sentences, max_sentence_length_raw_sentences)\n",
    "test_dataset_raw_sentences = MSPCDataset('data/msr_paraphrase_test.txt', word2vec_raw_sentences, max_sentence_length_raw_sentences)\n",
    "\n",
    "train_dataloader_raw_sentences = DataLoader(train_dataset_raw_sentences, batch_size=64, shuffle=False)\n",
    "test_dataloader_raw_sentences = DataLoader(test_dataset_raw_sentences, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to preprocess spacy data\n",
      "data/msr_paraphrase_train_stanza.txt\n",
      "data/msr_paraphrase_train_stanza.txt already exists\n",
      "Done preprocessing spacy data\n",
      "About to preprocess data\n",
      "data/msr_paraphrase_test_stanza.txt\n",
      "data/msr_paraphrase_test_stanza.txt already exists\n",
      "Done preprocessing data\n",
      "About to preprocess stanza data\n",
      "data/msr_paraphrase_train_stanza.txt\n",
      "data/msr_paraphrase_train_stanza.txt already exists\n",
      "Done preprocessing stanza data\n",
      "0       [amrozi, accused, called, distorting, brother,...\n",
      "1       [yucaipa, owned, selling, dominick, chain, saf...\n",
      "2       [had, published, offering, added, advertisemen...\n",
      "3       [gmt, tab, shares, were, having, set, cents, h...\n",
      "4       [stock, rose, close, percent, friday, stock, e...\n",
      "                              ...                        \n",
      "4071    [point, mr, brando, announced, put, continued,...\n",
      "4072    [martin, be, freed, serving, today, thirds, se...\n",
      "4073    [duisenberg, have, concluded, has, improved, l...\n",
      "4074         [notification, was, reported, friday, msnbc]\n",
      "4075    [bond, us, yt, rr, rose, yield, percent, wedne...\n",
      "Name: String1, Length: 4076, dtype: object\n",
      "Number of sentences processed in the String1 column: 4076\n",
      "Number of sentences processed in the String2 column: 4076\n",
      "About to preprocess stanza data\n",
      "data/msr_paraphrase_test_stanza.txt\n",
      "data/msr_paraphrase_test_stanza.txt already exists\n",
      "Done preprocessing stanza data\n",
      "0       [operating, officer, pccw, mike, butcher, alex...\n",
      "1       [automakers, world, said, declined, predicted,...\n",
      "2       [federal, centers, disease, control, preventio...\n",
      "3       [storm, developed, was, expected, hit, gulf, m...\n",
      "4       [company, did, detail, costs, replacement, rep...\n",
      "                              ...                        \n",
      "1720    [hughes, refused, rehire, complained, hernande...\n",
      "1721              [are, democrats, assembly, republicans]\n",
      "1722    [bethany, hamilton, remained, condition, satur...\n",
      "1723    [week, power, station, aes, corp, walked, refu...\n",
      "1724                                              [sobig]\n",
      "Name: String1, Length: 1725, dtype: object\n",
      "Number of sentences processed in the String1 column: 1725\n",
      "Number of sentences processed in the String2 column: 1725\n"
     ]
    }
   ],
   "source": [
    "word2vec_spacy_sentences, max_sentence_length_spacy_sentences = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt')\n",
    "train_dataset_spacy_sentences = MSPCDataset('data/msr_paraphrase_train.txt', word2vec_spacy_sentences, max_sentence_length_spacy_sentences)\n",
    "test_dataset_spacy_sentences = MSPCDataset('data/msr_paraphrase_test.txt', word2vec_spacy_sentences, max_sentence_length_spacy_sentences)\n",
    "\n",
    "train_dataloader_spacy_sentences = DataLoader(train_dataset_spacy_sentences, batch_size=64, shuffle=False)\n",
    "test_dataloader_spacy_sentences = DataLoader(test_dataset_spacy_sentences, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "outputs": [],
   "source": [
    "def conv_output_volume(W, F, S, P):\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: Given the input volume size $W$, the kernel/filter size $F$,\n",
    "    the stride $S$, and the amount of zero padding $P$ used on the border,\n",
    "    calculate the output volume size.\n",
    "    Note the output should a integer.\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    #https://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "    return int((W-F+2*P)/S+1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T14:32:15.557166Z",
     "end_time": "2023-04-07T14:32:15.560063Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(conv_output_volume(50, 3, 1, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T14:32:18.231576Z",
     "end_time": "2023-04-07T14:32:18.377984Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pooling Layers\n",
    "\n",
    "The original paper used a dynamic k-max pooling method in their model. The _k_ value is determine by equation (1).\n",
    "\n",
    "\\begin{equation*} k=\\max \\left({k_{top},\\left \\lceil{ \\frac {L-l}{L} \\left |{ s }\\right | }\\right \\rceil }\\right)\\end{equation*}\n",
    "\n",
    "__Dynamic K-Max Pooling Implementation:__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "outputs": [],
   "source": [
    "# #https://gist.github.com/anna-hope/7a2b2e66c3645aa8e4f94dbf06aed8dc\n",
    "# \"\"\"\n",
    "# TODO: Remove hardcoded values and max it dynamic.\n",
    "# \"\"\"\n",
    "# class DynamicKMaxPooling(nn.Module):\n",
    "#     def __init__(self, k_init, conv_layers, layer):\n",
    "#         super().__init__()\n",
    "#         # \"L is the total  number  of  convolutional  layers\n",
    "#         # in  the  network;\n",
    "#         # ktop is the fixed pooling parameter for the\n",
    "#         # topmost  convolutional  layer\"\n",
    "#         self.k_init = k_init\n",
    "#         self.conv_layers = conv_layers\n",
    "#         self.layer = layer\n",
    "#\n",
    "#     def forward(self, X):\n",
    "#         s = 50\n",
    "#         dyn_k = ((self.conv_layers - self.layer) / self.conv_layers) * 3\n",
    "#         k_max = int(round(max(self.k_init, np.ceil(dyn_k))))\n",
    "#         print(k_max)\n",
    "#         out = F.max_pool1d(X, kernel_size=k_max)\n",
    "#         return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T15:50:59.023740Z",
     "end_time": "2023-04-07T15:50:59.325715Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence Similarity Convolution Network (SSCN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pooling Layers\n",
    "\n",
    "The original paper used a dynamic k-max pooling method in their model. The _k_ value is determine by equation (1).\n",
    "\n",
    "\\begin{equation*} k=\\max \\left({k_{top},\\left \\lceil{ \\frac {L-l}{L} \\left |{ s }\\right | }\\right \\rceil }\\right)\\end{equation*}\n",
    "\n",
    "__Dynamic K-Max Pooling Implementation:__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "outputs": [],
   "source": [
    "#https://gist.github.com/anna-hope/7a2b2e66c3645aa8e4f94dbf06aed8dc\n",
    "\"\"\"\n",
    "TODO: Remove hardcoded values and max it dynamic.\n",
    "\"\"\"\n",
    "class DynamicKMaxPooling(nn.Module):\n",
    "    def __init__(self, k_init, conv_layers):\n",
    "        super().__init__()\n",
    "        # \"L is the total  number  of  convolutional  layers\n",
    "        # in  the  network;\n",
    "        # ktop is the fixed pooling parameter for the\n",
    "        # topmost  convolutional  layer\"\n",
    "        self.k_init = k_init\n",
    "        self.conv_layers = conv_layers\n",
    "\n",
    "    def pool(self, X, l):\n",
    "        # s is sequence length\n",
    "        # l is current layer in network\n",
    "        s = X.shape[2]\n",
    "        dyn_k = ((self.conv_layers - l) / self.conv_layers) * s\n",
    "        k_max = int(round(max(self.k_init, np.ceil(dyn_k))))\n",
    "        return F.max_pool1d(X, kernel_size=k_max)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer_i in range(self.conv_layers,0,-1):\n",
    "            X = self.pool(X, layer_i)\n",
    "\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.384185Z",
     "end_time": "2023-04-08T00:35:09.528064Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing Dynamic K-Max Pooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "outputs": [],
   "source": [
    "NUM_OF_SAMPLES = 20\n",
    "SAMPLE_SIZE = 2\n",
    "OUTPUT_SIZE = 15\n",
    "\n",
    "test_embedding = torch.rand((NUM_OF_SAMPLES, SAMPLE_SIZE, OUTPUT_SIZE))\n",
    "dyn_k_layer = DynamicKMaxPooling(3, SAMPLE_SIZE)\n",
    "\n",
    "# Call forward with convolution layer index [2,1]\n",
    "out = dyn_k_layer(test_embedding)\n",
    "\n",
    "assert out.shape[2] == 1\n",
    "assert out.shape[1] == SAMPLE_SIZE\n",
    "assert out.shape[0] == NUM_OF_SAMPLES"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.384376Z",
     "end_time": "2023-04-08T00:35:09.528155Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sentence Similarity\n",
    "\n",
    " \\begin{align*} Man(\\vec V_{x}, \\vec V_{y})=&\\left |{ x_{1}-y_{1} }\\right |\\! +\\! \\left |{ x_{2}-y_{2} }\\right | \\!+ \\!\\ldots \\!+ \\!\\left |{ x_{n}-y_{n} }\\right |\n",
    " \\\\ score=&e^{-Man(\\vec V_{x}, \\vec V_{y})},\\quad score\\in [{0,1}] \\end{align*}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* X: Pooled output of SSCN model of shape (sample_size, -1)\n",
    "* For the purpose of this experiment sample_size = 2\n",
    "\"\"\"\n",
    "def manhattan_similarity_score(X):\n",
    "    sample_count, _, M = X.shape\n",
    "    Vx = X[:,0].reshape((sample_count,M))\n",
    "    Vy = X[:,1].reshape((sample_count,M))\n",
    "    mdist = torch.sum(torch.abs(Vx-Vy),dim=1).view(sample_count,-1)\n",
    "    score = torch.exp(-1*mdist)\n",
    "    return score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.385728Z",
     "end_time": "2023-04-08T00:35:09.528218Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "outputs": [],
   "source": [
    "class KMaxPool1d(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super(KMaxPool1d, self).__init__()\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input shape (batch_size, num_channels, sequence_length)\n",
    "        # output shape (batch_size num_channels, k)\n",
    "        k_max_values, k_max_indices  = torch.topk(x, self.k, dim=2)\n",
    "        return k_max_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "outputs": [],
   "source": [
    "class DynamicKMaxPoolId(nn.Module):\n",
    "    def __init__(self, k, l, L):\n",
    "        super(DynamicKMaxPoolId, self).__init__()\n",
    "        self.k = k\n",
    "        self.l = l\n",
    "        self.L = L\n",
    "\n",
    "    def forward(self, x, sentence_length):\n",
    "        ktop = max(self.k, int((self.L - self.l)/self.L * sentence_length))\n",
    "        #print(f\"ktop: {ktop}\")\n",
    "        k_max_values, k_max_indices = torch.topk(x, ktop, dim=2)\n",
    "        return k_max_values\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentenceSimilarityCNN2(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_filters, filter_size, hidden_dim, dropout_prob=0.5):\n",
    "        super(SentenceSimilarityCNN2, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=filter_size, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels= num_filters, out_channels=num_filters * 2, kernel_size=filter_size, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels= num_filters * 2, out_channels=num_filters * 3, kernel_size=filter_size, padding=1)\n",
    "\n",
    "        self.pool1 = DynamicKMaxPoolId(k=3, l=1, L=3)\n",
    "\n",
    "        self.pool2 = DynamicKMaxPoolId(k=3, l=2, L=3)\n",
    "\n",
    "        self.k = 3\n",
    "        self.kmaxPool1d = KMaxPool1d(k=self.k)\n",
    "\n",
    "        #self.fc1 = nn.Linear(self.k * num_filters , hidden_dim)\n",
    "        self.fc1 = nn.Linear(self.k * num_filters * 3, hidden_dim)\n",
    "\n",
    "    def forward(self, input1_embedded, input2_embedded):\n",
    "\n",
    "        # input: input1_embedded is sentence1 and input2_embedding is sentence2\n",
    "        # input shape: (batch_size, max_sentence_length, embedding_size)\n",
    "        # output shap: (50) dimension vector that represents the sentence\n",
    "        # output: similarity score\n",
    "\n",
    "        # Find the sentence lengths\n",
    "        sent_length1 = (torch.nonzero(input1_embedded).max(dim=0).values[1] + 1).item()\n",
    "        sent_length2 = (torch.nonzero(input2_embedded).max(dim=0).values[1] + 1).item()\n",
    "        #print(f\"sent length1: {sent_length1} AND sent length2: {sent_length2}\")\n",
    "\n",
    "        # Convolution\n",
    "        #print(input1_embedded.shape)\n",
    "        # input shape (batch_size, max_sentence_length, embedding_size)\n",
    "        # permuted shape (batch_size, embedding_size, max_sentence_length)\n",
    "        # output shape (batch_size, out_channels, (max_sentence_length - kernel_size + 2*padding)/stride + 1\n",
    "        # i.e. if max_sentence_length=19 and kernel_size=3 and padding=1 and stride=1 and out_channels=64\n",
    "        #      output_shape (19-3+2*1)/1 + 1 = 19 => (64, 64, 19)\n",
    "        input1_embedded = self.conv1(input1_embedded.permute(0, 2, 1))\n",
    "        input2_embedded = self.conv1(input2_embedded.permute(0, 2, 1))\n",
    "        #print(f\"output of conv1: {input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = torch.relu(input1_embedded)\n",
    "        input2_embedded = torch.relu(input2_embedded)\n",
    "\n",
    "        # pool1 is dynamic k-max pooling and is a function of the following formula\n",
    "        # max(k-top, (L-l)/l * |s|) where L is total number of convolutional layers, l is the current convolution layer and |s| is sentence length k-top is the\n",
    "        # k-top important features and serves as a lower bound... we will always try and find at least k-top features\n",
    "        input1_embedded = self.pool1(input1_embedded, sent_length1)\n",
    "        input2_embedded = self.pool1(input2_embedded, sent_length2)\n",
    "        #print(f\"output of pool1: {input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = self.conv2(input1_embedded)\n",
    "        input2_embedded = self.conv2(input2_embedded)\n",
    "        #print(f\"output of conv2: {input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = torch.relu(input1_embedded)\n",
    "        input2_embedded = torch.relu(input2_embedded)\n",
    "\n",
    "        input1_embedded = self.pool2(input1_embedded, sent_length1)\n",
    "        input2_embedded = self.pool2(input2_embedded, sent_length2)\n",
    "        #print(f\"output of pool2: {input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = self.conv3(input1_embedded)\n",
    "        input2_embedded = self.conv3(input2_embedded)\n",
    "        #print(f\"output of conv3: {input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = torch.relu(input1_embedded)\n",
    "        input2_embedded = torch.relu(input2_embedded)\n",
    "\n",
    "        input1_embedded = self.kmaxPool1d(input1_embedded)\n",
    "        input2_embedded = self.kmaxPool1d(input2_embedded)\n",
    "        #print(f\"output of k-max pool: {input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = input1_embedded.view(input1_embedded.shape[0], input1_embedded.shape[1] * input1_embedded.shape[2])\n",
    "        input2_embedded = input2_embedded.view(input2_embedded.shape[0], input2_embedded.shape[1] * input2_embedded.shape[2])\n",
    "\n",
    "        #kmax_input1_embedded = self.kmaxPool1d(input1_embedded)\n",
    "        #input1_embedded = F.max_pool1d(input1_embedded, input1_embedded.shape[2]).squeeze(2)\n",
    "        #nput2_embedded = F.max_pool1d(input2_embedded, input2_embedded.shape[2]).squeeze(2)\n",
    "        #print(f\"output of max pool: {input1_embedded.shape}\")\n",
    "        #print(f\"output of kmax pool: {kmax_input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = self.fc1(input1_embedded)\n",
    "        input2_embedded = self.fc1(input2_embedded)\n",
    "        #print(input1_embedded.shape)\n",
    "\n",
    "        man_dist = torch.sum(torch.abs(input1_embedded - input2_embedded), axis=1)\n",
    "        # sentence1_mean = torch.mean(x1, axis=1)\n",
    "        # sentence2_mean = torch.mean(x2, axis=1)\n",
    "        # man_dist = torch.sum(torch.abs(sentence1_mean - sentence2_mean), axis=1)\n",
    "        # print(man_dist.shape)\n",
    "\n",
    "        return torch.exp(-man_dist)\n",
    "\n",
    "\n",
    "        #input2_embedded = self.conv1(input2_embedded.permute(0, 2, 1))\n",
    "\n",
    "\n",
    "        # Max pooling\n",
    "        # input1_pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in input1_conv]\n",
    "        # input2_pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in input2_conv]\n",
    "        #\n",
    "        # # Concatenate and flatten\n",
    "        # input1_concat = torch.cat(input1_pooled, dim=1)\n",
    "        # input2_concat = torch.cat(input2_pooled, dim=1)\n",
    "        #\n",
    "        # # Concatenate the two sentence representations\n",
    "        # sentence_similarity = torch.cat([input1_concat, input2_concat], dim=1)\n",
    "        #\n",
    "        # # Dense layers\n",
    "        # #sentence_similarity = self.dropout(F.relu(self.fc1(sentence_similarity)))\n",
    "        # #sentence_similarity = self.fc2(sentence_similarity)\n",
    "        #\n",
    "        # return torch.sigmoid(sentence_similarity)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: curr_epoch_loss=0.22881031036376953\n",
      "Epoch 1: curr_epoch_loss=0.2196347564458847\n",
      "Epoch 2: curr_epoch_loss=0.21538543701171875\n",
      "Epoch 3: curr_epoch_loss=0.21288608014583588\n",
      "Epoch 4: curr_epoch_loss=0.21000337600708008\n",
      "Epoch 5: curr_epoch_loss=0.2080463171005249\n",
      "Epoch 6: curr_epoch_loss=0.20674332976341248\n",
      "Epoch 7: curr_epoch_loss=0.2055101841688156\n",
      "Epoch 8: curr_epoch_loss=0.20370227098464966\n",
      "Epoch 9: curr_epoch_loss=0.20260974764823914\n",
      "Epoch 10: curr_epoch_loss=0.20119041204452515\n",
      "Epoch 11: curr_epoch_loss=0.19992920756340027\n",
      "Epoch 12: curr_epoch_loss=0.19951796531677246\n",
      "Epoch 13: curr_epoch_loss=0.19870170950889587\n",
      "Epoch 14: curr_epoch_loss=0.19799281656742096\n",
      "Epoch 15: curr_epoch_loss=0.19702181220054626\n",
      "Epoch 16: curr_epoch_loss=0.1961609125137329\n",
      "Epoch 17: curr_epoch_loss=0.1960534155368805\n",
      "Epoch 18: curr_epoch_loss=0.19516965746879578\n",
      "Epoch 19: curr_epoch_loss=0.1937691867351532\n",
      "Epoch 20: curr_epoch_loss=0.19254228472709656\n",
      "Epoch 21: curr_epoch_loss=0.191432923078537\n",
      "Epoch 22: curr_epoch_loss=0.19097191095352173\n",
      "Epoch 23: curr_epoch_loss=0.1905718892812729\n",
      "Epoch 24: curr_epoch_loss=0.1889382302761078\n",
      "Epoch 25: curr_epoch_loss=0.1888921856880188\n",
      "Epoch 26: curr_epoch_loss=0.1879897117614746\n",
      "Epoch 27: curr_epoch_loss=0.18614229559898376\n",
      "Epoch 28: curr_epoch_loss=0.18643814325332642\n",
      "Epoch 29: curr_epoch_loss=0.1845138967037201\n",
      "Epoch 30: curr_epoch_loss=0.18515460193157196\n",
      "Epoch 31: curr_epoch_loss=0.18480882048606873\n",
      "Epoch 32: curr_epoch_loss=0.183502197265625\n",
      "Epoch 33: curr_epoch_loss=0.18381112813949585\n",
      "Epoch 34: curr_epoch_loss=0.18272043764591217\n",
      "Epoch 35: curr_epoch_loss=0.18187487125396729\n",
      "Epoch 36: curr_epoch_loss=0.1823331117630005\n",
      "Epoch 37: curr_epoch_loss=0.1814306676387787\n",
      "Epoch 38: curr_epoch_loss=0.18039445579051971\n",
      "Epoch 39: curr_epoch_loss=0.18107548356056213\n",
      "Epoch 40: curr_epoch_loss=0.1812725067138672\n",
      "Epoch 41: curr_epoch_loss=0.17879731953144073\n",
      "Epoch 42: curr_epoch_loss=0.17982304096221924\n",
      "Epoch 43: curr_epoch_loss=0.17980116605758667\n",
      "Epoch 44: curr_epoch_loss=0.1786729395389557\n",
      "Epoch 45: curr_epoch_loss=0.18019014596939087\n",
      "Epoch 46: curr_epoch_loss=0.17878508567810059\n",
      "Epoch 47: curr_epoch_loss=0.17799395322799683\n",
      "Epoch 48: curr_epoch_loss=0.1793108582496643\n",
      "Epoch 49: curr_epoch_loss=0.17986726760864258\n",
      "Epoch 50: curr_epoch_loss=0.1777971088886261\n",
      "Epoch 51: curr_epoch_loss=0.1765970140695572\n",
      "Epoch 52: curr_epoch_loss=0.17510828375816345\n",
      "Epoch 53: curr_epoch_loss=0.17623718082904816\n",
      "Epoch 54: curr_epoch_loss=0.17861151695251465\n",
      "Epoch 55: curr_epoch_loss=0.17347374558448792\n",
      "Epoch 56: curr_epoch_loss=0.1739741861820221\n",
      "Epoch 57: curr_epoch_loss=0.17304903268814087\n",
      "Epoch 58: curr_epoch_loss=0.17441235482692719\n",
      "Epoch 59: curr_epoch_loss=0.1741812825202942\n",
      "Epoch 60: curr_epoch_loss=0.17756116390228271\n",
      "Epoch 61: curr_epoch_loss=0.1764196753501892\n",
      "Epoch 62: curr_epoch_loss=0.1754000037908554\n",
      "Epoch 63: curr_epoch_loss=0.17481686174869537\n",
      "Epoch 64: curr_epoch_loss=0.174812451004982\n",
      "Epoch 65: curr_epoch_loss=0.1711721122264862\n",
      "Epoch 66: curr_epoch_loss=0.17324349284172058\n",
      "Epoch 67: curr_epoch_loss=0.17565354704856873\n",
      "Epoch 68: curr_epoch_loss=0.17491021752357483\n",
      "Epoch 69: curr_epoch_loss=0.17466428875923157\n",
      "Epoch 70: curr_epoch_loss=0.17322996258735657\n",
      "Epoch 71: curr_epoch_loss=0.17366260290145874\n",
      "Epoch 72: curr_epoch_loss=0.1738249957561493\n",
      "Epoch 73: curr_epoch_loss=0.1736110895872116\n",
      "Epoch 74: curr_epoch_loss=0.17470033466815948\n",
      "Epoch 75: curr_epoch_loss=0.1733928918838501\n",
      "Epoch 76: curr_epoch_loss=0.17288917303085327\n",
      "Epoch 77: curr_epoch_loss=0.17111563682556152\n",
      "Epoch 78: curr_epoch_loss=0.17205366492271423\n",
      "Epoch 79: curr_epoch_loss=0.17618830502033234\n",
      "Epoch 80: curr_epoch_loss=0.16839677095413208\n",
      "Epoch 81: curr_epoch_loss=0.17006491124629974\n",
      "Epoch 82: curr_epoch_loss=0.1702151596546173\n",
      "Epoch 83: curr_epoch_loss=0.16917455196380615\n",
      "Epoch 84: curr_epoch_loss=0.16584664583206177\n",
      "Epoch 85: curr_epoch_loss=0.1687156856060028\n",
      "Epoch 86: curr_epoch_loss=0.17000865936279297\n",
      "Epoch 87: curr_epoch_loss=0.17006459832191467\n",
      "Epoch 88: curr_epoch_loss=0.16963927447795868\n",
      "Epoch 89: curr_epoch_loss=0.16886380314826965\n",
      "Epoch 90: curr_epoch_loss=0.16615420579910278\n",
      "Epoch 91: curr_epoch_loss=0.16774743795394897\n",
      "Epoch 92: curr_epoch_loss=0.16840697824954987\n",
      "Epoch 93: curr_epoch_loss=0.16376489400863647\n",
      "Epoch 94: curr_epoch_loss=0.1680026352405548\n",
      "Epoch 95: curr_epoch_loss=0.16440311074256897\n",
      "Epoch 96: curr_epoch_loss=0.163874551653862\n",
      "Epoch 97: curr_epoch_loss=0.1630394607782364\n",
      "Epoch 98: curr_epoch_loss=0.16342833638191223\n",
      "Epoch 99: curr_epoch_loss=0.16120539605617523\n",
      "Epoch 100: curr_epoch_loss=0.16682782769203186\n",
      "Epoch 101: curr_epoch_loss=0.16728070378303528\n",
      "Epoch 102: curr_epoch_loss=0.16140209138393402\n",
      "Epoch 103: curr_epoch_loss=0.16455021500587463\n",
      "Epoch 104: curr_epoch_loss=0.16821470856666565\n",
      "Epoch 105: curr_epoch_loss=0.16568097472190857\n",
      "Epoch 106: curr_epoch_loss=0.1623186320066452\n",
      "Epoch 107: curr_epoch_loss=0.1659942865371704\n",
      "Epoch 108: curr_epoch_loss=0.17037883400917053\n",
      "Epoch 109: curr_epoch_loss=0.1690365970134735\n",
      "Epoch 110: curr_epoch_loss=0.16979750990867615\n",
      "Epoch 111: curr_epoch_loss=0.16486096382141113\n",
      "Epoch 112: curr_epoch_loss=0.16248013079166412\n",
      "Epoch 113: curr_epoch_loss=0.16430765390396118\n",
      "Epoch 114: curr_epoch_loss=0.16216884553432465\n",
      "Epoch 115: curr_epoch_loss=0.1598525196313858\n",
      "Epoch 116: curr_epoch_loss=0.15926522016525269\n",
      "Epoch 117: curr_epoch_loss=0.15809360146522522\n",
      "Epoch 118: curr_epoch_loss=0.15460827946662903\n",
      "Epoch 119: curr_epoch_loss=0.15598233044147491\n",
      "Epoch 120: curr_epoch_loss=0.15853269398212433\n",
      "Epoch 121: curr_epoch_loss=0.16000890731811523\n",
      "Epoch 122: curr_epoch_loss=0.15683799982070923\n",
      "Epoch 123: curr_epoch_loss=0.15687626600265503\n",
      "Epoch 124: curr_epoch_loss=0.15987694263458252\n",
      "Epoch 125: curr_epoch_loss=0.1538311243057251\n",
      "Epoch 126: curr_epoch_loss=0.15308593213558197\n",
      "Epoch 127: curr_epoch_loss=0.15954098105430603\n",
      "Epoch 128: curr_epoch_loss=0.1532127857208252\n",
      "Epoch 129: curr_epoch_loss=0.15255342423915863\n",
      "Epoch 130: curr_epoch_loss=0.15635737776756287\n",
      "Epoch 131: curr_epoch_loss=0.15897653996944427\n",
      "Epoch 132: curr_epoch_loss=0.15496516227722168\n",
      "Epoch 133: curr_epoch_loss=0.15838873386383057\n",
      "Epoch 134: curr_epoch_loss=0.15821361541748047\n",
      "Epoch 135: curr_epoch_loss=0.1524365246295929\n",
      "Epoch 136: curr_epoch_loss=0.1508665382862091\n",
      "Epoch 137: curr_epoch_loss=0.15179625153541565\n",
      "Epoch 138: curr_epoch_loss=0.15109649300575256\n",
      "Epoch 139: curr_epoch_loss=0.15101709961891174\n",
      "Epoch 140: curr_epoch_loss=0.15226170420646667\n",
      "Epoch 141: curr_epoch_loss=0.15159274637699127\n",
      "Epoch 142: curr_epoch_loss=0.15279312431812286\n",
      "Epoch 143: curr_epoch_loss=0.15336079895496368\n",
      "Epoch 144: curr_epoch_loss=0.15924620628356934\n",
      "Epoch 145: curr_epoch_loss=0.15247347950935364\n",
      "Epoch 146: curr_epoch_loss=0.14980356395244598\n",
      "Epoch 147: curr_epoch_loss=0.15226277709007263\n",
      "Epoch 148: curr_epoch_loss=0.15199652314186096\n",
      "Epoch 149: curr_epoch_loss=0.14802035689353943\n",
      "Number of epochs:  150\n",
      "Training dataset size 4076\n",
      "Training time:  0:02:34.247679\n",
      "Epoch 0: curr_epoch_loss=0.22949358820915222\n",
      "Epoch 1: curr_epoch_loss=0.21967154741287231\n",
      "Epoch 2: curr_epoch_loss=0.21529051661491394\n",
      "Epoch 3: curr_epoch_loss=0.21217772364616394\n",
      "Epoch 4: curr_epoch_loss=0.20978638529777527\n",
      "Epoch 5: curr_epoch_loss=0.207966148853302\n",
      "Epoch 6: curr_epoch_loss=0.20727737247943878\n",
      "Epoch 7: curr_epoch_loss=0.20623978972434998\n",
      "Epoch 8: curr_epoch_loss=0.2045784592628479\n",
      "Epoch 9: curr_epoch_loss=0.20340968668460846\n",
      "Epoch 10: curr_epoch_loss=0.20229098200798035\n",
      "Epoch 11: curr_epoch_loss=0.20140305161476135\n",
      "Epoch 12: curr_epoch_loss=0.20011034607887268\n",
      "Epoch 13: curr_epoch_loss=0.1993354856967926\n",
      "Epoch 14: curr_epoch_loss=0.19888269901275635\n",
      "Epoch 15: curr_epoch_loss=0.1981506049633026\n",
      "Epoch 16: curr_epoch_loss=0.1971433460712433\n",
      "Epoch 17: curr_epoch_loss=0.19615538418293\n",
      "Epoch 18: curr_epoch_loss=0.19572079181671143\n",
      "Epoch 19: curr_epoch_loss=0.19490577280521393\n",
      "Epoch 20: curr_epoch_loss=0.19425326585769653\n",
      "Epoch 21: curr_epoch_loss=0.193466454744339\n",
      "Epoch 22: curr_epoch_loss=0.19260814785957336\n",
      "Epoch 23: curr_epoch_loss=0.1909278929233551\n",
      "Epoch 24: curr_epoch_loss=0.18999651074409485\n",
      "Epoch 25: curr_epoch_loss=0.1901731789112091\n",
      "Epoch 26: curr_epoch_loss=0.1891053318977356\n",
      "Epoch 27: curr_epoch_loss=0.18951095640659332\n",
      "Epoch 28: curr_epoch_loss=0.18779116868972778\n",
      "Epoch 29: curr_epoch_loss=0.1875792145729065\n",
      "Epoch 30: curr_epoch_loss=0.18669873476028442\n",
      "Epoch 31: curr_epoch_loss=0.18638403713703156\n",
      "Epoch 32: curr_epoch_loss=0.1845427006483078\n",
      "Epoch 33: curr_epoch_loss=0.18501903116703033\n",
      "Epoch 34: curr_epoch_loss=0.1865900456905365\n",
      "Epoch 35: curr_epoch_loss=0.18486636877059937\n",
      "Epoch 36: curr_epoch_loss=0.18324656784534454\n",
      "Epoch 37: curr_epoch_loss=0.18084485828876495\n",
      "Epoch 38: curr_epoch_loss=0.17966574430465698\n",
      "Epoch 39: curr_epoch_loss=0.1825931966304779\n",
      "Epoch 40: curr_epoch_loss=0.18140041828155518\n",
      "Epoch 41: curr_epoch_loss=0.1801588237285614\n",
      "Epoch 42: curr_epoch_loss=0.17921321094036102\n",
      "Epoch 43: curr_epoch_loss=0.18006056547164917\n",
      "Epoch 44: curr_epoch_loss=0.18051880598068237\n",
      "Epoch 45: curr_epoch_loss=0.17955031991004944\n",
      "Epoch 46: curr_epoch_loss=0.1793457567691803\n",
      "Epoch 47: curr_epoch_loss=0.17753809690475464\n",
      "Epoch 48: curr_epoch_loss=0.17789442837238312\n",
      "Epoch 49: curr_epoch_loss=0.17696857452392578\n",
      "Epoch 50: curr_epoch_loss=0.17815297842025757\n",
      "Epoch 51: curr_epoch_loss=0.17778396606445312\n",
      "Epoch 52: curr_epoch_loss=0.1772938072681427\n",
      "Epoch 53: curr_epoch_loss=0.18173182010650635\n",
      "Epoch 54: curr_epoch_loss=0.17859357595443726\n",
      "Epoch 55: curr_epoch_loss=0.18083137273788452\n",
      "Epoch 56: curr_epoch_loss=0.1761820912361145\n",
      "Epoch 57: curr_epoch_loss=0.17631329596042633\n",
      "Epoch 58: curr_epoch_loss=0.17649546265602112\n",
      "Epoch 59: curr_epoch_loss=0.17777566611766815\n",
      "Epoch 60: curr_epoch_loss=0.17661809921264648\n",
      "Epoch 61: curr_epoch_loss=0.17495636641979218\n",
      "Epoch 62: curr_epoch_loss=0.17629766464233398\n",
      "Epoch 63: curr_epoch_loss=0.17551013827323914\n",
      "Epoch 64: curr_epoch_loss=0.17617356777191162\n",
      "Epoch 65: curr_epoch_loss=0.17547962069511414\n",
      "Epoch 66: curr_epoch_loss=0.173922598361969\n",
      "Epoch 67: curr_epoch_loss=0.17320533096790314\n",
      "Epoch 68: curr_epoch_loss=0.1799786239862442\n",
      "Epoch 69: curr_epoch_loss=0.1755877286195755\n",
      "Epoch 70: curr_epoch_loss=0.17473773658275604\n",
      "Epoch 71: curr_epoch_loss=0.17327368259429932\n",
      "Epoch 72: curr_epoch_loss=0.17515358328819275\n",
      "Epoch 73: curr_epoch_loss=0.1742216944694519\n",
      "Epoch 74: curr_epoch_loss=0.1724955439567566\n",
      "Epoch 75: curr_epoch_loss=0.17022919654846191\n",
      "Epoch 76: curr_epoch_loss=0.17322145402431488\n",
      "Epoch 77: curr_epoch_loss=0.1713065505027771\n",
      "Epoch 78: curr_epoch_loss=0.1676529347896576\n",
      "Epoch 79: curr_epoch_loss=0.1668960303068161\n",
      "Epoch 80: curr_epoch_loss=0.16811467707157135\n",
      "Epoch 81: curr_epoch_loss=0.16722065210342407\n",
      "Epoch 82: curr_epoch_loss=0.1656723916530609\n",
      "Epoch 83: curr_epoch_loss=0.16674205660820007\n",
      "Epoch 84: curr_epoch_loss=0.16441816091537476\n",
      "Epoch 85: curr_epoch_loss=0.16530311107635498\n",
      "Epoch 86: curr_epoch_loss=0.16439896821975708\n",
      "Epoch 87: curr_epoch_loss=0.1636832356452942\n",
      "Epoch 88: curr_epoch_loss=0.16386128962039948\n",
      "Epoch 89: curr_epoch_loss=0.162124365568161\n",
      "Epoch 90: curr_epoch_loss=0.16413164138793945\n",
      "Epoch 91: curr_epoch_loss=0.1669996678829193\n",
      "Epoch 92: curr_epoch_loss=0.16824612021446228\n",
      "Epoch 93: curr_epoch_loss=0.1688237339258194\n",
      "Epoch 94: curr_epoch_loss=0.16592063009738922\n",
      "Epoch 95: curr_epoch_loss=0.17095372080802917\n",
      "Epoch 96: curr_epoch_loss=0.16615352034568787\n",
      "Epoch 97: curr_epoch_loss=0.1657482236623764\n",
      "Epoch 98: curr_epoch_loss=0.16309505701065063\n",
      "Epoch 99: curr_epoch_loss=0.164586141705513\n",
      "Epoch 100: curr_epoch_loss=0.1607474982738495\n",
      "Epoch 101: curr_epoch_loss=0.16684164106845856\n",
      "Epoch 102: curr_epoch_loss=0.15990589559078217\n",
      "Epoch 103: curr_epoch_loss=0.16378146409988403\n",
      "Epoch 104: curr_epoch_loss=0.16240227222442627\n",
      "Epoch 105: curr_epoch_loss=0.16190636157989502\n",
      "Epoch 106: curr_epoch_loss=0.1608923375606537\n",
      "Epoch 107: curr_epoch_loss=0.1626497209072113\n",
      "Epoch 108: curr_epoch_loss=0.1602083295583725\n",
      "Epoch 109: curr_epoch_loss=0.16142746806144714\n",
      "Epoch 110: curr_epoch_loss=0.16106730699539185\n",
      "Epoch 111: curr_epoch_loss=0.15562747418880463\n",
      "Epoch 112: curr_epoch_loss=0.15655209124088287\n",
      "Epoch 113: curr_epoch_loss=0.1651078462600708\n",
      "Epoch 114: curr_epoch_loss=0.15888339281082153\n",
      "Epoch 115: curr_epoch_loss=0.16786953806877136\n",
      "Epoch 116: curr_epoch_loss=0.16881516575813293\n",
      "Epoch 117: curr_epoch_loss=0.1634681522846222\n",
      "Epoch 118: curr_epoch_loss=0.1602395474910736\n",
      "Epoch 119: curr_epoch_loss=0.1556425392627716\n",
      "Epoch 120: curr_epoch_loss=0.15719905495643616\n",
      "Epoch 121: curr_epoch_loss=0.15992753207683563\n",
      "Epoch 122: curr_epoch_loss=0.15611404180526733\n",
      "Epoch 123: curr_epoch_loss=0.15334901213645935\n",
      "Epoch 124: curr_epoch_loss=0.14964962005615234\n",
      "Epoch 125: curr_epoch_loss=0.15465016663074493\n",
      "Epoch 126: curr_epoch_loss=0.14856579899787903\n",
      "Epoch 127: curr_epoch_loss=0.14473488926887512\n",
      "Epoch 128: curr_epoch_loss=0.14702445268630981\n",
      "Epoch 129: curr_epoch_loss=0.15133629739284515\n",
      "Epoch 130: curr_epoch_loss=0.1498461216688156\n",
      "Epoch 131: curr_epoch_loss=0.15077075362205505\n",
      "Epoch 132: curr_epoch_loss=0.1519598662853241\n",
      "Epoch 133: curr_epoch_loss=0.15003253519535065\n",
      "Epoch 134: curr_epoch_loss=0.15176060795783997\n",
      "Epoch 135: curr_epoch_loss=0.14588946104049683\n",
      "Epoch 136: curr_epoch_loss=0.1439487785100937\n",
      "Epoch 137: curr_epoch_loss=0.14717665314674377\n",
      "Epoch 138: curr_epoch_loss=0.1396356225013733\n",
      "Epoch 139: curr_epoch_loss=0.14348085224628448\n",
      "Epoch 140: curr_epoch_loss=0.14958405494689941\n",
      "Epoch 141: curr_epoch_loss=0.14707665145397186\n",
      "Epoch 142: curr_epoch_loss=0.14300556480884552\n",
      "Epoch 143: curr_epoch_loss=0.14444565773010254\n",
      "Epoch 144: curr_epoch_loss=0.14867368340492249\n",
      "Epoch 145: curr_epoch_loss=0.1427236944437027\n",
      "Epoch 146: curr_epoch_loss=0.14164923131465912\n",
      "Epoch 147: curr_epoch_loss=0.13841569423675537\n",
      "Epoch 148: curr_epoch_loss=0.13639216125011444\n",
      "Epoch 149: curr_epoch_loss=0.13714545965194702\n",
      "Number of epochs:  150\n",
      "Training dataset size 4076\n",
      "Training time:  0:02:29.876131\n",
      "Epoch 0: curr_epoch_loss=0.22900882363319397\n",
      "Epoch 1: curr_epoch_loss=0.21847987174987793\n",
      "Epoch 2: curr_epoch_loss=0.21450306475162506\n",
      "Epoch 3: curr_epoch_loss=0.212331622838974\n",
      "Epoch 4: curr_epoch_loss=0.20966124534606934\n",
      "Epoch 5: curr_epoch_loss=0.20768402516841888\n",
      "Epoch 6: curr_epoch_loss=0.20618396997451782\n",
      "Epoch 7: curr_epoch_loss=0.20422843098640442\n",
      "Epoch 8: curr_epoch_loss=0.20321553945541382\n",
      "Epoch 9: curr_epoch_loss=0.20268473029136658\n",
      "Epoch 10: curr_epoch_loss=0.2007167637348175\n",
      "Epoch 11: curr_epoch_loss=0.19980591535568237\n",
      "Epoch 12: curr_epoch_loss=0.19891488552093506\n",
      "Epoch 13: curr_epoch_loss=0.19855579733848572\n",
      "Epoch 14: curr_epoch_loss=0.19665275514125824\n",
      "Epoch 15: curr_epoch_loss=0.19595634937286377\n",
      "Epoch 16: curr_epoch_loss=0.1948927342891693\n",
      "Epoch 17: curr_epoch_loss=0.19412928819656372\n",
      "Epoch 18: curr_epoch_loss=0.1936153620481491\n",
      "Epoch 19: curr_epoch_loss=0.1924976110458374\n",
      "Epoch 20: curr_epoch_loss=0.19138656556606293\n",
      "Epoch 21: curr_epoch_loss=0.19048385322093964\n",
      "Epoch 22: curr_epoch_loss=0.18999925255775452\n",
      "Epoch 23: curr_epoch_loss=0.18817053735256195\n",
      "Epoch 24: curr_epoch_loss=0.18745270371437073\n",
      "Epoch 25: curr_epoch_loss=0.18754981458187103\n",
      "Epoch 26: curr_epoch_loss=0.1868188977241516\n",
      "Epoch 27: curr_epoch_loss=0.18661019206047058\n",
      "Epoch 28: curr_epoch_loss=0.1849173605442047\n",
      "Epoch 29: curr_epoch_loss=0.18376553058624268\n",
      "Epoch 30: curr_epoch_loss=0.1829381287097931\n",
      "Epoch 31: curr_epoch_loss=0.18245889246463776\n",
      "Epoch 32: curr_epoch_loss=0.18204790353775024\n",
      "Epoch 33: curr_epoch_loss=0.18105335533618927\n",
      "Epoch 34: curr_epoch_loss=0.18066050112247467\n",
      "Epoch 35: curr_epoch_loss=0.18076100945472717\n",
      "Epoch 36: curr_epoch_loss=0.17981328070163727\n",
      "Epoch 37: curr_epoch_loss=0.17876401543617249\n",
      "Epoch 38: curr_epoch_loss=0.17868979275226593\n",
      "Epoch 39: curr_epoch_loss=0.1787405014038086\n",
      "Epoch 40: curr_epoch_loss=0.1797197163105011\n",
      "Epoch 41: curr_epoch_loss=0.1776231825351715\n",
      "Epoch 42: curr_epoch_loss=0.17798079550266266\n",
      "Epoch 43: curr_epoch_loss=0.1771479845046997\n",
      "Epoch 44: curr_epoch_loss=0.1747816503047943\n",
      "Epoch 45: curr_epoch_loss=0.17655621469020844\n",
      "Epoch 46: curr_epoch_loss=0.17617928981781006\n",
      "Epoch 47: curr_epoch_loss=0.1764235496520996\n",
      "Epoch 48: curr_epoch_loss=0.17537495493888855\n",
      "Epoch 49: curr_epoch_loss=0.17526261508464813\n",
      "Epoch 50: curr_epoch_loss=0.1788269579410553\n",
      "Epoch 51: curr_epoch_loss=0.17737434804439545\n",
      "Epoch 52: curr_epoch_loss=0.1751246303319931\n",
      "Epoch 53: curr_epoch_loss=0.17466260492801666\n",
      "Epoch 54: curr_epoch_loss=0.17546366155147552\n",
      "Epoch 55: curr_epoch_loss=0.17743727564811707\n",
      "Epoch 56: curr_epoch_loss=0.17555834352970123\n",
      "Epoch 57: curr_epoch_loss=0.17262297868728638\n",
      "Epoch 58: curr_epoch_loss=0.1730664074420929\n",
      "Epoch 59: curr_epoch_loss=0.17331653833389282\n",
      "Epoch 60: curr_epoch_loss=0.17251111567020416\n",
      "Epoch 61: curr_epoch_loss=0.17304539680480957\n",
      "Epoch 62: curr_epoch_loss=0.1741543710231781\n",
      "Epoch 63: curr_epoch_loss=0.1748848706483841\n",
      "Epoch 64: curr_epoch_loss=0.17169171571731567\n",
      "Epoch 65: curr_epoch_loss=0.1706714630126953\n",
      "Epoch 66: curr_epoch_loss=0.17120420932769775\n",
      "Epoch 67: curr_epoch_loss=0.1698865294456482\n",
      "Epoch 68: curr_epoch_loss=0.16925455629825592\n",
      "Epoch 69: curr_epoch_loss=0.17072513699531555\n",
      "Epoch 70: curr_epoch_loss=0.1697312891483307\n",
      "Epoch 71: curr_epoch_loss=0.16935688257217407\n",
      "Epoch 72: curr_epoch_loss=0.16885896027088165\n",
      "Epoch 73: curr_epoch_loss=0.17117199301719666\n",
      "Epoch 74: curr_epoch_loss=0.1671997606754303\n",
      "Epoch 75: curr_epoch_loss=0.1678118109703064\n",
      "Epoch 76: curr_epoch_loss=0.16750971972942352\n",
      "Epoch 77: curr_epoch_loss=0.16738633811473846\n",
      "Epoch 78: curr_epoch_loss=0.16545942425727844\n",
      "Epoch 79: curr_epoch_loss=0.1668679565191269\n",
      "Epoch 80: curr_epoch_loss=0.16611449420452118\n",
      "Epoch 81: curr_epoch_loss=0.16626444458961487\n",
      "Epoch 82: curr_epoch_loss=0.16728369891643524\n",
      "Epoch 83: curr_epoch_loss=0.16440430283546448\n",
      "Epoch 84: curr_epoch_loss=0.1640544831752777\n",
      "Epoch 85: curr_epoch_loss=0.17075417935848236\n",
      "Epoch 86: curr_epoch_loss=0.16677387058734894\n",
      "Epoch 87: curr_epoch_loss=0.16252724826335907\n",
      "Epoch 88: curr_epoch_loss=0.16394440829753876\n",
      "Epoch 89: curr_epoch_loss=0.1626858413219452\n",
      "Epoch 90: curr_epoch_loss=0.1687379777431488\n",
      "Epoch 91: curr_epoch_loss=0.16275213658809662\n",
      "Epoch 92: curr_epoch_loss=0.16529197990894318\n",
      "Epoch 93: curr_epoch_loss=0.1664453148841858\n",
      "Epoch 94: curr_epoch_loss=0.16725625097751617\n",
      "Epoch 95: curr_epoch_loss=0.1622571051120758\n",
      "Epoch 96: curr_epoch_loss=0.16179189085960388\n",
      "Epoch 97: curr_epoch_loss=0.1572737991809845\n",
      "Epoch 98: curr_epoch_loss=0.1617358922958374\n",
      "Epoch 99: curr_epoch_loss=0.1556154489517212\n",
      "Epoch 100: curr_epoch_loss=0.15883968770503998\n",
      "Epoch 101: curr_epoch_loss=0.15241292119026184\n",
      "Epoch 102: curr_epoch_loss=0.15184931457042694\n",
      "Epoch 103: curr_epoch_loss=0.15304280817508698\n",
      "Epoch 104: curr_epoch_loss=0.15212491154670715\n",
      "Epoch 105: curr_epoch_loss=0.15023595094680786\n",
      "Epoch 106: curr_epoch_loss=0.1545448899269104\n",
      "Epoch 107: curr_epoch_loss=0.15083429217338562\n",
      "Epoch 108: curr_epoch_loss=0.15387603640556335\n",
      "Epoch 109: curr_epoch_loss=0.15103140473365784\n",
      "Epoch 110: curr_epoch_loss=0.14641767740249634\n",
      "Epoch 111: curr_epoch_loss=0.15143154561519623\n",
      "Epoch 112: curr_epoch_loss=0.1550655961036682\n",
      "Epoch 113: curr_epoch_loss=0.14622153341770172\n",
      "Epoch 114: curr_epoch_loss=0.15386684238910675\n",
      "Epoch 115: curr_epoch_loss=0.14928767085075378\n",
      "Epoch 116: curr_epoch_loss=0.1485796868801117\n",
      "Epoch 117: curr_epoch_loss=0.14790084958076477\n",
      "Epoch 118: curr_epoch_loss=0.15254542231559753\n",
      "Epoch 119: curr_epoch_loss=0.15444302558898926\n",
      "Epoch 120: curr_epoch_loss=0.14698460698127747\n",
      "Epoch 121: curr_epoch_loss=0.14579050242900848\n",
      "Epoch 122: curr_epoch_loss=0.1450948417186737\n",
      "Epoch 123: curr_epoch_loss=0.14946162700653076\n",
      "Epoch 124: curr_epoch_loss=0.15410873293876648\n",
      "Epoch 125: curr_epoch_loss=0.1561964452266693\n",
      "Epoch 126: curr_epoch_loss=0.14880506694316864\n",
      "Epoch 127: curr_epoch_loss=0.15603682398796082\n",
      "Epoch 128: curr_epoch_loss=0.14865592122077942\n",
      "Epoch 129: curr_epoch_loss=0.14804020524024963\n",
      "Epoch 130: curr_epoch_loss=0.14523762464523315\n",
      "Epoch 131: curr_epoch_loss=0.14127129316329956\n",
      "Epoch 132: curr_epoch_loss=0.14722192287445068\n",
      "Epoch 133: curr_epoch_loss=0.15305748581886292\n",
      "Epoch 134: curr_epoch_loss=0.14733999967575073\n",
      "Epoch 135: curr_epoch_loss=0.14408588409423828\n",
      "Epoch 136: curr_epoch_loss=0.1450989842414856\n",
      "Epoch 137: curr_epoch_loss=0.1401757299900055\n",
      "Epoch 138: curr_epoch_loss=0.13954788446426392\n",
      "Epoch 139: curr_epoch_loss=0.13994605839252472\n",
      "Epoch 140: curr_epoch_loss=0.14012141525745392\n",
      "Epoch 141: curr_epoch_loss=0.1423155665397644\n",
      "Epoch 142: curr_epoch_loss=0.14023113250732422\n",
      "Epoch 143: curr_epoch_loss=0.13924294710159302\n",
      "Epoch 144: curr_epoch_loss=0.14280655980110168\n",
      "Epoch 145: curr_epoch_loss=0.14606527984142303\n",
      "Epoch 146: curr_epoch_loss=0.13572023808956146\n",
      "Epoch 147: curr_epoch_loss=0.13506357371807098\n",
      "Epoch 148: curr_epoch_loss=0.13543397188186646\n",
      "Epoch 149: curr_epoch_loss=0.1327107548713684\n",
      "Number of epochs:  150\n",
      "Training dataset size 4076\n",
      "Training time:  0:02:29.701597\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2vec.wv)\n",
    "embedding_dim = 50\n",
    "num_filters = 64\n",
    "filter_size = 3\n",
    "hidden_dim = 50\n",
    "dropout = 0.5\n",
    "n_epochs=150\n",
    "\n",
    "def train(train_loader, n_epochs=n_epochs):\n",
    "    model = SentenceSimilarityCNN2(embedding_dim, num_filters, filter_size, hidden_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        curr_epoch_loss = []\n",
    "        for x1, x2, y in train_loader:\n",
    "            #print(x1.shape)\n",
    "            y_hat = model(x1, x2)\n",
    "            loss = criterion(y_hat, y.float())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "\n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train stanza sentences\n",
    "start_time = datetime.datetime.now()\n",
    "model = train(train_dataloader)\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Number of epochs: \", n_epochs)\n",
    "print (\"Training dataset size\", len(train_dataset))\n",
    "print(\"Training time: \", (end_time - start_time))\n",
    "\n",
    "# Train the raw sentences\n",
    "start_time = datetime.datetime.now()\n",
    "model_raw_sentences = train(train_dataloader_raw_sentences)\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Number of epochs: \", n_epochs)\n",
    "print (\"Training dataset size\", len(train_dataset))\n",
    "print(\"Training time: \", (end_time - start_time))\n",
    "\n",
    "# Train spacy sentences\n",
    "start_time = datetime.datetime.now()\n",
    "model_spacy_sentences = train(train_dataloader_spacy_sentences)\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Number of epochs: \", n_epochs)\n",
    "print (\"Training dataset size\", len(train_dataset))\n",
    "print(\"Training time: \", (end_time - start_time))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of pos test corpus =  1725\n",
      "accuracy for pos test corpus =  0.6811594202898551\n",
      "size of raw test corpus =  1725\n",
      "accuracy for raw test corpus =  0.6776811594202898\n",
      "size of spacy test corpus =  1725\n",
      "accuracy for spacy test corpus =  0.6730434782608695\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def eval_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    Y_pred = []\n",
    "    Y = []\n",
    "    for x1, x2, y in test_dataloader:\n",
    "        y_hat = model(x1, x2)\n",
    "        #print(y_hat)\n",
    "        y_pred = torch.zeros(y_hat.shape)\n",
    "        y_pred = (y_hat > 0.3).int()\n",
    "\n",
    "        Y_pred = np.concatenate((Y_pred, y_pred), axis=0)\n",
    "        Y = np.concatenate((Y, y), axis=0)\n",
    "\n",
    "        #print(y_pred)\n",
    "        #print(y)\n",
    "    return Y_pred, Y\n",
    "\n",
    "y_pred, y = eval_model(model, test_dataloader)\n",
    "y_pred_raw, y_raw = eval_model(model_raw_sentences, test_dataloader_raw_sentences)\n",
    "y_pred_spacy, y_spacy = eval_model(model_spacy_sentences, test_dataloader_spacy_sentences)\n",
    "print(\"size of pos test corpus = \", len(test_dataset))\n",
    "print(\"accuracy for pos test corpus = \", accuracy_score(y, y_pred))\n",
    "\n",
    "print(\"size of raw test corpus = \", len(test_dataset_raw_sentences))\n",
    "print(\"accuracy for raw test corpus = \", accuracy_score(y_raw, y_pred_raw))\n",
    "\n",
    "print(\"size of spacy test corpus = \", len(test_dataset_spacy_sentences))\n",
    "print(\"accuracy for spacy test corpus = \", accuracy_score(y_spacy, y_pred_spacy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " bn_Testing Similarity Scoring Function_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "NUM_OF_SAMPLES = 20\n",
    "SAMPLE_SIZE = 2\n",
    "OUTPUT_SIZE = 6\n",
    "\n",
    "test_embedding = torch.rand((NUM_OF_SAMPLES, SAMPLE_SIZE, OUTPUT_SIZE))\n",
    "scores = manhattan_similarity_score(test_embedding)\n",
    "\n",
    "assert scores.shape == (NUM_OF_SAMPLES, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.386951Z",
     "end_time": "2023-04-08T00:35:09.528280Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "class SSCN(nn.Module):\n",
    "    def __init__(self, sample_size, stride=1, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.conv_layers =sample_size\n",
    "\n",
    "        #NN layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.conv_layers, out_channels=self.conv_layers, \\\n",
    "                               kernel_size=self.kernel_size, padding=self.padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.conv_layers, out_channels=self.conv_layers, \\\n",
    "                               kernel_size=self.kernel_size, padding=self.padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool1 = DynamicKMaxPooling(self.kernel_size, self.conv_layers)\n",
    "\n",
    "        self.sscn = nn.Sequential(self.conv1, self.relu1, self.conv2, self.relu2, self.pool1)\n",
    "\n",
    "    \"\"\"\n",
    "    * X: Pooled output of SSCN model of shape (sample_size, -1)\n",
    "    * For the purpose of this experiment sample_size = 2\n",
    "    \"\"\"\n",
    "    def manhattan_similarity_score(self, X):\n",
    "        score = manhattan_similarity_score(X)\n",
    "        return score\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X = self.conv1(X)\n",
    "        # print(X.shape)\n",
    "        # X = self.relu1(X)\n",
    "        # print(X.shape)\n",
    "        # X = self.conv1(X)\n",
    "        # print(X.shape)\n",
    "        # X = self.relu2(X)\n",
    "        # print(X.shape)\n",
    "        # X = self.pool1(X)\n",
    "        # print(X.shape)\n",
    "        # X = self.manhattan_similarity_score(X)\n",
    "        # print(X.shape)\n",
    "        X = self.manhattan_similarity_score(self.sscn(X))\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.387486Z",
     "end_time": "2023-04-08T00:35:09.528347Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Testing:__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSCN(\n",
      "  (conv1): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (relu2): ReLU()\n",
      "  (pool1): DynamicKMaxPooling()\n",
      "  (sscn): Sequential(\n",
      "    (0): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (3): ReLU()\n",
      "    (4): DynamicKMaxPooling()\n",
      "  )\n",
      ")\n",
      "torch.Size([20, 1])\n"
     ]
    }
   ],
   "source": [
    "NUM_OF_SAMPLES = 20\n",
    "SAMPLE_SIZE = 2\n",
    "UNIQUE_FEATURES = 18\n",
    "\n",
    "test_embedding = torch.rand((NUM_OF_SAMPLES, SAMPLE_SIZE, UNIQUE_FEATURES))\n",
    "\n",
    "model = SSCN(SAMPLE_SIZE)\n",
    "# shape (batch,sample,sentence,word)?\n",
    "print(model)\n",
    "\n",
    "out = model(test_embedding)\n",
    "\n",
    "assert out.shape[0] == NUM_OF_SAMPLES\n",
    "assert out.shape[1] == 1\n",
    "print(out.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.388774Z",
     "end_time": "2023-04-08T00:35:09.528518Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
