{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gensim\n",
    "import numpy as np\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import torch, stanza\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import threading\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:01.824854Z",
     "end_time": "2023-04-08T00:35:02.331708Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Processing\n",
    "\n",
    "This section can have stuff related to data prep.\n",
    "\n",
    "\n",
    "Should the MSPC Dataset be a part of this section? - Adam"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    # Note: Unable to use pd.read_csv... the function complained about an issue with the formatting of the tsv file\n",
    "    # train = pd.read_csv('data/msr_paraphrase_train.txt', sep='\\t', encoding='latin1')\n",
    "    # train\n",
    "\n",
    "    # opting to read file in and split columns manually to create a pandas dataframe\n",
    "    list = []\n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            fields = line.split('\\t')\n",
    "            list.append(fields)\n",
    "\n",
    "    df = pd.DataFrame(list[1:], columns=['Quality', 'ID1', 'ID2', 'String1', 'String2'])\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:01.868489Z",
     "end_time": "2023-04-08T00:35:02.351344Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Definition\n",
    "\n",
    "![Model Overview](./images/overview.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Input Layer\n",
    "\n",
    "In the input layer is made up of a Stanford Parser to provide a syntactic tree so that the model can extract significant words (mainly, subject, predicate, object) in the input corpus. Word2Vec is then used to map the words into vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 22:19:15 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-08 22:19:15 INFO: Using device: cpu\n",
      "2023-04-08 22:19:15 INFO: Loading: tokenize\n",
      "2023-04-08 22:19:15 INFO: Loading: pos\n",
      "2023-04-08 22:19:15 INFO: Loading: constituency\n",
      "2023-04-08 22:19:16 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# set 'download_method = None' to not download the resources over and over\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency', download_method=None, use_gpu=False)\n",
    "\n",
    "def trunk_construction(str, parent_label = None):\n",
    "    doc = nlp(str)\n",
    "    tree = doc.sentences[0].constituency\n",
    "\n",
    "    words = construct_sentence(tree, parent_label)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def construct_sentence(tree, parent_label = None, leave_pos=False):\n",
    "\n",
    "    sentences = []\n",
    "    if 'NN' in tree.label:\n",
    "        if parent_label == 'NP':\n",
    "            # sentences.append(tree)\n",
    "            sentences = sentences + tree.leaf_labels()\n",
    "    if 'VB' in tree.label:\n",
    "        if parent_label == 'VP':\n",
    "            #sentences.append(tree)\n",
    "            sentences = sentences + tree.leaf_labels()\n",
    "    for child in tree.children:\n",
    "        sentences = sentences + construct_sentence(child, tree.label)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def find_subject_branches(tree):\n",
    "    subjects = []\n",
    "    if tree.label == 'S':\n",
    "        subjects.append(tree)\n",
    "\n",
    "    for child in tree.children:\n",
    "        subjects = subjects + find_subject_branches(child)\n",
    "\n",
    "    return subjects\n",
    "\n",
    "# #\n",
    "# # # According to the paper the subject is the first NN child of NP\n",
    "# def find_subject(noun_phrase):\n",
    "#     subject = None\n",
    "#     for child in noun_phrase.children:\n",
    "#         if  'NN' in child.label:\n",
    "#             subject = ' '.join(child.leaf_labels())\n",
    "#             break\n",
    "#\n",
    "#     print(subject)\n",
    "#     return subject\n",
    "#\n",
    "# def find_predicate(verb_phrase):\n",
    "#     predicate = None\n",
    "#     for child in verb_phrase.children:\n",
    "#         if 'VB' in child.label:\n",
    "#             predicate = child.leaf_labels()\n",
    "#             break\n",
    "#\n",
    "#     return predicate\n",
    "#\n",
    "# def find_object(verb_phrase, parent_label = None):\n",
    "#     objects = []\n",
    "#     if 'NN' in verb_phrase.label and (parent_label == 'NP' or parent_label == 'PP' or parent_label == 'ADJP'):\n",
    "#         objects.append(' '.join(verb_phrase.leaf_labels()))\n",
    "#     else:\n",
    "#         for child in verb_phrase.children:\n",
    "#             objects = objects + find_object(child, verb_phrase.label)\n",
    "#\n",
    "#     return objects\n",
    "#\n",
    "#\n",
    "# def find_spo(s):\n",
    "#     subject, predicate, obj = None, None, None\n",
    "#     for child in s.children:\n",
    "#         if child.label == 'NP':\n",
    "#             subject = find_subject(child)\n",
    "#         if child.label == 'VP':\n",
    "#             predicate = find_predicate(child)\n",
    "#             obj = ' '.join(find_object(child))\n",
    "#\n",
    "#     return subject, predicate, obj\n",
    "#\n",
    "# def trunk_construction(str):\n",
    "#     doc = nlp(str)\n",
    "#     tree = doc.sentences[0].constituency\n",
    "#     print(tree)\n",
    "#     subject_branches = find_subject_branches(tree)\n",
    "#     subjects, predicates, objects = [], [], []\n",
    "#     for subject in subject_branches:\n",
    "#         subject, predicate, object = find_spo(subject)\n",
    "#         if subject is not None: subjects.append(subject)\n",
    "#         if predicate is not None: predicates.append(predicate)\n",
    "#         if objects is not None: objects.append(object)\n",
    "\n",
    "    return subjects, predicates, objects"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:01.890215Z",
     "end_time": "2023-04-08T00:35:03.008883Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "def test_parser(str, valid_sentence):\n",
    "\n",
    "    new_sentence = trunk_construction(str)\n",
    "    # print(new_sentence)\n",
    "    assert new_sentence == valid_sentence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:03.013569Z",
     "end_time": "2023-04-08T00:35:03.015960Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parser Test Cases\n",
    "Test the parser using some of the training data sentences as input and asserting the output sentence matches the algorithm defined in the paper."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "test_parser('Syrian forces launch new attacks', \"forces launch attacks\")\n",
    "test_parser(\"\"\"the flat tire was replaced by the driver\"\"\",\"tire was replaced driver\")\n",
    "test_parser(\"\"\"Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.\"\"\",\n",
    "           \"Amrozi accused brother called witness distorting evidence\")\n",
    "test_parser(\"\"\"Shares of Genentech, a much larger company with several products on the market, rose more than 2 percent\"\"\",\n",
    "            \"Shares Genentech company products market rose percent\")\n",
    "test_parser(\"\"\"Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\"\"\",\n",
    "             \"Gyorgy Heizler head disaster unit said coach was carrying passengers\")\n",
    "test_parser(\"\"\"Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.\"\"\",\n",
    "           \"Referring witness Amrozi accused brother distorting evidence\")\n",
    "test_parser(\"\"\"His wife said he was \"100 percent behind George Bush\" and looked forward to using his years of training in the war.\"\"\",\n",
    "            \"wife said was percent George Bush looked using years training war\")\n",
    "test_parser(\"\"\"Sheena Young of Child, the national infertility support network, hoped the guidelines would lead to a more \"fair and equitable\" service for infertility sufferers\"\"\", \"Sheena Young Child network hoped guidelines lead service infertility sufferers\")\n",
    "test_parser(\"\"\"Sheena Young, for Child, the national infertility support network, said the proposed guidelines should lead to a more \"fair and equitable\" service for infertility sufferers.\"\"\", \"Sheena Young Child network said guidelines lead service infertility sufferers\")\n",
    "\n",
    "test_parser(\"\"\"Among CNN viewers, 29 percent said they were Republicans and 36 percent called themselves conservatives.\"\"\",\n",
    "            \"CNN viewers percent said were Republicans percent called conservatives\")\n",
    "test_parser(\"\"\"Out of Fox viewers, 41 percent describe themselves as Republicans, 24 percent as Democrats and 30 percent as Independents\"\"\",\n",
    "            \"Fox viewers percent describe Republicans percent Democrats percent Independents\")\n",
    "\n",
    "# Note: stanza parser has a problem with the below sentence.  It is unable to parse it correctly\n",
    "# test_parser(\"\"\"Sheena Young, for Child, the national infertility support network, said the proposed guidelines should lead to a more \"fair and equitable\" service for infertility sufferers.\"\"\", \"\")\n",
    "# test_parser(\"\"\"Among Fox viewers, 41 percent describe themselves as Republicans, 24 percent as Democrats and 30 percent as Independents\"\"\", \"Fox viewers percent describe Republicans percent Democrats percent Independents\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:03.023262Z",
     "end_time": "2023-04-08T00:35:08.918548Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Concurrency Parsing\n",
    "Added support for concurrent parsing.  This can help in the performance of the preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "class SentenceProcessingThread(threading.Thread):\n",
    "    def __init__(self, sentences, output_list, begin, end):\n",
    "        super(SentenceProcessingThread, self).__init__()\n",
    "        self.sentences = sentences\n",
    "        self.nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency', download_method=None, use_gpu=True)\n",
    "        self.output_list = output_list\n",
    "        self.begin = begin\n",
    "        self.end = end\n",
    "\n",
    "    def trunk_construction(self, str, parent_label = None):\n",
    "        doc = self.nlp(str)\n",
    "        tree = doc.sentences[0].constituency\n",
    "\n",
    "        words = construct_sentence(tree, parent_label)\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"going to process {self.begin} to {self.end}\")\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "            new_sentence = trunk_construction(sentence)\n",
    "            self.output_list[self.begin + i] = new_sentence\n",
    "\n",
    "def process_sentences_concurrently(sentences, output, p=2):\n",
    "    total = len(sentences)\n",
    "    interval = int(total / p)\n",
    "    threads = []\n",
    "    for i in range(p):\n",
    "        s = i*interval\n",
    "        if i == p-1:\n",
    "            e = total\n",
    "        else:\n",
    "            e = (i+1) * interval\n",
    "        sentences_slice = sentences[s:e]\n",
    "        sentence_thread = SentenceProcessingThread(sentences_slice, output, s, e)\n",
    "        sentence_thread.start()\n",
    "        threads.append(sentence_thread)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "def preprocess_corpus(input_file='data/msr_paraphrase_train.txt', output_file='data/msr_paraphrase_train_stanza.txt', N=None):\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"{output_file} already exists\")\n",
    "        return\n",
    "\n",
    "    starttime = datetime.datetime.now()\n",
    "    df = read_file(input_file)\n",
    "\n",
    "    if N is None:\n",
    "        N = len(df.String1)\n",
    "\n",
    "    output1 = [None] * N\n",
    "    output2 = [None] * N\n",
    "\n",
    "    # we can process with more threads if we only have CPU\n",
    "    p = 8\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # if cuda is available we don't need that many threads\n",
    "        # and if the number of threads is set too large using cuda\n",
    "        # we can get out of memory exceptions\n",
    "        p = 2\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    process_sentences_concurrently(df.String1[:N], output1, p)\n",
    "\n",
    "    # try and be careful with gpu memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    process_sentences_concurrently(df.String2[:N], output2, p)\n",
    "\n",
    "    endtime = datetime.datetime.now()\n",
    "\n",
    "    print(f\"time to process {N*2} sentences is {endtime - starttime}\")\n",
    "\n",
    "    stanza_df = df[:N]\n",
    "\n",
    "    processed_string1 = pd.Series(output1)\n",
    "    # processed_string1.apply(gensim.utils.simple_preprocess)\n",
    "    processed_string2 = pd.Series(output2)\n",
    "    #processed_string2.apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "    stanza_df.String1 = processed_string1\n",
    "    stanza_df.String2 = processed_string2\n",
    "\n",
    "    # write the file out.  This can help in the future\n",
    "    print(f\"about to write out {output_file}\")\n",
    "    stanza_df.to_csv(output_file, sep=\"\\t\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.905455Z",
     "end_time": "2023-04-08T00:35:08.941546Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence Preprocessing\n",
    "pass the input sentences from the training dataset through the stanford/stanza parser, extracting the relevant parts of speech and then tokenize the processed sentences using the gensim.utils.simple_preprocess utility"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# start_time = datetime.datetime.now()\n",
    "#\n",
    "# processed_string1 = df[:500].String1.apply(trunk_construction)\n",
    "# processed_string2 = df[:500].String2.apply(trunk_construction)\n",
    "#\n",
    "# end_time = datetime.datetime.now()\n",
    "#\n",
    "# print (f\"Processing 200 sentences with stanza library took {end_time - start_time}\")\n",
    "#\n",
    "# start_time = datetime.datetime.now()\n",
    "#\n",
    "# processed_string1 = processed_string1.apply(gensim.utils.simple_preprocess)\n",
    "# processed_string2 = processed_string2.apply(gensim.utils.simple_preprocess)\n",
    "#\n",
    "# end_time = datetime.datetime.now()\n",
    "#\n",
    "# print (f\"Processing 200 sentences with gensim.utils.simple_preprocess took {end_time - start_time}\")\n",
    "# print(f\"Number of sentences processed in the String1 column: len(processed_string1\")\n",
    "# print(f\"Number of sentences processed in the String2 column: len(processed_string2\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.907470Z",
     "end_time": "2023-04-08T00:35:08.942412Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec Embeddings\n",
    "Take the preprocessed and tokenized sentences and use Word2Vec to get the word embeddings.  Take each word embedding in a sentence and find the mean which will represent the embedding for the sentence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "#\n",
    "# corpus = pd.concat([processed_string1, processed_string2], ignore_index=True)\n",
    "#\n",
    "# #model = Word2Vec(sentences=corpus, vector_size=(len(processed_string1) + len(processed_string2)), min_count=2)\n",
    "# # set vector size to reduce computational complexity\n",
    "# model = Word2Vec(sentences=corpus, min_count=1, window=2, vector_size=50)\n",
    "# #model.build_vocab(sentences=corpus)\n",
    "# #model.train(corpus, total_examples=model.corpus_count, epochs=5)\n",
    "#\n",
    "# print(model)\n",
    "# print(model.wv.key_to_index)\n",
    "#\n",
    "# model.wv.get_vector('president')\n",
    "#\n",
    "# # for index, word in enumerate(model.wv.index_to_key):\n",
    "# #     if index == 120:\n",
    "# #         break\n",
    "# #     print(f\"word #{index}/{len(model.wv.index_to_key)} is {word}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.908218Z",
     "end_time": "2023-04-08T00:35:08.942594Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Function is broken out for testing purposes\n",
    "def generate_word2vec_model(corpus):\n",
    "    # Creating the Word2Vec model\n",
    "    model = Word2Vec(sentences=corpus, min_count=1, window=2, vector_size=50)\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.908525Z",
     "end_time": "2023-04-08T00:35:08.942658Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# Function is broken out for testing purposes\n",
    "def sentence_embeddings(w2v_model, sentence, size):\n",
    "    np_embedding = np.zeros(size)\n",
    "    for i, word in enumerate(sentence):\n",
    "        np_embedding[i] = w2v_model.wv.get_vector(word)\n",
    "\n",
    "    return np_embedding\n",
    "    # list = []\n",
    "    # for word in sentence:\n",
    "    #     list.append(w2v_model.wv.get_vector(word))\n",
    "    #\n",
    "    # word_matrix = np.row_stack(list)\n",
    "    # #return np.mean(word_matrix, axis=0)\n",
    "    # return word_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.910846Z",
     "end_time": "2023-04-08T00:35:08.942887Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "def test_word2vec():\n",
    "\n",
    "    df = read_file('data/msr_paraphrase_train.txt')\n",
    "\n",
    "    sentences1 = df.String1[:5].apply(gensim.utils.simple_preprocess)\n",
    "    sentences2 = df.String2[:5].apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "    corpus = pd.concat([sentences1, sentences2], ignore_index=True)\n",
    "\n",
    "    max_sentence_len = corpus.apply(len).max()\n",
    "\n",
    "    model = generate_word2vec_model(corpus)\n",
    "\n",
    "    embedding = sentence_embeddings(model, corpus[0], (max_sentence_len, 50))\n",
    "    assert embedding.shape == (max_sentence_len, 50)\n",
    "\n",
    "    # Not sure if this will always generate the same embedding\n",
    "    # test_embedding = np.array([-0.00113049, -0.00124808,  0.00252251,  0.00058141,  0.00187964,  0.00379025,\n",
    "    #           -0.00012356,  0.00347055, -0.00241507,  0.00545258, -0.00574078, -0.00489824,\n",
    "    #           -0.00224492,  0.00744946,  0.00350835, -0.00139295, -0.00081134,  0.00655962,\n",
    "    #           0.00244374, -0.00447209, -0.00124291, -0.00092616,  0.0021044,  -0.00092541,\n",
    "    #           0.00284307,  0.00367638,  0.00364716,  0.00519976, -0.00088121,  0.00109841,\n",
    "    #           -0.00219322, -0.00372483,  0.00078702, -0.00612309, -0.00312131,  0.00088071,\n",
    "    #           0.00503909, -0.0009484,  -0.00068209, -0.0004782,   0.00367015,  0.00314679,\n",
    "    #           -0.00302592,  0.00346377,  0.00151145, -0.00076442, -0.0012528,  -0.00087095,\n",
    "    #           -0.00075365,  0.00468711])\n",
    "    #\n",
    "    #\n",
    "    # # not sure if this will always be equal based on comment on test_embedding variable\n",
    "    # assert np.allclose(embedding, test_embedding)\n",
    "\n",
    "test_word2vec()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.911696Z",
     "end_time": "2023-04-08T00:35:09.477774Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "def corpus_embeddings(model, corpus, max_sentence_len):\n",
    "    corpus_size = len(corpus)\n",
    "    embeddings_list = []\n",
    "    embedding_matrix = np.zeros((corpus_size, max_sentence_len, 50))\n",
    "    for i, sentence in enumerate(corpus):\n",
    "        embeddings = sentence_embeddings(model, sentence, size=(max_sentence_len, 50))\n",
    "        embedding_matrix[i] = embeddings\n",
    "        embeddings_list.append(embeddings)\n",
    "\n",
    "    return embedding_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.972122Z",
     "end_time": "2023-04-08T00:35:09.479438Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Dataset for the MSPC dataset\n",
    "class MSPCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        tsv_file (string): path to the tsv file with sentences to compare and associate quality score\n",
    "        num_records (int): number of records to load.  Defaults to None which is all\n",
    "    \"\"\"\n",
    "    def __init__(self, tsv_file, num_records=None):\n",
    "\n",
    "        file_parts = os.path.splitext(tsv_file)\n",
    "        output_file = f\"{file_parts[0]}_stanza{file_parts[1]}\"\n",
    "        print(\"About to preprocess data\")\n",
    "        preprocess_corpus(input_file=tsv_file, output_file=output_file)\n",
    "        print(\"Done preprocessing data\")\n",
    "\n",
    "\n",
    "        #df = read_file('data/msr_paraphrase_train.txt')\n",
    "        df = pd.read_csv(output_file, sep=\"\\t\")\n",
    "\n",
    "        if num_records is not None:\n",
    "            processed_string1 = df[:num_records].String1\n",
    "            processed_string2 = df[:num_records].String2\n",
    "            self.quality = df[:num_records].Quality\n",
    "        else:\n",
    "            processed_string1 = df.String1\n",
    "            processed_string2 = df.String2\n",
    "            self.quality = df.Quality\n",
    "\n",
    "        processed_string1 = processed_string1.apply(gensim.utils.simple_preprocess)\n",
    "        processed_string2 = processed_string2.apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "        corpus = pd.concat([processed_string1, processed_string2], ignore_index=True)\n",
    "\n",
    "\n",
    "        max_sentence_len = corpus.apply(len).max()\n",
    "\n",
    "        w2v_model = generate_word2vec_model(corpus)\n",
    "\n",
    "        sentence_embeddings1 = corpus_embeddings(w2v_model, processed_string1, max_sentence_len=max_sentence_len)\n",
    "        sentence_embeddings2 = corpus_embeddings(w2v_model, processed_string2, max_sentence_len=max_sentence_len)\n",
    "\n",
    "        self.w2v_model = w2v_model\n",
    "        self.sentences_embeddings1 = sentence_embeddings1\n",
    "        self.sentences_embeddings2 = sentence_embeddings2\n",
    "\n",
    "        # print (f\"Processing 200 sentences with gensim.utils.simple_preprocess took {end_time - start_time}\")\n",
    "        print(f\"Number of sentences processed in the String1 column: {len(processed_string1)}\")\n",
    "        print(f\"Number of sentences processed in the String2 column: {len(processed_string2)}\")\n",
    "        #print(self.sentences_embeddings1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_embeddings1)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.sentences_embeddings1[i], self.sentences_embeddings2[i], self.quality[i]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:08.981724Z",
     "end_time": "2023-04-08T00:35:09.479637Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to preprocess data\n",
      "data/msr_paraphrase_train_stanza.txt already exists\n",
      "Done preprocessing data\n",
      "Number of sentences processed in the String1 column: 10\n",
      "Number of sentences processed in the String2 column: 10\n"
     ]
    }
   ],
   "source": [
    "def test_dataset():\n",
    "    dataset = MSPCDataset('data/msr_paraphrase_train.txt', 10)\n",
    "    assert len(dataset) == 10\n",
    "    # for set in dataset:\n",
    "    #     print(len(set[0]), len(set[1]))\n",
    "    # print(len(dataset[0][0]))\n",
    "    # print(dataset[0])\n",
    "test_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T14:32:14.561411Z",
     "end_time": "2023-04-07T14:32:14.897963Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataloaders\n",
    "Create training and test dataloaders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to preprocess data\n",
      "data/msr_paraphrase_train_stanza.txt already exists\n",
      "Done preprocessing data\n",
      "Number of sentences processed in the String1 column: 4076\n",
      "Number of sentences processed in the String2 column: 4076\n",
      "About to preprocess data\n",
      "data/msr_paraphrase_test_stanza.txt already exists\n",
      "Done preprocessing data\n",
      "Number of sentences processed in the String1 column: 1725\n",
      "Number of sentences processed in the String2 column: 1725\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MSPCDataset('data/msr_paraphrase_train.txt')\n",
    "test_dataset = MSPCDataset('data/msr_paraphrase_test.txt')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1.77404061e-02,  3.47889327e-02, -1.09778875e-02,\n         1.27714574e-02,  1.10109346e-02, -4.84661795e-02,\n         2.93790512e-02,  5.26951738e-02, -2.61720475e-02,\n         5.47409151e-03,  2.55659539e-02, -2.58834083e-02,\n         6.07441831e-03,  7.61951879e-03, -1.08305123e-02,\n         1.58339683e-02,  3.09274253e-02,  7.77853746e-03,\n        -6.11838661e-02, -8.98842793e-03, -6.94689946e-03,\n         2.58505139e-02,  6.81013986e-02, -1.13576297e-02,\n         2.46121306e-02,  1.10584153e-02,  5.27332630e-03,\n        -8.68066400e-03, -5.39070740e-02, -1.27882352e-02,\n         1.68112516e-02, -5.95621206e-03, -3.49961519e-02,\n        -4.92039928e-03, -2.73613464e-02,  2.04547103e-02,\n         1.47048524e-02,  2.34515313e-02,  3.96893956e-02,\n        -4.59628627e-02,  1.71677582e-02,  5.12278266e-03,\n        -2.52861921e-02, -4.77366149e-03,  6.09214492e-02,\n         2.46468913e-02, -4.51211771e-03, -4.14408594e-02,\n         8.22822750e-03,  3.58437784e-02],\n       [-3.41980681e-02,  5.01731932e-02, -1.69156920e-02,\n         2.38432847e-02, -1.85527094e-02, -1.26868367e-01,\n         1.28824025e-01,  1.92677706e-01, -1.63371742e-01,\n        -2.60858927e-02,  2.43179817e-02, -8.75038803e-02,\n        -8.38945806e-03,  2.76639126e-02, -4.81380299e-02,\n         5.72602125e-03,  1.17860325e-01,  1.43159032e-02,\n        -1.70538351e-01, -9.06110704e-02,  2.87734810e-02,\n         1.45094469e-01,  1.75698727e-01, -5.01826629e-02,\n         1.31812379e-01,  6.43808842e-02, -9.96631663e-03,\n         1.59085877e-02, -1.49574846e-01, -9.82276537e-03,\n         6.85092807e-02, -4.61921543e-02, -5.41417189e-02,\n         3.06251589e-02, -4.35448252e-02,  5.64326793e-02,\n         8.97658691e-02,  4.22429703e-02,  8.11343342e-02,\n        -8.64218771e-02,  1.22548349e-01, -3.19680497e-02,\n        -3.72126549e-02,  1.53165879e-02,  2.59462655e-01,\n         3.28037590e-02, -3.17577161e-02, -1.36775210e-01,\n         5.84316440e-02,  8.88916105e-02],\n       [-2.66990345e-02,  1.59307644e-02,  1.10643152e-02,\n         1.37874037e-02, -9.15254466e-03, -2.79119145e-02,\n         2.68513300e-02,  6.34301677e-02, -2.60535870e-02,\n         7.81852193e-03,  2.32966430e-02, -2.93843821e-02,\n        -3.81714199e-04,  2.22162921e-02, -2.39232145e-02,\n         1.63636636e-02,  4.70080301e-02,  2.32934598e-02,\n        -3.99180949e-02, -3.68061252e-02, -6.58841990e-03,\n         4.96577807e-02,  6.36105016e-02, -9.03175585e-03,\n         3.11966445e-02,  1.35847563e-02,  3.72569589e-03,\n         2.75127683e-02, -5.02218753e-02, -8.75827856e-03,\n         3.05386428e-02, -6.35390216e-03, -5.18624531e-03,\n        -1.08048245e-02, -2.11270079e-02,  1.21683786e-02,\n         2.38060597e-02,  4.31027217e-03, -6.34453841e-04,\n        -2.81544626e-02,  3.31173688e-02,  1.03659593e-02,\n        -5.48521639e-04,  9.81700141e-04,  8.40216875e-02,\n         2.63610501e-02, -1.40214292e-02, -4.43401001e-02,\n         9.71057545e-03,  3.53660583e-02],\n       [-9.87884179e-02,  9.71245989e-02, -1.92841142e-02,\n         1.95497312e-02, -3.44038978e-02, -2.38704860e-01,\n         1.90118790e-01,  3.22184354e-01, -2.72258967e-01,\n        -6.84545264e-02,  4.80542965e-02, -1.81076080e-01,\n        -2.02318281e-02,  6.10595644e-02, -1.16386563e-01,\n         3.02697979e-02,  1.67249858e-01,  5.60383163e-02,\n        -2.98859328e-01, -1.55024797e-01,  2.92589106e-02,\n         2.66371131e-01,  3.66770208e-01, -1.12026274e-01,\n         2.36849964e-01,  1.49364710e-01, -5.21666035e-02,\n         2.15419009e-02, -2.75088489e-01, -3.38734910e-02,\n         1.10721946e-01, -1.12742618e-01, -1.36755556e-01,\n         5.78712188e-02, -6.30335137e-02,  8.06469545e-02,\n         1.88970760e-01,  3.35029997e-02,  1.27363130e-01,\n        -1.87175244e-01,  1.93676114e-01, -4.35616486e-02,\n        -5.90742379e-02,  4.95338440e-02,  5.02034962e-01,\n         9.34754908e-02, -7.46472180e-02, -2.57773399e-01,\n         1.03449576e-01,  1.21547200e-01],\n       [-3.10620703e-02,  1.59848984e-02, -8.94037168e-03,\n        -6.59981649e-03,  1.32095590e-02, -8.08889121e-02,\n         4.32043336e-02,  1.00159399e-01, -7.72516802e-02,\n        -1.03358296e-03,  9.31438524e-03, -4.90402840e-02,\n         2.25381111e-03,  1.11629069e-02, -3.92844751e-02,\n        -4.38966136e-03,  4.80157956e-02,  2.30463166e-02,\n        -6.35449216e-02, -1.94193386e-02,  1.87476061e-03,\n         7.82592073e-02,  1.02126792e-01, -4.40818556e-02,\n         4.65790257e-02,  3.70791331e-02, -1.60686187e-02,\n         5.63486526e-03, -8.51493776e-02, -4.35586600e-03,\n         3.82739753e-02, -2.79262979e-02, -1.71279255e-02,\n         1.16092646e-02, -8.32593360e-04,  1.37178022e-02,\n         3.83084603e-02,  2.59433892e-02,  4.36698347e-02,\n        -2.44981237e-02,  3.73297296e-02, -2.78440993e-02,\n        -6.02826849e-03,  2.21547280e-02,  1.18417531e-01,\n         1.35550573e-02,  8.78560531e-05, -6.65625855e-02,\n         4.43967059e-02,  3.71115357e-02],\n       [ 6.82851300e-03, -1.33257871e-02,  1.24040488e-02,\n         3.36730992e-03, -9.58760548e-03, -2.27704346e-02,\n        -1.04385829e-02,  1.24268490e-03,  5.93798235e-03,\n         1.08899446e-02,  1.75381992e-02, -1.27279963e-02,\n         3.29673116e-04, -1.49121657e-02,  1.80063618e-03,\n         1.89163797e-02,  7.59192696e-03,  1.88010391e-02,\n        -2.47698929e-02, -2.22889483e-02, -4.26468346e-03,\n         3.00576119e-03,  7.54263764e-03,  1.53442407e-02,\n         7.54455989e-03,  9.18152463e-03, -1.15799939e-03,\n         5.21567883e-03, -2.34114397e-02, -1.41445305e-02,\n        -4.93374420e-03,  1.20405527e-02,  2.64024292e-03,\n        -7.60239037e-03,  1.39333552e-03, -3.29685817e-03,\n         2.37331558e-02, -9.66600701e-03,  2.28396822e-02,\n        -1.39452396e-02,  2.64608450e-02,  1.19231467e-03,\n         1.16039310e-02,  1.23843029e-02,  9.97251086e-03,\n         6.21648505e-03,  3.47724068e-04, -1.93681214e-02,\n        -6.64750952e-03,  1.31229656e-02],\n       [-7.95915872e-02,  8.07763338e-02, -1.06212813e-02,\n         4.09028493e-02, -1.46386288e-02, -1.55786410e-01,\n         1.32311404e-01,  2.33751968e-01, -2.06928983e-01,\n        -4.97026108e-02,  3.78969572e-02, -1.49616152e-01,\n        -2.24349592e-02,  5.48008382e-02, -5.27820252e-02,\n         1.11420490e-02,  1.40588373e-01,  9.57433600e-03,\n        -2.19468027e-01, -9.08199996e-02,  1.73636694e-02,\n         1.65256381e-01,  2.43806452e-01, -1.01814225e-01,\n         1.54038385e-01,  1.15069531e-01, -4.20141779e-02,\n         4.08241376e-02, -2.00641781e-01, -1.05075464e-02,\n         7.86441937e-02, -5.97274527e-02, -9.17900801e-02,\n         6.98029948e-03, -5.34778982e-02,  6.94827661e-02,\n         1.05860688e-01,  5.18868268e-02,  1.02486193e-01,\n        -1.24830678e-01,  1.52816638e-01, -4.64153253e-02,\n        -4.78362963e-02,  2.08355654e-02,  3.52359354e-01,\n         6.29162192e-02, -4.80796844e-02, -1.99949980e-01,\n         6.80448562e-02,  1.01807475e-01]])"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filters out all zero filled columns to get the proper number of words in a sentence\n",
    "train_dataset[0][0][~np.all(train_dataset[0][0] == 0, axis=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def conv_output_volume(W, F, S, P):\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: Given the input volume size $W$, the kernel/filter size $F$,\n",
    "    the stride $S$, and the amount of zero padding $P$ used on the border,\n",
    "    calculate the output volume size.\n",
    "    Note the output should a integer.\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    #https://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "    return int((W-F+2*P)/S+1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T14:32:15.557166Z",
     "end_time": "2023-04-07T14:32:15.560063Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(conv_output_volume(50, 3, 1, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T14:32:18.231576Z",
     "end_time": "2023-04-07T14:32:18.377984Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pooling Layers\n",
    "\n",
    "The original paper used a dynamic k-max pooling method in their model. The _k_ value is determine by equation (1).\n",
    "\n",
    "\\begin{equation*} k=\\max \\left({k_{top},\\left \\lceil{ \\frac {L-l}{L} \\left |{ s }\\right | }\\right \\rceil }\\right)\\end{equation*}\n",
    "\n",
    "__Dynamic K-Max Pooling Implementation:__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "# #https://gist.github.com/anna-hope/7a2b2e66c3645aa8e4f94dbf06aed8dc\n",
    "# \"\"\"\n",
    "# TODO: Remove hardcoded values and max it dynamic.\n",
    "# \"\"\"\n",
    "# class DynamicKMaxPooling(nn.Module):\n",
    "#     def __init__(self, k_init, conv_layers, layer):\n",
    "#         super().__init__()\n",
    "#         # \"L is the total  number  of  convolutional  layers\n",
    "#         # in  the  network;\n",
    "#         # ktop is the fixed pooling parameter for the\n",
    "#         # topmost  convolutional  layer\"\n",
    "#         self.k_init = k_init\n",
    "#         self.conv_layers = conv_layers\n",
    "#         self.layer = layer\n",
    "#\n",
    "#     def forward(self, X):\n",
    "#         s = 50\n",
    "#         dyn_k = ((self.conv_layers - self.layer) / self.conv_layers) * 3\n",
    "#         k_max = int(round(max(self.k_init, np.ceil(dyn_k))))\n",
    "#         print(k_max)\n",
    "#         out = F.max_pool1d(X, kernel_size=k_max)\n",
    "#         return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T15:50:59.023740Z",
     "end_time": "2023-04-07T15:50:59.325715Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence Similarity Convolution Network (SSCN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pooling Layers\n",
    "\n",
    "The original paper used a dynamic k-max pooling method in their model. The _k_ value is determine by equation (1).\n",
    "\n",
    "\\begin{equation*} k=\\max \\left({k_{top},\\left \\lceil{ \\frac {L-l}{L} \\left |{ s }\\right | }\\right \\rceil }\\right)\\end{equation*}\n",
    "\n",
    "__Dynamic K-Max Pooling Implementation:__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "#https://gist.github.com/anna-hope/7a2b2e66c3645aa8e4f94dbf06aed8dc\n",
    "\"\"\"\n",
    "TODO: Remove hardcoded values and max it dynamic.\n",
    "\"\"\"\n",
    "class DynamicKMaxPooling(nn.Module):\n",
    "    def __init__(self, k_init, conv_layers):\n",
    "        super().__init__()\n",
    "        # \"L is the total  number  of  convolutional  layers\n",
    "        # in  the  network;\n",
    "        # ktop is the fixed pooling parameter for the\n",
    "        # topmost  convolutional  layer\"\n",
    "        self.k_init = k_init\n",
    "        self.conv_layers = conv_layers\n",
    "\n",
    "    def pool(self, X, l):\n",
    "        # s is sequence length\n",
    "        # l is current layer in network\n",
    "        s = X.shape[2]\n",
    "        dyn_k = ((self.conv_layers - l) / self.conv_layers) * s\n",
    "        k_max = int(round(max(self.k_init, np.ceil(dyn_k))))\n",
    "        return F.max_pool1d(X, kernel_size=k_max)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer_i in range(self.conv_layers,0,-1):\n",
    "            X = self.pool(X, layer_i)\n",
    "\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.384185Z",
     "end_time": "2023-04-08T00:35:09.528064Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing Dynamic K-Max Pooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "NUM_OF_SAMPLES = 20\n",
    "SAMPLE_SIZE = 2\n",
    "OUTPUT_SIZE = 15\n",
    "\n",
    "test_embedding = torch.rand((NUM_OF_SAMPLES, SAMPLE_SIZE, OUTPUT_SIZE))\n",
    "dyn_k_layer = DynamicKMaxPooling(3, SAMPLE_SIZE)\n",
    "\n",
    "# Call forward with convolution layer index [2,1]\n",
    "out = dyn_k_layer(test_embedding)\n",
    "\n",
    "assert out.shape[2] == 1\n",
    "assert out.shape[1] == SAMPLE_SIZE\n",
    "assert out.shape[0] == NUM_OF_SAMPLES"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.384376Z",
     "end_time": "2023-04-08T00:35:09.528155Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sentence Similarity\n",
    "\n",
    " \\begin{align*} Man(\\vec V_{x}, \\vec V_{y})=&\\left |{ x_{1}-y_{1} }\\right |\\! +\\! \\left |{ x_{2}-y_{2} }\\right | \\!+ \\!\\ldots \\!+ \\!\\left |{ x_{n}-y_{n} }\\right |\n",
    " \\\\ score=&e^{-Man(\\vec V_{x}, \\vec V_{y})},\\quad score\\in [{0,1}] \\end{align*}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* X: Pooled output of SSCN model of shape (sample_size, -1)\n",
    "* For the purpose of this experiment sample_size = 2\n",
    "\"\"\"\n",
    "def manhattan_similarity_score(X):\n",
    "    sample_count, _, M = X.shape\n",
    "    Vx = X[:,0].reshape((sample_count,M))\n",
    "    Vy = X[:,1].reshape((sample_count,M))\n",
    "    mdist = torch.sum(torch.abs(Vx-Vy),dim=1).view(sample_count,-1)\n",
    "    score = torch.exp(-1*mdist)\n",
    "    return score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.385728Z",
     "end_time": "2023-04-08T00:35:09.528218Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Testing Similarity Scoring Function_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "NUM_OF_SAMPLES = 20\n",
    "SAMPLE_SIZE = 2\n",
    "OUTPUT_SIZE = 6\n",
    "\n",
    "test_embedding = torch.rand((NUM_OF_SAMPLES, SAMPLE_SIZE, OUTPUT_SIZE))\n",
    "scores = manhattan_similarity_score(test_embedding)\n",
    "\n",
    "assert scores.shape == (NUM_OF_SAMPLES, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.386951Z",
     "end_time": "2023-04-08T00:35:09.528280Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "class SSCN(nn.Module):\n",
    "    def __init__(self, sample_size, stride=1, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.conv_layers =sample_size\n",
    "\n",
    "        #NN layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.conv_layers, out_channels=self.conv_layers, \\\n",
    "                               kernel_size=self.kernel_size, padding=self.padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.conv_layers, out_channels=self.conv_layers, \\\n",
    "                               kernel_size=self.kernel_size, padding=self.padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool1 = DynamicKMaxPooling(self.kernel_size, self.conv_layers)\n",
    "\n",
    "        self.sscn = nn.Sequential(self.conv1, self.relu1, self.conv2, self.relu2, self.pool1)\n",
    "\n",
    "    \"\"\"\n",
    "    * X: Pooled output of SSCN model of shape (sample_size, -1)\n",
    "    * For the purpose of this experiment sample_size = 2\n",
    "    \"\"\"\n",
    "    def manhattan_similarity_score(self, X):\n",
    "        score = manhattan_similarity_score(X)\n",
    "        return score\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X = self.conv1(X)\n",
    "        # print(X.shape)\n",
    "        # X = self.relu1(X)\n",
    "        # print(X.shape)\n",
    "        # X = self.conv1(X)\n",
    "        # print(X.shape)\n",
    "        # X = self.relu2(X)\n",
    "        # print(X.shape)\n",
    "        # X = self.pool1(X)\n",
    "        # print(X.shape)\n",
    "        # X = self.manhattan_similarity_score(X)\n",
    "        # print(X.shape)\n",
    "        X = self.manhattan_similarity_score(self.sscn(X))\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.387486Z",
     "end_time": "2023-04-08T00:35:09.528347Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Testing:__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSCN(\n",
      "  (conv1): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (relu2): ReLU()\n",
      "  (pool1): DynamicKMaxPooling()\n",
      "  (sscn): Sequential(\n",
      "    (0): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (3): ReLU()\n",
      "    (4): DynamicKMaxPooling()\n",
      "  )\n",
      ")\n",
      "torch.Size([20, 1])\n"
     ]
    }
   ],
   "source": [
    "NUM_OF_SAMPLES = 20\n",
    "SAMPLE_SIZE = 2\n",
    "UNIQUE_FEATURES = 18\n",
    "\n",
    "test_embedding = torch.rand((NUM_OF_SAMPLES, SAMPLE_SIZE, UNIQUE_FEATURES))\n",
    "\n",
    "model = SSCN(SAMPLE_SIZE)\n",
    "# shape (batch,sample,sentence,word)?\n",
    "print(model)\n",
    "\n",
    "out = model(test_embedding)\n",
    "\n",
    "assert out.shape[0] == NUM_OF_SAMPLES\n",
    "assert out.shape[1] == 1\n",
    "print(out.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T00:35:09.388774Z",
     "end_time": "2023-04-08T00:35:09.528518Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
