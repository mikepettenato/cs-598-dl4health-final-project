{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility Project \n",
    "CS598 Deep Learning for Healthcare - Spring 2023\n",
    "\n",
    "Orignal Paper: [Disease Prediction and Early Intervention System Based on Symptom Similarity Analysis](https://ieeexplore.ieee.org/document/8924757)<br>\n",
    "Reproduction Paper (DRAFT): [Reproducibility Project for CS598 DL4H in Spring 2023](https://drive.google.com/file/d/1OlaeqqWVPh9-TqqRmcs1-KRbOKmktVPi/view?usp=sharing)\n",
    "\n",
    "*Note: Reproduction Paper must be accessed using UIUC email address (i.e, @illinois.edu).*\n",
    "\n",
    "_Contributors:_\n",
    " * Michael Pettenato - mp34@illinois.edu\n",
    " * Adam Michalsky - adamwm3@illinois.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Analysis with Convolutional Neural Network\n",
    "\n",
    "## Introduction\n",
    "\n",
    "When physicians work with their patients, they often start by listening to their patient's symptom statements. The physcian will then map the patient's sympton statements to the symptoms that have been cataloged by the healthcare industry. This process or task of assess similarity between two sentences (e.g., A patient's symptom statement _and_ The cataloged symptom) is a task that can be easily mapped into computing.\n",
    "\n",
    "Sentence similarity is a task that has had significant research done on it already. Deep learning models were developed to perform this task in the healthcare industry already but each model has its pros and cons. One of the most common traits of models that assess similarity is the training time required, which is what the original paper (cited above) aimed to solve using an approach comprised of leveraging a Stanford Parser, Word2Vec embedding, and a convolutional neural network (CNN) based model.  \n",
    "\n",
    "The original paper did not have a documented repository, but did offer pseudo-code for certain aspects of the experiment. The following notebook is a reproduction of the original paper with some ablations that will be called out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T13:12:48.349739Z",
     "start_time": "2023-04-20T13:12:04.644180Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gensim\n",
    "import numpy as np\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import torch, stanza,spacy\n",
    "from torch import nn\n",
    "from enum import Enum\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Data\n",
    "\n",
    "The data that will be used to train and test is sourced from [Microsoft Research Paraphrase Corpus](https://www.microsoft.com/en-us/download/details.aspx?id=52398). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Data files used with this notebook can be found [here](https://github.com/mikepettenato/cs-598-dl4health-final-project/tree/main/data). The files were downloaded, extracted, and then uploaded to GitHub.\n",
    "\n",
    "### Helper Functions\n",
    "\n",
    "Below we define helper functions for ingesting the data used in the experiments.\n",
    "* `read_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    # Note: Unable to use pd.read_csv... the function complained about an issue with the formatting of the tsv file\n",
    "    # train = pd.read_csv('data/msr_paraphrase_train.txt', sep='\\t', encoding='latin1')\n",
    "    # train\n",
    "\n",
    "    # opting to read file in and split columns manually to create a pandas dataframe\n",
    "    list = []\n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            fields = line.split('\\t')\n",
    "            list.append(fields)\n",
    "\n",
    "    df = pd.DataFrame(list[1:], columns=['Quality', 'ID1', 'ID2', 'String1', 'String2'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "The approach described in the original paper can be described in three seperate _stages_. \n",
    "\n",
    "1) **Data Pre-Processing** - Here we will read in the raw data, parse it using the described algorithm, and prepare the parsed data by performing embedding using `Word2Vec`.\n",
    "\n",
    "2) **Neural Network** - Here we build the CNN-based model described in the original paper. This CNN will take the vectorized sentences pairs as inputs and return a similarity assesment [0,1].  No specifics of network architecture were offered in the original paper so the architecture is based on the our own knowledge.\n",
    "\n",
    "3) **Training & Validation** - Here we train and test the model using the data and network established in the previous stages.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "<div>\n",
    "    <figure>\n",
    "        <img src=\"./images/overview.png\" width=\"500\"/>\n",
    "        <figcaption align='center'>Figure 1: Overview of the Approach</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "In data pre-processing, perform two critical functions:\n",
    "\n",
    "   1. Read in and parse the files stored `.\\data` directory.\n",
    "   2. Embed the parsed sentences using `Word2Vec` embedding.\n",
    "\n",
    "Data parsed is the stored in a custom dataset using PyTorch class `Dataset`. Our implementation is `MSPCDataset`\n",
    "\n",
    "### Parsing \n",
    "<br>\n",
    "\n",
    "The original paper used the Stanford Parser to find the parts of speech for each word in a sentence. This parser has been deprecated in favor of the `stanza` parser, so we used this as a replacement for the Stanford parser.\n",
    "\n",
    "\n",
    "_Ablation 1:_ After reviewing the output of the `stanza` parser, we found that we did not agree with the subject, predicate, and object (SPO) result 100% of the time. Therefore the decision was made to support alternative parsing approaches. In addition to `stanza`, we have introduced support for two more parsing methods:\n",
    "* `spaCy` - Documentation and install instructions can be found at their [site](https://spacy.io/usage).\n",
    "* `2 Char Stop Word Removal` - A method where stop words less than 3 characters in length are not considered while using `spaCy` parsing. \n",
    "\n",
    "To allow flexibility in coding we have defined enum values for each parse method:\n",
    " * `STANZA` - Stanford Parser(`stanza`)\n",
    " * `SPACY` - spaCy Parser\n",
    " * `RAW` - 2 Char Stop Word Removal (Michael to validate?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parsing(Enum):\n",
    "    STANZA=1\n",
    "    SPACY=2\n",
    "    RAW=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stanford Parser (`stanza`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the Stanford Parser was what was used by the original paper to parse its raw sentences so we were able to reproduce the custom logic used to parse the tree produced by the parser. For ease of use, we implemented a function designed to extract _subject(s)_, _predicate(s)_, and _object(s)_ from an input parse tree (Figure 2). Going forward this will be reffered to as a _SPO Function_.\n",
    "\n",
    "\n",
    "<div>\n",
    "    <figure>\n",
    "        <img src=\"./images/parsetree.png\" width=\"500\"/>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Helper Functions\n",
    "\n",
    "Below we haved defined several functions that aid in finding the SPO provided a parsed sentence tree obtained from a `stanza` parser. \n",
    "\n",
    "* `find_branches()` - Extract phrases from a provided tree given a list of labels to include and exclude.\n",
    "* `find_subject()` - Locate a _subject_ given a noun phrase.\n",
    "    * _subject_ - Any `NN` child found in a noun phrase extracted from the parse tree.\n",
    "* `find_predicate()` - Locate a _predicate_ given a verb phrase.\n",
    "    * _predicate_ - Any `VB`child found in a verb phrase extracted from the parse tree.\n",
    "* `find_object()` - Locate an _object_ given a verb phrase.\n",
    "   * _object_ - Any `NN` child of a `NP`, `PP`, or `ADJP`object OR any child object that is not a `VP`found in the verb phrase extracted from the parse tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:03.008883Z",
     "start_time": "2023-04-08T00:35:01.890215Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_branches(tree, label, not_in_label=None, ancestors=[]):\n",
    "    branches = []\n",
    "    # print(\"-------------\")\n",
    "    # print(ancestors)\n",
    "    # print(f\"{tree.label} == {label}\")\n",
    "    if tree.label == label and not_in_label not in ancestors:\n",
    "        # print(f\"adding {tree}\")\n",
    "        branches.append(tree)\n",
    "    for child in tree.children:\n",
    "        branches = branches + find_branches(child, label, not_in_label, ancestors + [tree.label])\n",
    "\n",
    "    return branches\n",
    "\n",
    "#\n",
    "# # According to the paper the subject is the first NN child of NP\n",
    "def find_subject(noun_phrase_for_subject):\n",
    "    subject = []\n",
    "    for child in noun_phrase_for_subject.children:\n",
    "        if 'NN' in child.label:\n",
    "            subject = subject + child.leaf_labels()\n",
    "\n",
    "    #print(f\"subject = {subject}\")\n",
    "    #if len(subject) > 0:\n",
    "    #    return ' '.join(subject)\n",
    "    return subject\n",
    "\n",
    "    return None\n",
    "\n",
    "def find_predicate(verb_phrase_for_predicate):\n",
    "    predicate = []\n",
    "    for child in verb_phrase_for_predicate.children:\n",
    "        if child.label.startswith('VB'):\n",
    "            predicate = predicate + child.leaf_labels()\n",
    "\n",
    "    if len(predicate) > 0:\n",
    "        return ' '.join(predicate)\n",
    "\n",
    "    return None\n",
    "\n",
    "def find_object(verb_phase_for_object, parent='VP'):\n",
    "    objects = []\n",
    "    for child in verb_phase_for_object.children:\n",
    "        if child.label == 'VP':\n",
    "            continue\n",
    "        if 'NN' in child.label and parent in ['NP', 'PP', 'ADJP']:\n",
    "            #objects = objects + child.leaf_labels()\n",
    "            new_objects = child.leaf_labels()\n",
    "            for new_object in new_objects:\n",
    "                if new_object not in objects:\n",
    "                    objects.append(new_object)\n",
    "        else:\n",
    "            new_objects = find_object(child, child.label)\n",
    "            #if new_objects not in objects and new_objects is not None:\n",
    "            for new_object in new_objects:\n",
    "                if new_object not in objects:\n",
    "                    objects.append(new_object)\n",
    "                #objects = objects + new_objects\n",
    "\n",
    "    return objects\n",
    "    # if len(objects) > 0:\n",
    "    #     #return ' '.join(objects)\n",
    "    #     return objects\n",
    "    # else:\n",
    "    #     return None\n",
    "\n",
    "###### SPO Function: `find_spo(tree)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SPO Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:03.008883Z",
     "start_time": "2023-04-08T00:35:01.890215Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_spo(tree):\n",
    "    noun_phrases_for_subject = find_branches(tree, label='NP', not_in_label='VP', ancestors=[])\n",
    "    subject_list = []\n",
    "    for noun_phrase_for_subject in noun_phrases_for_subject:\n",
    "        subject = find_subject(noun_phrase_for_subject)\n",
    "        #if subject is not None:\n",
    "        #   subject_list.append(subject)\n",
    "        subject_list = subject_list + subject\n",
    "\n",
    "    verb_phrases = find_branches(tree, label='VP')\n",
    "    predicate_list = []\n",
    "    object_list = []\n",
    "    for verb_phrase in verb_phrases:\n",
    "        predicate = find_predicate(verb_phrase)\n",
    "        if predicate is not None:\n",
    "            predicate_list.append(predicate)\n",
    "        object = find_object(verb_phrase)\n",
    "        object_list = object_list + object\n",
    "        #if object is not None:\n",
    "        #    object_list.append(object)\n",
    "\n",
    "    # dedupe list\n",
    "    subject_list = list(dict.fromkeys(subject_list))\n",
    "    predicate_list = list(dict.fromkeys(predicate_list))\n",
    "    object_list = list(dict.fromkeys(object_list))\n",
    "\n",
    "    return subject_list, predicate_list, object_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Experimental Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:03.008883Z",
     "start_time": "2023-04-08T00:35:01.890215Z"
    }
   },
   "outputs": [],
   "source": [
    "# # set 'download_method = None' to not download the resources over and over\n",
    "# nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency', download_method=None, use_gpu=False)\n",
    "\n",
    "# def trunk_construction(str, parent_label = None):\n",
    "#     doc = nlp(str)\n",
    "#     tree = doc.sentences[0].constituency\n",
    "\n",
    "#     words = construct_sentence(tree, parent_label)\n",
    "#     return ' '.join(words)\n",
    "\n",
    "# def construct_sentence(tree, parent_label = None, leave_pos=False):\n",
    "\n",
    "#     sentences = []\n",
    "#     if 'NN' in tree.label:\n",
    "#         if parent_label == 'NP':\n",
    "#             # sentences.append(tree)\n",
    "#             sentences = sentences + tree.leaf_labels()\n",
    "#     if 'VB' in tree.label:\n",
    "#         if parent_label == 'VP':\n",
    "#             #sentences.append(tree)\n",
    "#             sentences = sentences + tree.leaf_labels()\n",
    "#     for child in tree.children:\n",
    "#         sentences = sentences + construct_sentence(child, tree.label)\n",
    "\n",
    "#     return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def test_parser(str, valid_subject, valid_predicate, valid_object):\n",
    "\n",
    "#     #new_sentence = trunk_construction(str)\n",
    "#     #print(new_sentence)\n",
    "#     #assert new_sentence == valid_sentence\n",
    "\n",
    "#     doc = nlp(str)\n",
    "#     tree = doc.sentences[0].constituency\n",
    "\n",
    "#     subject_list, predicate_list, object_list = find_spo(tree)\n",
    "#     print(f\"Subject = {' '.join(subject_list)}\")\n",
    "#     print(f\"Predicate = {' '.join(predicate_list)}\")\n",
    "#     print(f\"Object = {' '.join(object_list)}\")\n",
    "\n",
    "#     print(f\"{subject_list} = {valid_subject}\")\n",
    "#     print(f\"{predicate_list} = {valid_predicate}\")\n",
    "#     print(f\"{object_list} = {valid_object}\")\n",
    "#     assert subject_list == valid_subject\n",
    "#     assert predicate_list == valid_predicate\n",
    "#     assert object_list == valid_object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:08.918548Z",
     "start_time": "2023-04-08T00:35:03.023262Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_parser(\"\"\"\"We have an incredible amount of work to do, but it is not in [designing new] instruction set architectures.\"\"\", [], ['have', 'do', 'is', 'designing'], ['amount', 'work', 'set', 'architectures'])\n",
    "# test_parser(\"\"\"We have got an incredible amount of work to do, but it ain't in the instruction set,\" he said.\"\"\",  ['instruction', 'set'], ['have', 'got', 'do', 'said'], ['amount', 'work'])\n",
    "# test_parser('Syrian forces launch new attacks', ['forces'], ['launch'], ['attacks'])\n",
    "# test_parser(\"\"\"the flat tire was replaced by the driver\"\"\", ['tire'], ['was', 'replaced'], ['driver'])\n",
    "# test_parser(\"\"\"Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.\"\"\",\n",
    "#             ['Amrozi'], ['accused', 'called', 'distorting'], ['brother',  'witness', 'evidence'])\n",
    "# test_parser(\"\"\"Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.\"\"\",\n",
    "#          ['Amrozi'], ['Referring', 'accused', 'distorting'], ['witness', 'brother', 'evidence'])\n",
    "# test_parser(\"\"\"Shares of Genentech, a much larger company with several products on the market, rose more than 2 percent\"\"\",\n",
    "#             ['Shares', 'Genentech', 'company', 'products', 'market'], ['rose'], ['percent'])\n",
    "\n",
    "# test_parser(\"\"\"Shares of Xoma fell 16 percent in early trade, while shares of Genentech, a much larger company with several products on the market, were up 2 percent.\"\"\", ['Shares', 'Xoma'], ['fell', 'were'], ['percent', 'trade', 'shares', 'Genentech', 'company', 'products', 'market'])\n",
    "\n",
    "# test_parser(\"\"\"Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\"\"\",\n",
    "#               ['Gyorgy', 'Heizler', 'head', 'disaster', 'unit'], ['said', 'was', 'carrying'], ['coach', 'passengers'])\n",
    "# test_parser(\"\"\"The head of the local disaster unit, Gyorgy Heizler, said the coach driver had failed to heed red stop lights.\"\"\",\n",
    "#             ['head', 'disaster', 'unit', 'Gyorgy', 'Heizler'], ['said', 'had', 'failed', 'heed'], ['coach', 'driver', 'stop', 'lights'])\n",
    "# # test_parser(\"\"\"His wife said he was \"100 percent behind George Bush\" and looked forward to using his years of training in the war.\"\"\",\n",
    "# #             \"wife said was percent George Bush looked using years training war\")\n",
    "# test_parser(\"\"\"Sheena Young of Child, the national infertility support network, hoped the guidelines would lead to a more \"fair and equitable\" service for infertility sufferers\"\"\", ['Sheena', 'Young', 'Child', 'network'], ['hoped', 'lead'], ['guidelines', 'service', 'infertility', 'sufferers'])\n",
    "# test_parser(\"\"\"Sheena Young, for Child, the national infertility support network, said the proposed guidelines should lead to a more \"fair and equitable\" service for infertility sufferers.\"\"\", ['Sheena', 'Young', 'Child', 'network'], ['said', 'lead'], ['guidelines', 'service', 'infertility', 'sufferers'])\n",
    "# #\n",
    "# # test_parser(\"\"\"Among CNN viewers, 29 percent said they were Republicans and 36 percent called themselves conservatives.\"\"\",\n",
    "# #             \"CNN viewers percent said were Republicans percent called conservatives\")\n",
    "# # test_parser(\"\"\"Out of Fox viewers, 41 percent describe themselves as Republicans, 24 percent as Democrats and 30 percent as Independents\"\"\",\n",
    "# #             \"Fox viewers percent describe Republicans percent Democrats percent Independents\")\n",
    "\n",
    "# # Note: stanza parser has a problem with the below sentence.  It is unable to parse it correctly\n",
    "# # test_parser(\"\"\"Sheena Young, for Child, the national infertility support network, said the proposed guidelines should lead to a more \"fair and equitable\" service for infertility sufferers.\"\"\", \"\")\n",
    "# # test_parser(\"\"\"Among Fox viewers, 41 percent describe themselves as Republicans, 24 percent as Democrats and 30 percent as Independents\"\"\", \"Fox viewers percent describe Republicans percent Democrats percent Independents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the Stanford Parser, a _SPO Function_ is implemented to traverse a sentence _tree_ or `doc` to locate the SPO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SPO Function: `find_spacy_spo(doc)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_spacy_spo(doc):\n",
    "    # Extract the subject, predicate, and object\n",
    "    subject = []\n",
    "    predicate = []\n",
    "    obj = []\n",
    "\n",
    "    for token in doc:\n",
    "        #print(f\"{token.dep_} : {token.text}\")\n",
    "        if \"subj\" in token.dep_:\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            subject .append(doc[start:end])\n",
    "        elif \"obj\" in token.dep_ or \"pcomp\" in token.dep_:\n",
    "            obj.append(token.text)\n",
    "        elif \"ROOT\" in token.dep_ or \"pred\" in token.dep_:\n",
    "            predicate.append(token.text)\n",
    "\n",
    "    # Print the results\n",
    "    # print(\"Subject: \", subject)\n",
    "    # print(\"Predicate: \", predicate)\n",
    "    # print(\"Object: \", obj)\n",
    "    return subject, predicate, object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Char Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To-be implemented "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ablation 2:_ We found the parts-of-speech parsing to be slow.  In order to speed it up we implemented a multi-threaded parsing class called `SentenceProcessingThread`.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concurreny Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:08.941546Z",
     "start_time": "2023-04-08T00:35:08.905455Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentenceProcessingThread(threading.Thread):\n",
    "    def __init__(self, sentences, output_list, begin, end, parsing_enum=Parsing.STANZA):\n",
    "        super(SentenceProcessingThread, self).__init__()\n",
    "        self.sentences = sentences\n",
    "        self.parsing_enum = parsing_enum\n",
    "\n",
    "        if parsing_enum == Parsing.STANZA:\n",
    "            self.nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency', download_method=None, use_gpu=True)\n",
    "        else:\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.output_list = output_list\n",
    "        self.begin = begin\n",
    "        self.end = end\n",
    "\n",
    "    def trunk_construction(self, str, parent_label = None):\n",
    "        doc = self.nlp(str)\n",
    "        tree = doc.sentences[0].constituency\n",
    "\n",
    "        #words = construct_sentence(tree, parent_label)\n",
    "        #return ' '.join(words)\n",
    "\n",
    "        if self.parsing_enum == Parsing.SPACY:\n",
    "            subjects, predicates, objects = find_spacy_spo(tree)\n",
    "        else:\n",
    "            subjects, predicates, objects = find_spo(tree)\n",
    "\n",
    "        return f\"{' '.join(subjects)},{' '.join(predicates)},{' '.join(objects)}\"\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"going to process {self.begin} to {self.end}\")\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "            new_sentence = self.trunk_construction(sentence)\n",
    "            self.output_list[self.begin + i] = new_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class instantiated a stanza parser and took as parameters the sentences to process, the output list and the start and end indexes of the output list to store the results in.  Additionally, we were able to detect if GPUs were available on the host machine and if they were we set the number of threads to a certain value and ran the stanza parser with GPUs enabled, adding additional performance improvements to the parsing performance.  If GPU support was not enabled we changed the number of threads to use and ran in a slightly different configuration under CPU.  \n",
    "\n",
    "The original paper presents a pseudo-code algorithm, found in \\emph{Section III, subsection B SPO Kernel}.  The paper had some discrepancies, where the pseudo-code did not match the textual description, found in \\emph{Algorithm 1, Trunk Construction}.  We implemented our parsing algorithm according to the textual description, which seemed more elaborate than the pseudo-code, and from information gleaned from \\emph{Figure 3}, shown below for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the parser using some of the training data sentences as input and asserting the output sentence matches the algorithm defined in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Parsing(Enum):\n",
    "    STANZA=1\n",
    "    SPACY=2\n",
    "    RAW=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrency Parsing\n",
    "Added support for concurrent parsing.  This can help in the performance of the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:08.941546Z",
     "start_time": "2023-04-08T00:35:08.905455Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_sentences_concurrently(sentences, output, p=2):\n",
    "    total = len(sentences)\n",
    "    interval = int(total / p)\n",
    "    threads = []\n",
    "    for i in range(p):\n",
    "        s = i*interval\n",
    "        if i == p-1:\n",
    "            e = total\n",
    "        else:\n",
    "            e = (i+1) * interval\n",
    "        sentences_slice = sentences[s:e]\n",
    "        sentence_thread = SentenceProcessingThread(sentences_slice, output, s, e)\n",
    "        sentence_thread.start()\n",
    "        threads.append(sentence_thread)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "def preprocess_corpus(input_file='data/msr_paraphrase_train.txt', output_file='data/msr_paraphrase_train_stanza.txt', N=None, parsing_enum=Parsing.STANZA):\n",
    "    print(output_file)\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"{output_file} already exists\")\n",
    "        return\n",
    "\n",
    "    starttime = datetime.datetime.now()\n",
    "    df = read_file(input_file)\n",
    "\n",
    "    if N is None:\n",
    "        N = len(df.String1)\n",
    "\n",
    "    output1 = [None] * N\n",
    "    output2 = [None] * N\n",
    "\n",
    "    # we can process with more threads if we only have CPU\n",
    "    p = 8\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # if cuda is available we don't need that many threads\n",
    "        # and if the number of threads is set too large using cuda\n",
    "        # we can get out of memory exceptions\n",
    "        p = 2\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    process_sentences_concurrently(df.String1[:N], output1, p)\n",
    "\n",
    "    # try and be careful with gpu memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    process_sentences_concurrently(df.String2[:N], output2, p)\n",
    "\n",
    "    endtime = datetime.datetime.now()\n",
    "\n",
    "    print(f\"time to process {N*2} sentences is {endtime - starttime}\")\n",
    "\n",
    "    stanza_df = df[:N]\n",
    "\n",
    "    processed_string1 = pd.Series(output1)\n",
    "    # processed_string1.apply(gensim.utils.simple_preprocess)\n",
    "    processed_string2 = pd.Series(output2)\n",
    "    #processed_string2.apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "    stanza_df.String1 = processed_string1\n",
    "    stanza_df.String2 = processed_string2\n",
    "\n",
    "    # write the file out.  This can help in the future\n",
    "    print(f\"about to write out {output_file}\")\n",
    "    stanza_df.to_csv(output_file, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Preprocessing\n",
    "pass the input sentences from the training dataset through the stanford/stanza parser, extracting the relevant parts of speech and then tokenize the processed sentences using the gensim.utils.simple_preprocess utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Embeddings\n",
    "Take the preprocessed and tokenized sentences and use Word2Vec to get the word embeddings.  Take each word embedding in a sentence and find the mean which will represent the embedding for the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:08.942658Z",
     "start_time": "2023-04-08T00:35:08.908525Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Function is broken out for testing purposes\n",
    "def generate_word2vec_model(corpus):\n",
    "    # Creating the Word2Vec model\n",
    "    model = Word2Vec(sentences=corpus, min_count=1, window=2, vector_size=50)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:08.942887Z",
     "start_time": "2023-04-08T00:35:08.910846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function is broken out for testing purposes\n",
    "def sentence_embeddings(w2v_model, sentence, size):\n",
    "    np_embedding = np.zeros(size)\n",
    "    for i, word in enumerate(sentence):\n",
    "        #print(word)\n",
    "        np_embedding[i] = w2v_model.wv.get_vector(word)\n",
    "\n",
    "    return np_embedding\n",
    "    # list = []\n",
    "    # for word in sentence:\n",
    "    #     list.append(w2v_model.wv.get_vector(word))\n",
    "    #\n",
    "    # word_matrix = np.row_stack(list)\n",
    "    # #return np.mean(word_matrix, axis=0)\n",
    "    # return word_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:09.477774Z",
     "start_time": "2023-04-08T00:35:08.911696Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_word2vec():\n",
    "\n",
    "    df = read_file('data/msr_paraphrase_train.txt')\n",
    "\n",
    "    sentences1 = df.String1[:5].apply(gensim.utils.simple_preprocess)\n",
    "    sentences2 = df.String2[:5].apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "    corpus = pd.concat([sentences1, sentences2], ignore_index=True)\n",
    "\n",
    "    max_sentence_len = corpus.apply(len).max()\n",
    "\n",
    "    model = generate_word2vec_model(corpus)\n",
    "\n",
    "    embedding = sentence_embeddings(model, corpus[0], (max_sentence_len, 50))\n",
    "    assert embedding.shape == (max_sentence_len, 50)\n",
    "\n",
    "test_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_word2vec(train_input_file, test_input_file, parsing_enum=Parsing.STANZA):\n",
    "\n",
    "    if parsing_enum == Parsing.RAW:\n",
    "        train_output_file = train_input_file\n",
    "        test_output_file = test_input_file\n",
    "        train_df = read_file(train_output_file)\n",
    "        test_df = read_file(test_output_file)\n",
    "\n",
    "    elif parsing_enum == Parsing.STANZA:\n",
    "        file_parts = os.path.splitext(train_input_file)\n",
    "        train_output_file = f\"{file_parts[0]}_stanza{file_parts[1]}\"\n",
    "        print(\"About to preprocess spacy data\")\n",
    "        preprocess_corpus(input_file=train_input_file, output_file=train_output_file, parsing_enum=parsing_enum)\n",
    "        print(\"Done preprocessing spacy data\")\n",
    "\n",
    "        file_parts = os.path.splitext(test_input_file)\n",
    "        test_output_file = f\"{file_parts[0]}_stanza{file_parts[1]}\"\n",
    "        print(\"About to preprocess data\")\n",
    "        preprocess_corpus(input_file=test_input_file, output_file=test_output_file, parsing_enum=parsing_enum)\n",
    "        print(\"Done preprocessing data\")\n",
    "        train_df = pd.read_csv(train_output_file, sep=\"\\t\")\n",
    "        test_df = pd.read_csv(test_output_file, sep=\"\\t\")\n",
    "    else:\n",
    "        file_parts = os.path.splitext(train_input_file)\n",
    "        train_output_file = f\"{file_parts[0]}_spacy{file_parts[1]}\"\n",
    "        print(\"About to preprocess spacy data\")\n",
    "        preprocess_corpus(input_file=train_input_file, output_file=train_output_file, parsing_enum=parsing_enum)\n",
    "        print(\"Done preprocessing spacy data\")\n",
    "\n",
    "        file_parts = os.path.splitext(test_input_file)\n",
    "        test_output_file = f\"{file_parts[0]}_spacy{file_parts[1]}\"\n",
    "        print(\"About to preprocess data\")\n",
    "        preprocess_corpus(input_file=test_input_file, output_file=test_output_file, parsing_enum=parsing_enum)\n",
    "        print(\"Done preprocessing data\")\n",
    "        train_df = pd.read_csv(train_output_file, sep=\"\\t\")\n",
    "        test_df = pd.read_csv(test_output_file, sep=\"\\t\")\n",
    "\n",
    "    # train_df = read_file(train)\n",
    "    # test_df = read_file(test)\n",
    "\n",
    "    train_sentences1 = train_df.String1.apply(gensim.utils.simple_preprocess)\n",
    "    train_sentences2 = train_df.String2.apply(gensim.utils.simple_preprocess)\n",
    "    test_sentences1 = test_df.String1.apply(gensim.utils.simple_preprocess)\n",
    "    test_sentences2 = test_df.String2.apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "    corpus = pd.concat([train_sentences1, train_sentences2, test_sentences1, test_sentences2], ignore_index=True)\n",
    "    max_sentence_len = corpus.apply(len).max()\n",
    "\n",
    "    word2vec = generate_word2vec_model(corpus)\n",
    "\n",
    "\n",
    "    return word2vec, max_sentence_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:09.479438Z",
     "start_time": "2023-04-08T00:35:08.972122Z"
    }
   },
   "outputs": [],
   "source": [
    "def corpus_embeddings(model, corpus, max_sentence_len):\n",
    "    corpus_size = len(corpus)\n",
    "    embeddings_list = []\n",
    "    embedding_matrix = np.zeros((corpus_size, max_sentence_len, 50))\n",
    "    for i, sentence in enumerate(corpus):\n",
    "        embeddings = sentence_embeddings(model, sentence, size=(max_sentence_len, 50))\n",
    "        embedding_matrix[i] = embeddings\n",
    "        embeddings_list.append(embeddings)\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T14:32:14.897963Z",
     "start_time": "2023-04-07T14:32:14.561411Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_dataset():\n",
    "    word2vec, max_sentence_length = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt', parsing_enum=Parsing.STANZA)\n",
    "    dataset = MSPCDataset('data/msr_paraphrase_train.txt', word2vec, max_sentence_length, 10)\n",
    "    assert len(dataset) == 10\n",
    "\n",
    "    x1, y1, quality = dataset[0]\n",
    "    assert x1.shape[0] == max_sentence_length\n",
    "    assert x1.shape[1] == 50\n",
    "\n",
    "test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_raw_sentences():\n",
    "    word2vec_raw, max_sentence_length_raw = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt', Parsing.RAW)\n",
    "    dataset = MSPCDataset('data/msr_paraphrase_train.txt', word2vec_raw, max_sentence_length_raw, 10, parsing_enum=Parsing.RAW)\n",
    "    assert len(dataset) == 10\n",
    "\n",
    "    x1, y1, quality = dataset[0]\n",
    "    assert x1.shape[0] == max_sentence_length_raw\n",
    "    assert x1.shape[1] == 50\n",
    "\n",
    "test_dataset_raw_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_spacy_sentences():\n",
    "    word2vec_spacy, max_sentence_length_spacy = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt', parsing_enum=Parsing.SPACY)\n",
    "    dataset = MSPCDataset('data/msr_paraphrase_train.txt', word2vec_spacy, max_sentence_length_spacy, 10, parsing_enum=Parsing.SPACY)\n",
    "    assert len(dataset) == 10\n",
    "\n",
    "    x1, y1, quality = dataset[0]\n",
    "    assert x1.shape[0] == max_sentence_length_spacy\n",
    "    assert x1.shape[1] == 50\n",
    "\n",
    "test_dataset_spacy_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "Create training and test dataloaders for sentences parsed with parts-of-speech parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec, max_sentence_length = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt')\n",
    "train_dataset = MSPCDataset('data/msr_paraphrase_train.txt', word2vec, max_sentence_length)\n",
    "test_dataset = MSPCDataset('data/msr_paraphrase_test.txt', word2vec, max_sentence_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_raw_sentences, max_sentence_length_raw_sentences = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt')\n",
    "train_dataset_raw_sentences = MSPCDataset('data/msr_paraphrase_train.txt', word2vec_raw_sentences, max_sentence_length_raw_sentences)\n",
    "test_dataset_raw_sentences = MSPCDataset('data/msr_paraphrase_test.txt', word2vec_raw_sentences, max_sentence_length_raw_sentences)\n",
    "\n",
    "train_dataloader_raw_sentences = DataLoader(train_dataset_raw_sentences, batch_size=64, shuffle=False)\n",
    "test_dataloader_raw_sentences = DataLoader(test_dataset_raw_sentences, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_spacy_sentences, max_sentence_length_spacy_sentences = init_word2vec('data/msr_paraphrase_train.txt', 'data/msr_paraphrase_test.txt')\n",
    "train_dataset_spacy_sentences = MSPCDataset('data/msr_paraphrase_train.txt', word2vec_spacy_sentences, max_sentence_length_spacy_sentences)\n",
    "test_dataset_spacy_sentences = MSPCDataset('data/msr_paraphrase_test.txt', word2vec_spacy_sentences, max_sentence_length_spacy_sentences)\n",
    "\n",
    "train_dataloader_spacy_sentences = DataLoader(train_dataset_spacy_sentences, batch_size=64, shuffle=False)\n",
    "test_dataloader_spacy_sentences = DataLoader(test_dataset_spacy_sentences, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T14:32:15.560063Z",
     "start_time": "2023-04-07T14:32:15.557166Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_output_volume(W, F, S, P):\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: Given the input volume size $W$, the kernel/filter size $F$,\n",
    "    the stride $S$, and the amount of zero padding $P$ used on the border,\n",
    "    calculate the output volume size.\n",
    "    Note the output should a integer.\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    #https://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "    return int((W-F+2*P)/S+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T14:32:18.377984Z",
     "start_time": "2023-04-07T14:32:18.231576Z"
    }
   },
   "outputs": [],
   "source": [
    "print(conv_output_volume(50, 3, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layers\n",
    "\n",
    "The original paper used a dynamic k-max pooling method in their model. The _k_ value is determine by equation (1).\n",
    "\n",
    "\\begin{equation*} k=\\max \\left({k_{top},\\left \\lceil{ \\frac {L-l}{L} \\left |{ s }\\right | }\\right \\rceil }\\right)\\end{equation*}\n",
    "\n",
    "__Dynamic K-Max Pooling Implementation:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T15:50:59.325715Z",
     "start_time": "2023-04-07T15:50:59.023740Z"
    }
   },
   "outputs": [],
   "source": [
    "# #https://gist.github.com/anna-hope/7a2b2e66c3645aa8e4f94dbf06aed8dc\n",
    "# \"\"\"\n",
    "# TODO: Remove hardcoded values and max it dynamic.\n",
    "# \"\"\"\n",
    "# class DynamicKMaxPooling(nn.Module):\n",
    "#     def __init__(self, k_init, conv_layers, layer):\n",
    "#         super().__init__()\n",
    "#         # \"L is the total  number  of  convolutional  layers\n",
    "#         # in  the  network;\n",
    "#         # ktop is the fixed pooling parameter for the\n",
    "#         # topmost  convolutional  layer\"\n",
    "#         self.k_init = k_init\n",
    "#         self.conv_layers = conv_layers\n",
    "#         self.layer = layer\n",
    "#\n",
    "#     def forward(self, X):\n",
    "#         s = 50\n",
    "#         dyn_k = ((self.conv_layers - self.layer) / self.conv_layers) * 3\n",
    "#         k_max = int(round(max(self.k_init, np.ceil(dyn_k))))\n",
    "#         print(k_max)\n",
    "#         out = F.max_pool1d(X, kernel_size=k_max)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Similarity Convolution Network (SSCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "\n",
    "The original paper used a dynamic k-max pooling method in their model. The _k_ value is determine by equation (1).\n",
    "\n",
    "\\begin{equation*} k=\\max \\left({k_{top},\\left \\lceil{ \\frac {L-l}{L} \\left |{ s }\\right | }\\right \\rceil }\\right)\\end{equation*}\n",
    "\n",
    "__Dynamic K-Max Pooling Implementation:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:09.528064Z",
     "start_time": "2023-04-08T00:35:09.384185Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://gist.github.com/anna-hope/7a2b2e66c3645aa8e4f94dbf06aed8dc\n",
    "\"\"\"\n",
    "TODO: Remove hardcoded values and max it dynamic.\n",
    "\"\"\"\n",
    "class DynamicKMaxPooling(nn.Module):\n",
    "    def __init__(self, k_init, conv_layers):\n",
    "        super().__init__()\n",
    "        # \"L is the total  number  of  convolutional  layers\n",
    "        # in  the  network;\n",
    "        # ktop is the fixed pooling parameter for the\n",
    "        # topmost  convolutional  layer\"\n",
    "        self.k_init = k_init\n",
    "        self.conv_layers = conv_layers\n",
    "\n",
    "    def pool(self, X, l):\n",
    "        # s is sequence length\n",
    "        # l is current layer in network\n",
    "        s = X.shape[2]\n",
    "        dyn_k = ((self.conv_layers - l) / self.conv_layers) * s\n",
    "        k_max = int(round(max(self.k_init, np.ceil(dyn_k))))\n",
    "        return F.max_pool1d(X, kernel_size=k_max)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer_i in range(self.conv_layers,0,-1):\n",
    "            X = self.pool(X, layer_i)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Dynamic K-Max Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:09.528155Z",
     "start_time": "2023-04-08T00:35:09.384376Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_OF_SAMPLES = 20\n",
    "SAMPLE_SIZE = 2\n",
    "OUTPUT_SIZE = 15\n",
    "\n",
    "test_embedding = torch.rand((NUM_OF_SAMPLES, SAMPLE_SIZE, OUTPUT_SIZE))\n",
    "dyn_k_layer = DynamicKMaxPooling(3, SAMPLE_SIZE)\n",
    "\n",
    "# Call forward with convolution layer index [2,1]\n",
    "out = dyn_k_layer(test_embedding)\n",
    "\n",
    "assert out.shape[2] == 1\n",
    "assert out.shape[1] == SAMPLE_SIZE\n",
    "assert out.shape[0] == NUM_OF_SAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Similarity\n",
    "\n",
    " \\begin{align*} Man(\\vec V_{x}, \\vec V_{y})=&\\left |{ x_{1}-y_{1} }\\right |\\! +\\! \\left |{ x_{2}-y_{2} }\\right | \\!+ \\!\\ldots \\!+ \\!\\left |{ x_{n}-y_{n} }\\right |\n",
    " \\\\ score=&e^{-Man(\\vec V_{x}, \\vec V_{y})},\\quad score\\in [{0,1}] \\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T00:35:09.528218Z",
     "start_time": "2023-04-08T00:35:09.385728Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* X: Pooled output of SSCN model of shape (sample_size, -1)\n",
    "* For the purpose of this experiment sample_size = 2\n",
    "\"\"\"\n",
    "def manhattan_similarity_score(X):\n",
    "    sample_count, _, M = X.shape\n",
    "    Vx = X[:,0].reshape((sample_count,M))\n",
    "    Vy = X[:,1].reshape((sample_count,M))\n",
    "    mdist = torch.sum(torch.abs(Vx-Vy),dim=1).view(sample_count,-1)\n",
    "    score = torch.exp(-1*mdist)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMaxPool1d(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super(KMaxPool1d, self).__init__()\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input shape (batch_size, num_channels, sequence_length)\n",
    "        # output shape (batch_size num_channels, k)\n",
    "        k_max_values, k_max_indices  = torch.topk(x, self.k, dim=2)\n",
    "        return k_max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicKMaxPoolId(nn.Module):\n",
    "    def __init__(self, k, l, L):\n",
    "        super(DynamicKMaxPoolId, self).__init__()\n",
    "        self.k = k\n",
    "        self.l = l\n",
    "        self.L = L\n",
    "\n",
    "    def forward(self, x, sentence_length):\n",
    "        ktop = max(self.k, int((self.L - self.l)/self.L * sentence_length))\n",
    "        #print(f\"ktop: {ktop}\")\n",
    "        k_max_values, k_max_indices = torch.topk(x, ktop, dim=2)\n",
    "        return k_max_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentenceSimilarityCNN2(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_filters, filter_size, hidden_dim, dropout_prob=0.5):\n",
    "        super(SentenceSimilarityCNN2, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=filter_size, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels= num_filters, out_channels=num_filters * 2, kernel_size=filter_size, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels= num_filters * 2, out_channels=num_filters * 3, kernel_size=filter_size, padding=1)\n",
    "\n",
    "        self.pool1 = DynamicKMaxPoolId(k=3, l=1, L=3)\n",
    "\n",
    "        self.pool2 = DynamicKMaxPoolId(k=3, l=2, L=3)\n",
    "\n",
    "        self.k = 3\n",
    "        self.kmaxPool1d = KMaxPool1d(k=self.k)\n",
    "\n",
    "        #self.fc1 = nn.Linear(self.k * num_filters , hidden_dim)\n",
    "        self.fc1 = nn.Linear(self.k * num_filters * 3, hidden_dim)\n",
    "\n",
    "    def forward(self, input1_embedded, input2_embedded):\n",
    "\n",
    "        # input: input1_embedded is sentence1 and input2_embedding is sentence2\n",
    "        # input shape: (batch_size, max_sentence_length, embedding_size)\n",
    "        # output shap: (50) dimension vector that represents the sentence\n",
    "        # output: similarity score\n",
    "\n",
    "        # Find the sentence lengths\n",
    "        sent_length1 = (torch.nonzero(input1_embedded).max(dim=0).values[1] + 1).item()\n",
    "        sent_length2 = (torch.nonzero(input2_embedded).max(dim=0).values[1] + 1).item()\n",
    "        #print(f\"sent length1: {sent_length1} AND sent length2: {sent_length2}\")\n",
    "\n",
    "        # Convolution\n",
    "        #print(input1_embedded.shape)\n",
    "        # input shape (batch_size, max_sentence_length, embedding_size)\n",
    "        # permuted shape (batch_size, embedding_size, max_sentence_length)\n",
    "        # output shape (batch_size, out_channels, (max_sentence_length - kernel_size + 2*padding)/stride + 1\n",
    "        # i.e. if max_sentence_length=19 and kernel_size=3 and padding=1 and stride=1 and out_channels=64\n",
    "        #      output_shape (19-3+2*1)/1 + 1 = 19 => (64, 64, 19)\n",
    "        input1_embedded = self.conv1(input1_embedded.permute(0, 2, 1))\n",
    "        input2_embedded = self.conv1(input2_embedded.permute(0, 2, 1))\n",
    "        #print(f\"output of conv1: {input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = torch.relu(input1_embedded)\n",
    "        input2_embedded = torch.relu(input2_embedded)\n",
    "\n",
    "        # pool1 is dynamic k-max pooling and is a function of the following formula\n",
    "        # max(k-top, (L-l)/l * |s|) where L is total number of convolutional layers, l is the current convolution layer and |s| is sentence length k-top is the\n",
    "        # k-top important features and serves as a lower bound... we will always try and find at least k-top features\n",
    "        input1_embedded = self.pool1(input1_embedded, sent_length1)\n",
    "        input2_embedded = self.pool1(input2_embedded, sent_length2)\n",
    "        #print(f\"output of pool1: {input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = self.conv2(input1_embedded)\n",
    "        input2_embedded = self.conv2(input2_embedded)\n",
    "        #print(f\"output of conv2: {input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = torch.relu(input1_embedded)\n",
    "        input2_embedded = torch.relu(input2_embedded)\n",
    "\n",
    "        input1_embedded = self.pool2(input1_embedded, sent_length1)\n",
    "        input2_embedded = self.pool2(input2_embedded, sent_length2)\n",
    "        #print(f\"output of pool2: {input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = self.conv3(input1_embedded)\n",
    "        input2_embedded = self.conv3(input2_embedded)\n",
    "        #print(f\"output of conv3: {input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = torch.relu(input1_embedded)\n",
    "        input2_embedded = torch.relu(input2_embedded)\n",
    "\n",
    "        input1_embedded = self.kmaxPool1d(input1_embedded)\n",
    "        input2_embedded = self.kmaxPool1d(input2_embedded)\n",
    "        #print(f\"output of k-max pool: {input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = input1_embedded.view(input1_embedded.shape[0], input1_embedded.shape[1] * input1_embedded.shape[2])\n",
    "        input2_embedded = input2_embedded.view(input2_embedded.shape[0], input2_embedded.shape[1] * input2_embedded.shape[2])\n",
    "\n",
    "        #kmax_input1_embedded = self.kmaxPool1d(input1_embedded)\n",
    "        #input1_embedded = F.max_pool1d(input1_embedded, input1_embedded.shape[2]).squeeze(2)\n",
    "        #nput2_embedded = F.max_pool1d(input2_embedded, input2_embedded.shape[2]).squeeze(2)\n",
    "        #print(f\"output of max pool: {input1_embedded.shape}\")\n",
    "        #print(f\"output of kmax pool: {kmax_input1_embedded.shape}\")\n",
    "\n",
    "        input1_embedded = self.fc1(input1_embedded)\n",
    "        input2_embedded = self.fc1(input2_embedded)\n",
    "        #print(input1_embedded.shape)\n",
    "\n",
    "        man_dist = torch.sum(torch.abs(input1_embedded - input2_embedded), axis=1)\n",
    "        # sentence1_mean = torch.mean(x1, axis=1)\n",
    "        # sentence2_mean = torch.mean(x2, axis=1)\n",
    "        # man_dist = torch.sum(torch.abs(sentence1_mean - sentence2_mean), axis=1)\n",
    "        # print(man_dist.shape)\n",
    "\n",
    "        return torch.exp(-man_dist)\n",
    "\n",
    "\n",
    "        #input2_embedded = self.conv1(input2_embedded.permute(0, 2, 1))\n",
    "\n",
    "\n",
    "        # Max pooling\n",
    "        # input1_pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in input1_conv]\n",
    "        # input2_pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in input2_conv]\n",
    "        #\n",
    "        # # Concatenate and flatten\n",
    "        # input1_concat = torch.cat(input1_pooled, dim=1)\n",
    "        # input2_concat = torch.cat(input2_pooled, dim=1)\n",
    "        #\n",
    "        # # Concatenate the two sentence representations\n",
    "        # sentence_similarity = torch.cat([input1_concat, input2_concat], dim=1)\n",
    "        #\n",
    "        # # Dense layers\n",
    "        # #sentence_similarity = self.dropout(F.relu(self.fc1(sentence_similarity)))\n",
    "        # #sentence_similarity = self.fc2(sentence_similarity)\n",
    "        #\n",
    "        # return torch.sigmoid(sentence_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2vec.wv)\n",
    "embedding_dim = 50\n",
    "num_filters = 64\n",
    "filter_size = 3\n",
    "hidden_dim = 50\n",
    "dropout = 0.5\n",
    "n_epochs=150\n",
    "\n",
    "def train(train_loader, n_epochs=n_epochs):\n",
    "    model = SentenceSimilarityCNN2(embedding_dim, num_filters, filter_size, hidden_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        curr_epoch_loss = []\n",
    "        for x1, x2, y in train_loader:\n",
    "            #print(x1.shape)\n",
    "            y_hat = model(x1, x2)\n",
    "            loss = criterion(y_hat, y.float())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "\n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train stanza sentences\n",
    "start_time = datetime.datetime.now()\n",
    "model = train(train_dataloader)\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Number of epochs: \", n_epochs)\n",
    "print (\"Training dataset size\", len(train_dataset))\n",
    "print(\"Training time: \", (end_time - start_time))\n",
    "\n",
    "# Train the raw sentences\n",
    "start_time = datetime.datetime.now()\n",
    "model_raw_sentences = train(train_dataloader_raw_sentences)\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Number of epochs: \", n_epochs)\n",
    "print (\"Training dataset size\", len(train_dataset))\n",
    "print(\"Training time: \", (end_time - start_time))\n",
    "\n",
    "# Train spacy sentences\n",
    "start_time = datetime.datetime.now()\n",
    "model_spacy_sentences = train(train_dataloader_spacy_sentences)\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Number of epochs: \", n_epochs)\n",
    "print (\"Training dataset size\", len(train_dataset))\n",
    "print(\"Training time: \", (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def eval_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    Y_pred = []\n",
    "    Y = []\n",
    "    for x1, x2, y in test_dataloader:\n",
    "        y_hat = model(x1, x2)\n",
    "        #print(y_hat)\n",
    "        y_pred = torch.zeros(y_hat.shape)\n",
    "        y_pred = (y_hat > 0.3).int()\n",
    "\n",
    "        Y_pred = np.concatenate((Y_pred, y_pred), axis=0)\n",
    "        Y = np.concatenate((Y, y), axis=0)\n",
    "\n",
    "        #print(y_pred)\n",
    "        #print(y)\n",
    "    return Y_pred, Y\n",
    "\n",
    "y_pred, y = eval_model(model, test_dataloader)\n",
    "y_pred_raw, y_raw = eval_model(model_raw_sentences, test_dataloader_raw_sentences)\n",
    "y_pred_spacy, y_spacy = eval_model(model_spacy_sentences, test_dataloader_spacy_sentences)\n",
    "print(\"size of pos test corpus = \", len(test_dataset))\n",
    "print(\"accuracy for pos test corpus = \", accuracy_score(y, y_pred))\n",
    "\n",
    "print(\"size of raw test corpus = \", len(test_dataset_raw_sentences))\n",
    "print(\"accuracy for raw test corpus = \", accuracy_score(y_raw, y_pred_raw))\n",
    "\n",
    "print(\"size of spacy test corpus = \", len(test_dataset_spacy_sentences))\n",
    "print(\"accuracy for spacy test corpus = \", accuracy_score(y_spacy, y_pred_spacy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
