{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 20:29:12 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-01 20:29:12 INFO: Using device: cpu\n",
      "2023-04-01 20:29:12 INFO: Loading: tokenize\n",
      "2023-04-01 20:29:12 INFO: Loading: pos\n",
      "2023-04-01 20:29:12 INFO: Loading: constituency\n",
      "2023-04-01 20:29:12 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# set 'download_method = None' to not download the resources over and over\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency', download_method=None)\n",
    "\n",
    "def trunk_construction(str, parent_label = None):\n",
    "    doc = nlp(str)\n",
    "    tree = doc.sentences[0].constituency\n",
    "\n",
    "    words = construct_sentence(tree, parent_label)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def construct_sentence(tree, parent_label = None, leave_pos=False):\n",
    "\n",
    "    sentences = []\n",
    "    if 'NN' in tree.label:\n",
    "        if parent_label == 'NP':\n",
    "            # sentences.append(tree)\n",
    "            sentences = sentences + tree.leaf_labels()\n",
    "    if 'VB' in tree.label:\n",
    "        if parent_label == 'VP':\n",
    "            #sentences.append(tree)\n",
    "            sentences = sentences + tree.leaf_labels()\n",
    "    for child in tree.children:\n",
    "        sentences = sentences + construct_sentence(child, tree.label)\n",
    "\n",
    "    return sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "def test_parser(str, valid_sentence):\n",
    "\n",
    "    new_sentence = trunk_construction(str)\n",
    "    #new_sentence = ' '.join(words)\n",
    "    assert new_sentence == valid_sentence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "test_parser('Syrian forces launch new attacks', \"forces launch attacks\")\n",
    "test_parser(\"\"\"the flat tire was replaced by the driver\"\"\",\"tire was replaced driver\")\n",
    "test_parser(\"\"\"Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.\"\"\",\n",
    "           \"Amrozi accused brother called witness distorting evidence\")\n",
    "test_parser(\"\"\"Shares of Genentech, a much larger company with several products on the market, rose more than 2 percent\"\"\",\n",
    "            \"Shares Genentech company products market rose percent\")\n",
    "test_parser(\"\"\"Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\"\"\",\n",
    "             \"Gyorgy Heizler head disaster unit said coach was carrying passengers\")\n",
    "test_parser(\"\"\"Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.\"\"\",\n",
    "           \"Referring witness Amrozi accused brother distorting evidence\")\n",
    "test_parser(\"\"\"His wife said he was \"100 percent behind George Bush\" and looked forward to using his years of training in the war.\"\"\",\n",
    "            \"wife said was percent George Bush looked using years training war\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MRPCDataset(Dataset):\n",
    "    def __init__(self, string1, string2, quality):\n",
    "        self.string1 = string1\n",
    "        self.string2 = string2\n",
    "        self.quality = quality\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.string1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.string1, self.string2, self.quality"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_file(file_name):\n",
    "    # Note: Unable to use pd.read_csv... the function complained about an issue with the formatting of the tsv file\n",
    "    # train = pd.read_csv('data/msr_paraphrase_train.txt', sep='\\t', encoding='latin1')\n",
    "    # train\n",
    "\n",
    "    # opting to read file in and split columns manually to create a pandas dataframe\n",
    "    list = []\n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            fields = line.split('\\t')\n",
    "            list.append(fields)\n",
    "\n",
    "    df = pd.DataFrame(list[1:], columns=['Quality', 'ID1', 'ID2', 'String1', 'String2'])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#def collate_fn()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "df = read_file('data/msr_paraphrase_train.txt')\n",
    "\n",
    "processed_string1 = df[:10].String1.apply(trunk_construction)\n",
    "processed_string2 = df[:10].String2.apply(trunk_construction)\n",
    "\n",
    "processed_string1 = processed_string1.apply(gensim.utils.simple_preprocess)\n",
    "processed_string2 = processed_string2.apply(gensim.utils.simple_preprocess)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_string1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "0    [referring, witness, amrozi, accused, brother,...\n1           [yucaipa, bought, dominick, sold, safeway]\n2    [june, ship, owners, had, published, advertise...\n3     [tab, shares, jumped, cents, set, closing, high]\n4    [shares, jumped, percent, stock, exchange, fri...\n5    [scandal, hanging, stewart, company, revenue, ...\n6                     [ixic, rallied, points, percent]\n7       [dvd, cca, appealed, decision, supreme, court]\n8     [earnings, were, affected, tax, benefit, period]\n9              [business, does, fit, growth, strategy]\nName: String2, dtype: object"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_string2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "corpus = pd.concat([processed_string1, processed_string2], ignore_index=True)\n",
    "\n",
    "model = Word2Vec(sentences=corpus, vector_size=(len(processed_string1) + len(processed_string2)), min_count=2)\n",
    "#model.build_vocab(sentences=corpus)\n",
    "#model.train(corpus, total_examples=model.corpus_count, epochs=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/44 is percent\n",
      "word #1/44 is period\n",
      "word #2/44 is year\n",
      "word #3/44 is cents\n",
      "word #4/44 is shares\n",
      "word #5/44 is stock\n",
      "word #6/44 is friday\n",
      "word #7/44 is had\n",
      "word #8/44 is tab\n",
      "word #9/44 is sale\n",
      "word #10/44 is offering\n",
      "word #11/44 is june\n",
      "word #12/44 is internet\n",
      "word #13/44 is jumped\n",
      "word #14/44 is were\n",
      "word #15/44 is published\n",
      "word #16/44 is safeway\n",
      "word #17/44 is dominick\n",
      "word #18/44 is yucaipa\n",
      "word #19/44 is evidence\n",
      "word #20/44 is distorting\n",
      "word #21/44 is witness\n",
      "word #22/44 is brother\n",
      "word #23/44 is accused\n",
      "word #24/44 is advertisement\n",
      "word #25/44 is high\n",
      "word #26/44 is set\n",
      "word #27/44 is strategy\n",
      "word #28/44 is growth\n",
      "word #29/44 is company\n",
      "word #30/44 is fit\n",
      "word #31/44 is does\n",
      "word #32/44 is business\n",
      "word #33/44 is court\n",
      "word #34/44 is supreme\n",
      "word #35/44 is appealed\n",
      "word #36/44 is cca\n",
      "word #37/44 is dvd\n",
      "word #38/44 is closing\n",
      "word #39/44 is dropped\n",
      "word #40/44 is quarter\n",
      "word #41/44 is revenue\n",
      "word #42/44 is exchange\n",
      "word #43/44 is amrozi\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(model.wv.index_to_key):\n",
    "    if index == 120:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(model.wv.index_to_key)} is {word}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "His : poss\n",
      "wife : nsubj\n",
      "said : ROOT\n",
      "he : nsubj\n",
      "was : ccomp\n",
      "\" : punct\n",
      "100 : nummod\n",
      "percent : npadvmod\n",
      "behind : prep\n",
      "George : compound\n",
      "Bush : pobj\n",
      "\" : punct\n",
      "and : cc\n",
      "looked : conj\n",
      "forward : advmod\n",
      "to : prep\n",
      "using : pcomp\n",
      "his : poss\n",
      "years : dobj\n",
      "of : prep\n",
      "training : pobj\n",
      "in : prep\n",
      "the : det\n",
      "war : pobj\n",
      ". : punct\n",
      "[wife, said, he, was, George, Bush, using, years, training, war]\n"
     ]
    }
   ],
   "source": [
    "# Testing the spacy library to extract spo\n",
    "# This is only test code and should not be uncommented.\n",
    "\n",
    "# import spacy\n",
    "#\n",
    "# def get_spacy_subject_phrase(doc):\n",
    "#     for token in doc:\n",
    "#         if (\"subj\" in token.dep_):\n",
    "#             return token\n",
    "#             # subtree = list(token.subtree)\n",
    "#             # start = subtree[0].i\n",
    "#             # end = subtree[-1].i + 1\n",
    "#             # return doc[start:end]\n",
    "#\n",
    "# def get_spacy_predicate_phrase(doc):\n",
    "#     for token in doc:\n",
    "#         if (\"ROOT\" in token.dep_):\n",
    "#             subtree = list(token.subtree)\n",
    "#             start = subtree[0].i\n",
    "#             end = subtree[-1].i + 1\n",
    "#             return token\n",
    "#             # return doc[start:end]\n",
    "#\n",
    "# def get_spacy_object_phrase(doc):\n",
    "#     for token in doc:\n",
    "#         if (\"dobj\" in token.dep_):\n",
    "#             return token\n",
    "#             # subtree = list(token.subtree)\n",
    "#             # start = subtree[0].i\n",
    "#             # end = subtree[-1].i + 1\n",
    "#             # return doc[start:end]\n",
    "#\n",
    "# def spacy_find_spo(str):\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "#     doc = nlp(str)\n",
    "#     sentence = next(doc.sents)\n",
    "#     previous = None\n",
    "#     new_sentence = []\n",
    "#     for word in sentence:\n",
    "#         print(f\"{word} : {word.dep_}\")\n",
    "#         add_word = None\n",
    "#         if \"subj\" in word.dep_:\n",
    "#             add_word = word\n",
    "#         if \"ROOT\" in word.dep_:\n",
    "#             add_word = word\n",
    "#         if \"pobj\" in word.dep_:\n",
    "#             add_word = word\n",
    "#         if \"dobj\" in word.dep_:\n",
    "#             add_word = word\n",
    "#         # if \"prep\" in word.dep_:\n",
    "#         #     add_word = word\n",
    "#         if \"ccomp\" in word.dep_:\n",
    "#             add_word = word\n",
    "#         if \"pcomp\" in word.dep_:\n",
    "#             add_word = word\n",
    "#         if add_word is not None:\n",
    "#             if previous is not None and \"compound\" in previous.dep_:\n",
    "#                 new_sentence.append(previous)\n",
    "#             new_sentence.append(add_word)\n",
    "#         previous = word\n",
    "#     return new_sentence\n",
    "#\n",
    "# def find_spacy_subject(str):\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "#     doc = nlp(str)\n",
    "#     for token in doc:\n",
    "#         # Check if the token is a verb and has a subject\n",
    "#         if token.dep_ == \"ROOT\":\n",
    "#             for child in token.children:\n",
    "#                 if child.dep_ == \"nsubj\":\n",
    "#                     #subject = child.text\n",
    "#                     predicate = ' '.join(child.text for child in token.children)\n",
    "#                 break\n",
    "#     return predicate\n",
    "#\n",
    "# def find_spacy_object(str):\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "#     doc = nlp(str)\n",
    "#     for token in doc:\n",
    "#         # Check if the token is a verb and has a subject\n",
    "#         if token.dep_ == \"ROOT\":\n",
    "#             for child in token.children:\n",
    "#                 if child.dep_ == \"nsubj\":\n",
    "#                     obj = child.text\n",
    "#                 break\n",
    "#     return obj\n",
    "#\n",
    "# #print(spacy_find_spo(\"Syrian forces launch new attack\"))\n",
    "# #print(spacy_find_spo(\"Shares of Genentech, a much larger company with several products on the market, rose more than 2 percent\"))\n",
    "# #print(spacy_find_spo(\"\"\"Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\"\"\"))\n",
    "# #print(spacy_find_spo(\"\"\"Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.\"\"\"))\n",
    "# #print(spacy_find_spo(\"\"\"Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war.\"\"\"))\n",
    "# print(spacy_find_spo(\"\"\"His wife said he was \"100 percent behind George Bush\" and looked forward to using his years of training in the war.\"\"\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
