{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 14:57:32 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-01 14:57:32 INFO: Using device: cpu\n",
      "2023-04-01 14:57:32 INFO: Loading: tokenize\n",
      "2023-04-01 14:57:32 INFO: Loading: pos\n",
      "2023-04-01 14:57:32 INFO: Loading: constituency\n",
      "2023-04-01 14:57:33 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# set 'download_method = None' to not download the resources over and over\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency', download_method=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def trunk_construction(tree, parent_label = None, leave_pos=False):\n",
    "    sentences = []\n",
    "    if 'NN' in tree.label:\n",
    "        if parent_label == 'NP':\n",
    "            # sentences.append(tree)\n",
    "            sentences = sentences + tree.leaf_labels()\n",
    "    if 'VB' in tree.label:\n",
    "        if parent_label == 'VP':\n",
    "            #sentences.append(tree)\n",
    "            sentences = sentences + tree.leaf_labels()\n",
    "    for child in tree.children:\n",
    "        sentences = sentences + trunk_construction(child, tree.label)\n",
    "\n",
    "    return sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def test_parser(str, valid_sentence):\n",
    "\n",
    "    doc = nlp(str)\n",
    "    tree = doc.sentences[0].constituency\n",
    "\n",
    "    words = trunk_construction(tree)\n",
    "    new_sentence = ' '.join(words)\n",
    "    assert new_sentence == valid_sentence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# test_parser(\"\"\"Amrozi accused his brother of deliberately distorting his evidence.\"\"\", \"Amrozi\", \"distorting\", \"evidence\")\n",
    "test_parser('Syrian forces launch new attacks', \"forces launch attacks\")\n",
    "test_parser(\"\"\"the flat tire was replaced by the driver\"\"\",\"tire was replaced driver\")\n",
    "test_parser(\"\"\"Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.\"\"\",\n",
    "           \"Amrozi accused brother called witness distorting evidence\")\n",
    "test_parser(\"\"\"Shares of Genentech, a much larger company with several products on the market, rose more than 2 percent\"\"\",\n",
    "            \"Shares Genentech company products market rose percent\")\n",
    "test_parser(\"\"\"Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\"\"\",\n",
    "             \"Gyorgy Heizler head disaster unit said coach was carrying passengers\")\n",
    "test_parser(\"\"\"Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.\"\"\",\n",
    "           \"Referring witness Amrozi accused brother distorting evidence\")\n",
    "test_parser(\"\"\"His wife said he was \"100 percent behind George Bush\" and looked forward to using his years of training in the war.\"\"\",\n",
    "            \"wife said was percent George Bush looked using years training war\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MRPCDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence1, sentence2 = self.x[index]\n",
    "        sim = self.y[index]\n",
    "        return sentence1, sentence2, sim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "outputs": [
    {
     "data": {
      "text/plain": "['(NNP Amrozi) (VBG distorting) (NN evidence)',\n '(NN witness) (VBG Referring) (NN witness)',\n '(NNP Yucaipa) (VBG selling) (NNP Safeway)',\n '(NNP Yucaipa) (VBD sold) (NNP Safeway)']"
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "sentences = [\n",
    "    \"\"\"Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.\"\"\",\n",
    "    \"\"\"Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.\"\"\",\n",
    "    \"Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $2.5 billion.\",\n",
    "    \"Yucaipa bought Dominick's in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998.\"\n",
    "    \"\"\"His wife said he was \"100 percent\" behind George Bush\" and looked forward to using his years of training in the war.\"\"\"\n",
    "]\n",
    "\n",
    "process_docs(sentences)\n",
    "#w2v = word2vec(sentences, min_cound=1, size=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "His : poss\n",
      "wife : nsubj\n",
      "said : ROOT\n",
      "he : nsubj\n",
      "was : ccomp\n",
      "\" : punct\n",
      "100 : nummod\n",
      "percent : npadvmod\n",
      "behind : prep\n",
      "George : compound\n",
      "Bush : pobj\n",
      "\" : punct\n",
      "and : cc\n",
      "looked : conj\n",
      "forward : advmod\n",
      "to : prep\n",
      "using : pcomp\n",
      "his : poss\n",
      "years : dobj\n",
      "of : prep\n",
      "training : pobj\n",
      "in : prep\n",
      "the : det\n",
      "war : pobj\n",
      ". : punct\n",
      "[wife, said, he, was, George, Bush, using, years, training, war]\n"
     ]
    }
   ],
   "source": [
    "# Testing the spacy library to extract spo\n",
    "# This is only test code and should not be uncommented.\n",
    "\n",
    "# import spacy\n",
    "#\n",
    "# def get_spacy_subject_phrase(doc):\n",
    "#     for token in doc:\n",
    "#         if (\"subj\" in token.dep_):\n",
    "#             return token\n",
    "#             # subtree = list(token.subtree)\n",
    "#             # start = subtree[0].i\n",
    "#             # end = subtree[-1].i + 1\n",
    "#             # return doc[start:end]\n",
    "#\n",
    "# def get_spacy_predicate_phrase(doc):\n",
    "#     for token in doc:\n",
    "#         if (\"ROOT\" in token.dep_):\n",
    "#             subtree = list(token.subtree)\n",
    "#             start = subtree[0].i\n",
    "#             end = subtree[-1].i + 1\n",
    "#             return token\n",
    "#             # return doc[start:end]\n",
    "#\n",
    "# def get_spacy_object_phrase(doc):\n",
    "#     for token in doc:\n",
    "#         if (\"dobj\" in token.dep_):\n",
    "#             return token\n",
    "#             # subtree = list(token.subtree)\n",
    "#             # start = subtree[0].i\n",
    "#             # end = subtree[-1].i + 1\n",
    "#             # return doc[start:end]\n",
    "#\n",
    "# def spacy_find_spo(str):\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "#     doc = nlp(str)\n",
    "#     sentence = next(doc.sents)\n",
    "#     previous = None\n",
    "#     new_sentence = []\n",
    "#     for word in sentence:\n",
    "#         print(f\"{word} : {word.dep_}\")\n",
    "#         add_word = None\n",
    "#         if \"subj\" in word.dep_:\n",
    "#             add_word = word\n",
    "#         if \"ROOT\" in word.dep_:\n",
    "#             add_word = word\n",
    "#         if \"pobj\" in word.dep_:\n",
    "#             add_word = word\n",
    "#         if \"dobj\" in word.dep_:\n",
    "#             add_word = word\n",
    "#         # if \"prep\" in word.dep_:\n",
    "#         #     add_word = word\n",
    "#         if \"ccomp\" in word.dep_:\n",
    "#             add_word = word\n",
    "#         if \"pcomp\" in word.dep_:\n",
    "#             add_word = word\n",
    "#         if add_word is not None:\n",
    "#             if previous is not None and \"compound\" in previous.dep_:\n",
    "#                 new_sentence.append(previous)\n",
    "#             new_sentence.append(add_word)\n",
    "#         previous = word\n",
    "#     return new_sentence\n",
    "#\n",
    "# def find_spacy_subject(str):\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "#     doc = nlp(str)\n",
    "#     for token in doc:\n",
    "#         # Check if the token is a verb and has a subject\n",
    "#         if token.dep_ == \"ROOT\":\n",
    "#             for child in token.children:\n",
    "#                 if child.dep_ == \"nsubj\":\n",
    "#                     #subject = child.text\n",
    "#                     predicate = ' '.join(child.text for child in token.children)\n",
    "#                 break\n",
    "#     return predicate\n",
    "#\n",
    "# def find_spacy_object(str):\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "#     doc = nlp(str)\n",
    "#     for token in doc:\n",
    "#         # Check if the token is a verb and has a subject\n",
    "#         if token.dep_ == \"ROOT\":\n",
    "#             for child in token.children:\n",
    "#                 if child.dep_ == \"nsubj\":\n",
    "#                     obj = child.text\n",
    "#                 break\n",
    "#     return obj\n",
    "#\n",
    "# #print(spacy_find_spo(\"Syrian forces launch new attack\"))\n",
    "# #print(spacy_find_spo(\"Shares of Genentech, a much larger company with several products on the market, rose more than 2 percent\"))\n",
    "# #print(spacy_find_spo(\"\"\"Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\"\"\"))\n",
    "# #print(spacy_find_spo(\"\"\"Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.\"\"\"))\n",
    "# #print(spacy_find_spo(\"\"\"Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war.\"\"\"))\n",
    "# print(spacy_find_spo(\"\"\"His wife said he was \"100 percent behind George Bush\" and looked forward to using his years of training in the war.\"\"\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
